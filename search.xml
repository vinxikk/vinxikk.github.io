<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数据仓库：缓慢变化维度（SCD）</title>
      <link href="/2019/01/21/datawarehouse/dw-scd/"/>
      <url>/2019/01/21/datawarehouse/dw-scd/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是SCD"><a href="#什么是SCD" class="headerlink" title="什么是SCD"></a>什么是SCD</h3><p>SCD(Slowly Changing Dimensions)，一般翻译成：缓慢变化维。</p><p>SCD的提出是因为，在现实世界中，维度的属性并不是静态的，它会随着时间的流逝发生缓慢的变化。</p><p>缓慢变化维度其实就是指，在维度表中那些会随着时间变化的字段，比如用户基本资料。</p><p>缓慢是一个相对的概念，与缓慢变化的维度相比，数据增长快速是事实表。</p><h3 id="如何处理SCD问题"><a href="#如何处理SCD问题" class="headerlink" title="如何处理SCD问题"></a>如何处理SCD问题</h3><p>处理方法通常有3种，假设有如下的表数据：</p><table><thead><tr><th>id</th><th>name</th><th>city</th></tr></thead><tbody><tr><td>1001</td><td>tom</td><td>Shanghai</td></tr></tbody></table><p>现在tom被调到北京总部工作，所以需要对city进行更新。</p><h4 id="直接覆盖原值"><a href="#直接覆盖原值" class="headerlink" title="直接覆盖原值"></a>直接覆盖原值</h4><p>对其相应的需要重写的维度行中的旧值，以当前值替换。</p><table><thead><tr><th>id</th><th>name</th><th>city</th></tr></thead><tbody><tr><td>1001</td><td>tom</td><td>Beijing</td></tr></tbody></table><p>这样处理，易于实现，始终反映最近的情况，但是没有保留历史数据，无法分析历史变化信息。</p><h4 id="增加维度行"><a href="#增加维度行" class="headerlink" title="增加维度行"></a>增加维度行</h4><p>当有维度属性发生变化时，生成一条新的维度记录，如下：</p><table><thead><tr><th>id</th><th>name</th><th>city</th></tr></thead><tbody><tr><td>1001</td><td>tom</td><td>Shanghai</td></tr><tr><td>1001</td><td>tom</td><td>Beijing</td></tr></tbody></table><p>但是这样，当与别的表通过id关联时，有两个1001的id数据，这样是有问题的，这时就需要代理键的支持（Surrogate Key，唯一标识数据仓库表记录的键）。</p><table><thead><tr><th>sk_id</th><th>id</th><th>name</th><th>city</th></tr></thead><tbody><tr><td>0001</td><td>1001</td><td>tom</td><td>Shanghai</td></tr><tr><td>0002</td><td>1001</td><td>tom</td><td>Beijing</td></tr></tbody></table><p>现在每条数据都唯一，但又有一个问题，现在不知道哪条是当前在用的数据，虽然可以通过代理键找最大的（主键往往是自增的，最大的通常是最新的数据），但某些情况下要查找历史数据就不好找了，所以在维度表中加入时间字段，用NULL来标识哪条是当前最新数据，有变化再进行更新。</p><table><thead><tr><th>sk_id</th><th>id</th><th>name</th><th>city</th><th>start_time</th><th>end_time</th></tr></thead><tbody><tr><td>0001</td><td>1001</td><td>tom</td><td>Shanghai</td><td>2016/10/26</td><td>2018/01/21</td></tr><tr><td>0002</td><td>1001</td><td>tom</td><td>Beijing</td><td>2018/01/21</td><td>NULL</td></tr></tbody></table><p>以上其实就是拉链表的典型实现，可以记录缓慢变化维度属性的变化，是一种非常有效的工具。</p><h4 id="增加属性列"><a href="#增加属性列" class="headerlink" title="增加属性列"></a>增加属性列</h4><p>尽管第二种方式可以区分历史情况，但是无法保证能够将新属性值和过去的历史事实关联。</p><p>第三方式：增加属性列，比较适合需要根据先后顺序来得出某种结论的场景。</p><table><thead><tr><th>id</th><th>name</th><th>curr_city</th><th>old_city</th></tr></thead><tbody><tr><td>1001</td><td>tom</td><td>Beijing</td><td>Shanghai</td></tr></tbody></table><p>这种方式的优点是，可以同时分析当前及前一次变化的属性值；缺点是，只保留了最后一次变化信息。</p><p>这种方案，在一些场景中可以解决很多问题，但我们不能无限制地添加新的字段来记录历史的状态，因此在使用这种方案时，会有一些取舍。</p><p>总结：在实际建模中，我们可以结合使用三种方式，也可以对一个维度表中的不同属性使用不同的方式。这些都是需要根据实际情况来决定，但目的都是一样的，就是能够方便的支持分析历史变化情况。</p>]]></content>
      
      
      <categories>
          
          <category> DataWarehouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的Tumbling Windows和Sliding Windows</title>
      <link href="/2019/01/11/flink/flink-tumbling-window-vs-sliding-window/"/>
      <url>/2019/01/11/flink/flink-tumbling-window-vs-sliding-window/</url>
      
        <content type="html"><![CDATA[<h3 id="Flink的窗口函数"><a href="#Flink的窗口函数" class="headerlink" title="Flink的窗口函数"></a>Flink的窗口函数</h3><p>官网地址：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/operators/windows.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/operators/windows.html</a></p><p>在流系统中通常会使用到Windows来统计一定范围的数据，比如按时间、按个数等进行统计。</p><p>Flink中主要有4种窗口函数：</p><ul><li>Tumbling Windows - 滚动窗口</li><li>Sliding Windows - 滑动窗口</li><li>Session Windows - 会话窗口</li><li>Global Windows - 全局窗口</li></ul><p>其中比较常用的是前面的两种。</p><p>Tumbling Windows和Sliding Windows之间最大的区别就是：Tumbling Windows是不可能重叠的，而Sliding Windows是存在重叠的。</p><p>我们可以将Tumbling Windows看作是Sliding Windows的特殊情况，当Sliding Windows的滑动时间和窗口时间一样的时候，这时候Sliding Windows窗口之间就不会重叠，这就是Tumbling Windows。</p><h4 id="Tumbling-Windows-滚动窗口"><a href="#Tumbling-Windows-滚动窗口" class="headerlink" title="Tumbling Windows - 滚动窗口"></a>Tumbling Windows - 滚动窗口</h4><p>Tumbling Window在流数据中进行滚动，这种窗口不存在重叠，也就是说一个events/data只可能出现在一个窗口中，如下图所示：</p><p><img src="https://vinxikk.github.io/img/flink/flink-tumbling-window.png" alt="Tumbling Windows"></p><p>可以看到，Tumbling Window之间是不存在重叠的。</p><p>Tumbling Window的创建可以基于数量（比如每5个元素构成一个窗口）或者基于时间（每隔10s创建一个窗口）。</p><p>创建基于时间（每隔5s）的Tumbling Window：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowsApp</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"hadoop002"</span>, <span class="number">9999</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    text.flatMap(_.split(<span class="string">","</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">      .map((_, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">      .keyBy(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      .sum(<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">      .print()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">      .setParallelism(<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    env.execute(<span class="string">"WindowsApp"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>然后Linux机器上启动netcat，并输入数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop002 ~]<span class="comment"># nc -lk 9999</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">hello,world</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">hello,java</span></pre></td></tr></table></figure><p>在控制台上可以看到结果：</p><p>(world,1)<br>(hello,1)<br>(hello,1)<br>(java,1)</p><h4 id="Sliding-Windows-滑动窗口"><a href="#Sliding-Windows-滑动窗口" class="headerlink" title="Sliding Windows - 滑动窗口"></a>Sliding Windows - 滑动窗口</h4><p>Sliding Window是在流数据中进行滑动，窗口之间可以重叠，它可以在传入的数据流中进行平滑聚合，如下图所示：</p><p><img src="https://vinxikk.github.io/img/flink/flink-sliding_window.jpg" alt="Sliding Windows"></p><p>和Tumbling Window一样，我们可以根据需求创建基于数量或者基于时间的Sliding Window。</p><p>创建基于时间（窗口大小10s，滑动距离5s）的Sliding Windows：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowsApp</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"hadoop002"</span>, <span class="number">9999</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    text.flatMap(_.split(<span class="string">","</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">      .map((_, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">      .keyBy(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//      .timeWindow(Time.seconds(5))</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">      .sum(<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">      .print()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">      .setParallelism(<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    env.execute(<span class="string">"WindowsApp"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统：协同过滤算法</title>
      <link href="/2018/08/11/rs/rs-collaborative-filtering/"/>
      <url>/2018/08/11/rs/rs-collaborative-filtering/</url>
      
        <content type="html"><![CDATA[<h3 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h3><p>协同过滤算法是诞生最早，且较为著名的推荐算法，主要用于预测和推荐。</p><p>算法通过对用户历史行为数据的挖掘发现用户的偏好，基于不同的偏好对用户进行群组划分并推荐品味相似的商品。</p><p>协同过滤推荐算法分为两类：基于用户的协同过滤算法（user-based collaborative filtering），基于物品的协同过滤算法（item-based collaborative filtering）。</p><p>简单的说就是：人以群分，物以类聚。</p><h3 id="基于用户的协同过滤算法"><a href="#基于用户的协同过滤算法" class="headerlink" title="基于用户的协同过滤算法"></a>基于用户的协同过滤算法</h3><p>基于用户的协同过滤算法，是通过用户的历史行为数据发现用户对商品或内容的喜好（如商品购买、收藏、内容评论或分享等），并对这些喜好进行度量和打分，根据不同用户对相同商品或内容的态度和偏好程度计算用户之间的关系，最后在有相同喜好的用户间进行商品推荐。</p><p>简单的说就是，假如A、B两个用户都购买了x、y、z三本图书，并且都给出了5星的好评，那么就认为A和B属于同一类用户，可以将A看过的图书w也推荐给用户B。</p><p><img src="https://vinxikk.github.io/img/rs/user-based-cf.png" alt="基于用户的协同过滤"></p><h4 id="寻找偏好相似的用户"><a href="#寻找偏好相似的用户" class="headerlink" title="寻找偏好相似的用户"></a>寻找偏好相似的用户</h4><p>下面我们模拟5个用户对2件商品的评分，来说明如何通过用户对不同商品的态度和偏好寻找到相似的用户。</p><p><img src="https://vinxikk.github.io/img/rs/user-item-score.png" alt="用户对商品的评分"></p><p>在示例中，5个用户分别对两件商品进行了评分。这里的分值可以表现为真实的购买，也可以是用户对商品不同行为的量化指标。例如，浏览商品的次数，向朋友推荐商品，收藏，分享或评论等，这些行为都可以表示用户对商品的态度和偏好程度。</p><p>从表格中很难直观发现5个用户间的联系，我们将5个用户对2件商品的评分用散点图表示，用户间的关系就很容易发现了。</p><p><img src="https://vinxikk.github.io/img/rs/user-item-score-scatter.png" alt="用户间的关系"></p><p>在散点图中，X轴是商品1的评分，Y轴是商品2的评分。</p><p>通过用户的分布情况可以发现，A、C、D三个用户距离较近，即用户A(3.3, 6.5)和用户C(3.6, 6.3)、用户D(3.4, 5.8)对两件商品的评分较为接近，可以认为A、C、D三个用户属于同一类的用户。而用户B和E则形成了另一个群体。</p><p>散点图虽然直观，但无法投入实际的应用，也不能准确地度量用户间的关系。因此我们需要通过数字对用户间的关系进行准确的度量，并依据这些关系完成商品的推荐。</p><h4 id="相似性度量"><a href="#相似性度量" class="headerlink" title="相似性度量"></a>相似性度量</h4><h5 id="欧几里德距离评价"><a href="#欧几里德距离评价" class="headerlink" title="欧几里德距离评价"></a>欧几里德距离评价</h5><p>欧几里德距离评价是一个较为简单的用户关系评价方法，原理是通过计算两个用户在散点图中的距离来判断不同的用户是否有相同的偏好。</p><p><img src="https://vinxikk.github.io/img/rs/euclidean-distance.png" alt="欧氏距离计算公式"></p><p>我们获得了5个用户相互之间的欧几里德系数，也就是用户间的距离。系数越小表示两个用户间的距离越近，偏好也越是接近。不过这里有个问题，太小的数值可能无法准确地表现出不同用户间距离的差异，因此我们对求得的系数取倒数，使用户间的距离约接近，数值越大。</p><h5 id="皮尔逊相关度评价"><a href="#皮尔逊相关度评价" class="headerlink" title="皮尔逊相关度评价"></a>皮尔逊相关度评价</h5><p>皮尔逊相关度评价是另一种计算用户间关系的方法，他比欧几里德距离评价的计算要复杂一些，但对于评分数据不规范时，皮尔逊相关度评价能够给出更好的结果。</p><p>以下是多个用户对多件商品进行评分的示例，这个示例比之前的两件商品的情况要复杂一些，但也更接近真实的情况。</p><p>我们通过皮尔逊相关度评价对用户进行相似性度量和分组，并推荐商品。</p><p><img src="https://vinxikk.github.io/img/rs/user-item-score-2.png" alt="用户对多个商品的评分"></p><h5 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h5><p>皮尔逊相关系数的计算公式如下，结果是一个在-1与1之间的系数，该系数用来说明两个用户间联系的强弱程度。</p><p><img src="https://vinxikk.github.io/img/rs/pearson-correlation-coefficient.png" alt="皮尔逊相关系数计算公式"></p><p>相关系数的分类：</p><ul><li>0.8-1.0  极强相关</li><li>0.6-0.8  强相关</li><li>0.4-0.6  中等程度相关</li><li>0.2-0.4  弱相关</li><li>0.0-0.2  极弱相关或无相关</li></ul><p><img src="https://vinxikk.github.io/img/rs/user-pearson-correlation-coefficient.png" alt="用户间的相似度"></p><p>通过计算5个用户对5件商品的评分，我们获得了用户间的相似度数据，可以看到，用户A&amp;B，C&amp;D，C&amp;E和D&amp;E之间的相似度较高。下面，我们就可以按照相似度对用户进行商品推荐了。</p><h4 id="为相似的用户推荐物品"><a href="#为相似的用户推荐物品" class="headerlink" title="为相似的用户推荐物品"></a>为相似的用户推荐物品</h4><h5 id="为用户C推荐商品"><a href="#为用户C推荐商品" class="headerlink" title="为用户C推荐商品"></a>为用户C推荐商品</h5><p>当我们需要对用户C推荐商品时，首先检查之前的相似度列表，发现用户C和用户D、E的相似度较高。换句话说，这三个用户是同一类群体，拥有相同的偏好，因此，我们可以对用户C推荐D和E的商品。</p><p>但这里有一个问题，我们不能直接推荐前面商品1-5的商品，因为这些商品用户C已经浏览或者购买过了，不能重复推荐，因此我们要推荐用户C还没有浏览或购买过的商品。</p><h5 id="加权排序推荐"><a href="#加权排序推荐" class="headerlink" title="加权排序推荐"></a>加权排序推荐</h5><p>我们提取了用户D和用户E评价过的另外5件商品：商品A-F。并对不同商品的评分进行相似度加权，按加权后的结果对5件商品进行排序，然后推荐给用户C。这样，用户C就获得了与他偏好相似的用户D和E评价的商品。而在具体的推荐顺序和展示上我们按照用户D和E与用户C的相似度进行排序。</p><p><img src="https://vinxikk.github.io/img/rs/user-item-recommend.png" alt="为用户C推荐商品"></p><p>以上是基于用户的协同过滤算法，这个算法依靠用户的历史行为数据来计算相关度，也就是说，必须要有一定的数据积累（冷启动问题）。</p><p>对于新网站或数据量较少的网站，还有一种方法是基于物品的协同过滤算法。</p><h3 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h3><p>基于物品的协同过滤算法与基于用户的协同过滤算法很像，将商品和用户互换，通过计算不同用户对不同物品的评分获得物品间的关系，然后基于物品间的关系对用户进行相似物品的推荐。这里的评分代表用户对商品的态度和偏好。</p><p>简单来说就是，如果用户A同时购买了商品1和商品2，那么说明商品1和商品2的相关度较高。当用户B也购买了商品1时，可以推断他也有购买商品2的需求。</p><p><img src="https://vinxikk.github.io/img/rs/item-based-cf.png" alt="基于物品的协同过滤"></p><h4 id="寻找相似的物品"><a href="#寻找相似的物品" class="headerlink" title="寻找相似的物品"></a>寻找相似的物品</h4><p>表格中是两个用户对5件商品的评分。在这个表格中，用户和商品的位置进行了互换，通过两个用户的评分来获得5件商品之间的相似度情况。单从表格中我们依然很难发现其中的联系，因此我们选择通过散点图进行展示。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-score.png" alt="商品的评分"></p><p>单从表格中我们依然很难发现其中的联系，因此我们选择通过散点图进行展示。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-score-scatter.png" alt="物品间的关系"></p><p>在散点图中，X轴和Y轴分别是两个用户的评分，5件商品按照所获的评分值分布在散点图中。</p><p>我们可以发现，商品1, 3, 4在用户A和B中有着近似的评分，说明这三件商品的相关度较高，而商品5和2则在另一个群体中。</p><h4 id="相似性度量（皮尔逊相关度评价）"><a href="#相似性度量（皮尔逊相关度评价）" class="headerlink" title="相似性度量（皮尔逊相关度评价）"></a>相似性度量（皮尔逊相关度评价）</h4><p>我们选择使用皮尔逊相关度评价来计算多用户与多商品的关系，下面是5个用户对5件商品的评分表，我们通过这些评分计算出商品间的相关度。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-score-2.png" alt="商品的评分表"></p><p>商品间的皮尔逊相关度：</p><p><img src="https://vinxikk.github.io/img/rs/item-pearson-correlation-coefficient.png" alt="物品间的相似度"></p><p>通过计算可以发现，商品1&amp;2，商品3&amp;4，商品3&amp;5和商品4&amp;5相似度较高，下一步我们可以依据这些商品间的相关度对用户进行商品推荐。</p><h4 id="为用户提供基于相似物品的推荐"><a href="#为用户提供基于相似物品的推荐" class="headerlink" title="为用户提供基于相似物品的推荐"></a>为用户提供基于相似物品的推荐</h4><p>这里我们遇到了和基于用户进行商品推荐相同的问题，当需要对用户C基于商品3推荐商品时，需要一张新的商品与已有商品间的相似度列表。在前面的相似度计算中，商品3与商品4和5相似度较高，因此我们计算并获得了商品4, 5与其他商品的相似度列表。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-score-3.png" alt="其他商品评分表"></p><p><code>注意：这里的用户1, 2, 3原则上和上面的用户A-E不同</code></p><p>以下是通过计算获得的新商品与已有商品间的相似度数据。</p><p><img src="https://vinxikk.github.io/img/rs/new-item-pearson-correlation-coefficient.png" alt="新物品与已有物品间的相似度"></p><h5 id="加权排序推荐-1"><a href="#加权排序推荐-1" class="headerlink" title="加权排序推荐"></a>加权排序推荐</h5><p>这里是用户C已经购买过的商品4, 5与新商品A, B, C之间的相似程度。</p><p>我们将用户C对商品4, 5的评分作为权重，对商品A, B, C进行加权排序，用户C评分较高并且与之相似度较高的商品被优先推荐。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-recommend.png" alt="为用户C推荐商品"></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 协同过滤 </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark窗口函数</title>
      <link href="/2018/08/10/spark/spark-window-function/"/>
      <url>/2018/08/10/spark/spark-window-function/</url>
      
        <content type="html"><![CDATA[<h3 id="窗口函数引入"><a href="#窗口函数引入" class="headerlink" title="窗口函数引入"></a>窗口函数引入</h3><hr><p>使用Spark SQL进行复杂的离线统计任务，有时需要计算一些排序特征、窗口特征等，显然不能简单地通过groupBy来完成，这时就需要了解spark中的窗口函数。</p><p>比如下面的统计需求：</p><ol><li>统计订单表，每个店铺每个订单和前一单的价格和。此时如果通过groupBy来完成特别费劲。</li><li>统计订单表，每个店铺每个订单与前一单的差值。此时需要自定义聚合函数。</li><li>还有计算前几单的平均值、计算环比之类的，都要用到窗口函数。</li></ol><h4 id="窗口函数的使用"><a href="#窗口函数的使用" class="headerlink" title="窗口函数的使用"></a>窗口函数的使用</h4><p>下面以订单表推演spark中窗口函数的使用。</p><p>订单表字段：订单id，店铺id，支付时间，支付金额。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowFuncApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    .appName(<span class="string">"WindowFuncApp"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    .master(<span class="string">"local"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    .getOrCreate()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> spark.implicits._</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> ordersDF: <span class="type">DataFrame</span> = <span class="type">Seq</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o1"</span>, <span class="string">"s1"</span>, <span class="string">"2017-05-01"</span>, <span class="number">100</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o2"</span>, <span class="string">"s1"</span>, <span class="string">"2017-05-02"</span>, <span class="number">200</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o3"</span>, <span class="string">"s2"</span>, <span class="string">"2017-05-01"</span>, <span class="number">200</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o4"</span>, <span class="string">"s1"</span>, <span class="string">"2017-05-03"</span>, <span class="number">200</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o5"</span>, <span class="string">"s2"</span>, <span class="string">"2017-05-02"</span>, <span class="number">100</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o6"</span>, <span class="string">"s1"</span>, <span class="string">"2017-05-04"</span>, <span class="number">300</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  ).toDF(<span class="string">"order_id"</span>, <span class="string">"seller_id"</span>, <span class="string">"pay_time"</span>, <span class="string">"price"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">  ordersDF.printSchema()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//1.店铺订单顺序</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.functions._</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> rankSpec: <span class="type">WindowSpec</span> = <span class="type">Window</span>.partitionBy(<span class="string">"seller_id"</span>).orderBy(<span class="string">"pay_time"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"rank"</span>, dense_rank.over(rankSpec)).show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> rankSpec2: <span class="type">WindowSpec</span> = <span class="type">Window</span>.partitionBy(<span class="string">"seller_id"</span>).orderBy(<span class="string">"price"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"rank2"</span>, rank.over(rankSpec2)).show() <span class="comment">//1,2,2,4</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"dense_rank2"</span>, dense_rank.over(rankSpec2)).show() <span class="comment">//1,2,2,3</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">  println(<span class="string">"****************************************"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//定义前一单和本单的窗口</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> winSpec: <span class="type">WindowSpec</span> = <span class="type">Window</span>.partitionBy(<span class="string">"seller_id"</span>).orderBy(<span class="string">"pay_time"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//本单及前一单的价格和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"sum_pay"</span>, sum(<span class="string">"price"</span>).over(winSpec)).show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//本单与前一单的平均值，用UDAF</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAvgUdaf</span></span>: <span class="type">UserDefinedAggregateFunction</span> = <span class="keyword">new</span> <span class="type">MyAverage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"avg"</span>, getAvgUdaf($<span class="string">"price"</span>).over(winSpec)).show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"avg2"</span>, avg(<span class="string">"price"</span>).over(winSpec)).show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">  println(<span class="string">"****************************************"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//每个店铺当前订单与前一单的差值，需要自定义聚合函数，或者lag函数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMinusUdaf</span></span>: <span class="type">UserDefinedAggregateFunction</span> = <span class="keyword">new</span> <span class="type">MyMinus</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"rank"</span>, dense_rank.over(rankSpec))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">      .withColumn(<span class="string">"prePrice"</span>, lag(<span class="string">"price"</span>, <span class="number">1</span>).over(rankSpec)) <span class="comment">//前一行的值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">      .withColumn(<span class="string">"minus"</span>, getMinusUdaf($<span class="string">"price"</span>).over(winSpec)) <span class="comment">//在前面的基础上用UDF也行</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">      .show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">/*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line"><span class="comment">  * lag(field, n): 就是取从当前字段往前第n个值，这里是取前一行的值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"><span class="comment">  * first/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line"><span class="comment">  * lag/lead(field, n): lead就是lag相反的操作，这个用于做数据回测特别有用，结果回推条件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line"><span class="comment">  * */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">  spark.stop()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line"><span class="comment">    * 自定义聚合函数UDAF</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line"><span class="comment">    */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//继承抽象函数必须实现以下方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//输入参数的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//缓冲区中进行聚合时，所处理的数据的类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"count"</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">"sum"</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//初始化给定的聚合缓冲区，即聚合缓冲区的零值（缓冲区内的数组和映射仍然是不可变的）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">      buffer(<span class="number">0</span>) = <span class="number">0</span>L <span class="comment">//表示次数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">      buffer(<span class="number">1</span>) = <span class="number">0.0</span>D <span class="comment">//表示总和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//使用来自input的新输入数据更新给定的聚合缓冲区buffer，每个输入行调用一次</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line">        buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + <span class="number">1</span>L <span class="comment">//次数加1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">        buffer(<span class="number">1</span>) = buffer.getDouble(<span class="number">1</span>) + input.getAs[<span class="type">Long</span>](<span class="number">0</span>).toDouble <span class="comment">//求和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//此函数是否始终在相同输入上返回相同的输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//合并两个聚合缓冲区并将更新的缓冲区值存储回buffer1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//当我们将两个部分聚合的数据合并在一起时调用此方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//Spark是分布式的，所以不同的区需要进行合并</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">      buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>) <span class="comment">//求次数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">      buffer1(<span class="number">1</span>) = buffer1.getDouble(<span class="number">1</span>) + buffer2.getDouble(<span class="number">1</span>) <span class="comment">//求和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//计算最终的结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line">      buffer.getDouble(<span class="number">1</span>) / buffer.getLong(<span class="number">0</span>).toDouble</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//返回值的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyMinus</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">104</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">105</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">106</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"minus"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">107</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">108</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">109</span></pre></td><td class="code"><pre><span class="line">      buffer(<span class="number">0</span>) = <span class="number">0</span>L <span class="comment">//表示差值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">110</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">111</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">112</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">113</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">114</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//输入的后者减去前者</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">115</span></pre></td><td class="code"><pre><span class="line">        buffer(<span class="number">0</span>) = input.getLong(<span class="number">0</span>) - buffer.getLong(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">116</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">117</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">118</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">119</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//此函数是否始终在相同输入上返回相同的输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">120</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">121</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">122</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">123</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//分区合并，也是后者减前者</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">124</span></pre></td><td class="code"><pre><span class="line">      buffer1(<span class="number">0</span>) = buffer2.getLong(<span class="number">0</span>) - buffer1.getLong(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">125</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">126</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">127</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//计算最终的结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">128</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">129</span></pre></td><td class="code"><pre><span class="line">      buffer.getLong(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">130</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">131</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">132</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//返回值的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">133</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">LongType</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">134</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">135</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark窗口函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库：拉链表</title>
      <link href="/2018/06/25/datawarehouse/dw-zipper/"/>
      <url>/2018/06/25/datawarehouse/dw-zipper/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是拉链表"><a href="#什么是拉链表" class="headerlink" title="什么是拉链表"></a>什么是拉链表</h3><p>拉链表是针对数据仓库中表存储数据的方式而定义的。</p><p>所谓拉链，就是记录历史，记录一个事物从开始，一直到当前状态的所有变化的信息。</p><p>下面的示例就是一张拉链表，存储的是用户的基本信息以及每条记录的生命周期。我们可以使用这张表拿到当天的最新数据以及之前的历史数据。</p><table><thead><tr><th>register_date</th><th>user_id</th><th>phone_number</th><th>t_start_date</th><th>t_end_date</th></tr></thead><tbody><tr><td>2017-09-01</td><td>0001</td><td>13620321111</td><td>2017-09-01</td><td>9999-12-31</td></tr><tr><td>2017-09-01</td><td>0002</td><td>13620321112</td><td>2017-09-01</td><td>2017-09-01</td></tr><tr><td>2017-09-01</td><td>0002</td><td>13620321113</td><td>2017-09-02</td><td>9999-12-31</td></tr><tr><td>2017-09-01</td><td>0003</td><td>13620321114</td><td>2017-09-01</td><td>9999-12-31</td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321115</td><td>2017-09-01</td><td>2017-09-01</td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321116</td><td>2017-09-02</td><td>2017-09-02</td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321117</td><td>2017-09-03</td><td>9999-12-31</td></tr><tr><td>2017-09-02</td><td>0005</td><td>13620321118</td><td>2017-09-02</td><td>2017-09-07</td></tr><tr><td>2017-09-02</td><td>0005</td><td>13620321119</td><td>2017-09-08</td><td>9999-12-31</td></tr><tr><td>2017-09-03</td><td>0006</td><td>13620321110</td><td>2017-09-03</td><td>9999-12-31</td></tr></tbody></table><h3 id="拉链表的使用场景"><a href="#拉链表的使用场景" class="headerlink" title="拉链表的使用场景"></a>拉链表的使用场景</h3><p>在数据仓库的模型设计过程中，经常会遇到下面这种表的设计：</p><ol><li>有一些表的数据量很大，比如一张用户表，大约10亿条记录，50个字段。这种表，及即使用ORC压缩，单张表的存储也会超过100G，在HDFS上使用双备份或三备份的话，所占用的看空间会更大。</li><li>表中的部分字段会被update更新操作，如用户联系方式，产品描述信息，订单状态等。</li><li>需要查看某一个时间点或时间段的历史快照信息，比如，查看某一个订单在历史某一个时间点的状态。</li><li>表中的记录变化的比例和频率不是很大，比如，总共有10亿的用户，每天新增和发生变化的有200万左右，变化的比例占的很小。</li></ol><p>那么，对于这种表该如何设计呢，下面有几种方案：</p><ul><li>方案一：每天只留最新的一份，比如每天用Sqoop抽取最新的一份全量数据到Hive中</li><li>方案二：每天保留一份全量的切片数据</li><li>方案三：使用拉链表</li></ul><h3 id="为什么使用拉链表"><a href="#为什么使用拉链表" class="headerlink" title="为什么使用拉链表"></a>为什么使用拉链表</h3><p><strong>方案一：</strong></p><p>实现起来简单，每天drop掉前一天的数据，重新抽一份最新的。</p><p>优点：节省空间，一些普通的使用也很方便，不用在选择表的时候加一个时间分区什么的。</p><p>缺点：没有历史数据，想翻翻旧账只能通过其它方式，比如从流水表里面抽。</p><p><strong>方案二：</strong></p><p>每天一份全量的切片是一种比较稳妥的方案，而且历史数据也在。</p><p>缺点：存储空间占用量太大，如果每天都保留一份全量，那么每次全量中会保存很多不变的信息，对存储是极大的浪费。</p><p>当然也可以做一些取舍，比如只保留近一个月的数据。但是，需求是无耻的，数据的生命周期不是我们能完全左右的，你会发现，存储周期可能会从30天变为90天，然后再从90天变为1年，然后需要永久保存。</p><p><strong>拉链表：</strong></p><p>拉链表在使用上基本兼顾了我们的需求。</p><p>首先它在空间上做了一个取舍，虽说不像方案一那样占用量那么小，但是它每天的增量可能只有方案二的千分之一，甚至是万分之一。</p><p>它能满足方案二的需求，既能获取最新的数据，也能添加筛选条件获取历史数据，所以在一些场景下，拉链表是能解决很多问题的。</p><h3 id="拉链表的设计和实现"><a href="#拉链表的设计和实现" class="headerlink" title="拉链表的设计和实现"></a>拉链表的设计和实现</h3><h4 id="如何设计一张拉链表"><a href="#如何设计一张拉链表" class="headerlink" title="如何设计一张拉链表"></a>如何设计一张拉链表</h4><p>以用户资料表为例，我们先看一下在关系型数据库里的user表中的信息变化。</p><p>在2017-09-01这一天表中的数据是：</p><table><thead><tr><th>register_date</th><th>user_id</th><th>phone_number</th></tr></thead><tbody><tr><td>2017-09-01</td><td>0001</td><td>13620321111</td></tr><tr><td>2017-09-01</td><td>0002</td><td>13620321112</td></tr><tr><td>2017-09-01</td><td>0003</td><td>13620321114</td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321115</td></tr></tbody></table><p>在2017-09-02这一天表中的数据是，用户0002和0004的资料进行了修改，0005是新增用户：</p><table><thead><tr><th>register_date</th><th>user_id</th><th>phone_number</th><th>remark</th></tr></thead><tbody><tr><td>2017-09-01</td><td>0001</td><td>13620321111</td><td></td></tr><tr><td>2017-09-01</td><td>0002</td><td>13620321113</td><td>13620321112 =&gt; 13620321113</td></tr><tr><td>2017-09-01</td><td>0003</td><td>13620321114</td><td></td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321116</td><td>13620321115 =&gt; 13620321116</td></tr><tr><td>2017-09-02</td><td>0005</td><td>13620321118</td><td>2017-09-02新增</td></tr></tbody></table><p>在2019-09-03这一天表中的数据是，用户0004和0005资料进行了修改，0006是新增用户：</p><table><thead><tr><th>register_date</th><th>user_id</th><th>phone_number</th><th>remark</th></tr></thead><tbody><tr><td>2017-09-01</td><td>0001</td><td>13620321111</td><td></td></tr><tr><td>2017-09-01</td><td>0002</td><td>13620321113</td><td></td></tr><tr><td>2017-09-01</td><td>0003</td><td>13620321114</td><td></td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321117</td><td>13620321116 =&gt; 13620321117</td></tr><tr><td>2017-09-02</td><td>0005</td><td>13620321119</td><td>13620321118 = &gt; 13620321119</td></tr><tr><td>2017-09-03</td><td>0006</td><td>13620321110</td><td>2017-09-03新增</td></tr></tbody></table><p>如果在数据仓库中设计成历史拉链表保存改变，则会有下面这样一张表，这是最新一天（即2017-09-03）的数据：</p><table><thead><tr><th>register_date</th><th>user_id</th><th>phone_number</th><th>t_start_date</th><th>t_end_date</th></tr></thead><tbody><tr><td>2017-09-01</td><td>0001</td><td>13620321111</td><td>2017-09-01</td><td>9999-12-31</td></tr><tr><td>2017-09-01</td><td>0002</td><td>13620321112</td><td>2017-09-01</td><td>2017-09-01</td></tr><tr><td>2017-09-01</td><td>0002</td><td>13620321113</td><td>2017-09-02</td><td>9999-12-31</td></tr><tr><td>2017-09-01</td><td>0003</td><td>13620321114</td><td>2017-09-01</td><td>9999-12-31</td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321115</td><td>2017-09-01</td><td>2017-09-01</td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321116</td><td>2017-09-02</td><td>2017-09-02</td></tr><tr><td>2017-09-01</td><td>0004</td><td>13620321117</td><td>2017-09-03</td><td>9999-12-31</td></tr><tr><td>2017-09-02</td><td>0005</td><td>13620321118</td><td>2017-09-02</td><td>2017-09-07</td></tr><tr><td>2017-09-02</td><td>0005</td><td>13620321119</td><td>2017-09-08</td><td>9999-12-31</td></tr><tr><td>2017-09-03</td><td>0006</td><td>13620321110</td><td>2017-09-03</td><td>9999-12-31</td></tr></tbody></table><p>说明：</p><ul><li><code>t_start_date</code>表示该条记录的生命周期开始时间，<code>t_end_date</code>表示该条记录的生命周期结束时间。</li><li><code>t_end_date = &#39;9999-12-31&#39;</code>表示该条记录目前处于有效状态。</li><li>如果查询当前所有有效的记录，则是<code>select * from user where t_end_date = &#39;9999-12-31&#39;</code>。</li><li>如果查询2017-09-02的快照，则是<code>select * from user where t_start_date &lt;= &#39;2017-09-02&#39; and t_end_date &gt;= &#39;2017-09-02&#39;</code>。</li></ul><h4 id="Hive实现拉链表"><a href="#Hive实现拉链表" class="headerlink" title="Hive实现拉链表"></a>Hive实现拉链表</h4><p>大部分公司都会选择以HDFS和Hive为主的数据仓库架构，在Hive的0.14版本之前，Hive的表只能进行删除和添加，不能进行update。</p><p>以上面的用户表为例，我们要实现用户的拉链表，首先确定数据源：</p><ul><li>需要一张ODS层的用户全量表，用来初始化数据</li><li>每天的用户更新表</li></ul><p>还要确定拉链表的时间粒度，比如说，拉链表每天只取一个状态，也就是说如果一天有3个状态变更，我们只取最后一个状态，这种天的粒度的表其实已经能解决大部分的问题。</p><p>获取每天的用户更新和增量信息的3种方式：</p><ol><li>监听MySQL数据的变化，比如说用Canal，最后合并每日的变化，获取到最新的一个状态。</li><li>假设每天都会获得一份切片数据，可以通过取两天切片数据的不同来作为每日更新表，这种情况下可以对所有的字段先进行concat，再取md5，这样就ok了。</li><li>流水表，有每日的变更流水表。</li></ol><h5 id="ODS层的user表"><a href="#ODS层的user表" class="headerlink" title="ODS层的user表"></a>ODS层的user表</h5><p>用户资料切片表的建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> ods.user (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  user_id <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'用户编号'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  phone_number <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'手机号码'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  register_date <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'注册日期'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">COMMENT</span> <span class="string">'用户资料表'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">PARTITIONED <span class="keyword">BY</span> (dt <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span> <span class="keyword">LINES</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">LOCATION <span class="string">'/ods/user'</span>;</span></pre></td></tr></table></figure><h5 id="ODS层的user-update表"><a href="#ODS层的user-update表" class="headerlink" title="ODS层的user_update表"></a>ODS层的user_update表</h5><p>我们还需要一张用户每日更新表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> ods.user_update (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  user_id <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'用户编号'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  phone_number <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'手机号码'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  register_date <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'注册日期'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">COMMENT</span> <span class="string">'每日用户资料更新表'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">PARTITIONED <span class="keyword">BY</span> (dt <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span> <span class="keyword">LINES</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">LOCATION <span class="string">'/ods/user_update'</span>;</span></pre></td></tr></table></figure><h5 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h5><p>创建一张拉链表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> dws.user_his (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  user_id <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'用户编号'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  phone_number <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'手机号码'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  register_date <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'用户编号'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  t_start_date ,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  t_end_date</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">COMMENT</span> <span class="string">'用户资料拉链表'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span> <span class="keyword">LINES</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">LOCATION <span class="string">'/dws/user_his'</span>;</span></pre></td></tr></table></figure><h5 id="实现SQL语句"><a href="#实现SQL语句" class="headerlink" title="实现SQL语句"></a>实现SQL语句</h5><p>现在假设已经初始化了2017-09-01的数据，然后需要更新2017-09-02那一天的数据，那么每日更新的SQL语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> dws.user_his</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">SELECT</span> A.user_id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">           A.phone_number,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">           A.register_date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">           A.t_start_time,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">           <span class="keyword">CASE</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">WHEN</span> A.t_end_time = <span class="string">'9999-12-31'</span> <span class="keyword">AND</span> B.user_num <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">THEN</span> <span class="string">'2017-09-01'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">ELSE</span> A.t_end_time</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">           <span class="keyword">END</span> <span class="keyword">AS</span> t_end_time</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">FROM</span> dws.user_his <span class="keyword">AS</span> A</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> ods.user_update <span class="keyword">AS</span> B</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">ON</span> A.user_id = B.user_id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">UNION</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">SELECT</span> C.user_id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">           C.phone_number,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">           C.register_date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">           <span class="string">'2017-09-02'</span> <span class="keyword">AS</span> t_start_time,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">           <span class="string">'9999-12-31'</span> <span class="keyword">AS</span> t_end_time</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">FROM</span> ods.user_update <span class="keyword">AS</span> C</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">AS</span> T</span></pre></td></tr></table></figure><p>如果需要更新其它日期的数据，把两个日期设置为变量就可以了。</p><h4 id="拉链表和流水表"><a href="#拉链表和流水表" class="headerlink" title="拉链表和流水表"></a>拉链表和流水表</h4><p>流水表存放的是一个用户的变更记录，比如在一张流水表中，一天的数据中，会存放一个用户的每条修改记录，但是在拉链表中只有一条记录。</p><p>这是拉链表设计时需要注意的一个粒度问题。我们当然也可以设置的粒度更小一些，一般按天就足够。</p><h4 id="查询性能"><a href="#查询性能" class="headerlink" title="查询性能"></a>查询性能</h4><p>拉链表当然也会遇到查询性能的问题，比如说我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了，有两种解决思路：</p><ol><li>在一些查询引擎中，我们对 start_date 和 end_date 做索引，这样能提高不少性能。这种方法其实在 Hive 中行不通，因为 Hive 相当于没有索引，不过在其它系统中可以考虑。</li><li>保留部分历史数据，比如说我们一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近 3 个月数据的拉链表。</li></ol><h4 id="淘汰机制"><a href="#淘汰机制" class="headerlink" title="淘汰机制"></a>淘汰机制</h4><p>关于淘汰机制，其实和性能也是有关系的，一方面是因为所有数据的积累会导致计算越来越慢，另一方面是业务侧其实对历史数据的需求也有一定的优先级的。</p><p>因此在设计拉链表的时候可以制定一些数据的淘汰机制。淘汰的数据不一定要删除，比如我们建立两张拉链表，一张拉链表中只保存最新的十条数据，其它的数据会存入一张历史拉链表中。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>拉链表使用心得：</p><ul><li>使用拉链表的时候可以不加 t_end_date，即失效日期，但是加上之后，能优化很多查询。</li><li>可以加上当前行状态标识，能快速定位到当前状态。</li><li>在拉链表的设计中可以加一些内容，因为我们每天保存一个状态，如果我们在这个状态里面加一个字段，比如如当天修改次数，那么拉链表的作用就会更大。</li></ul>]]></content>
      
      
      <categories>
          
          <category> DataWarehouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库：模型设计和维度建模</title>
      <link href="/2018/06/24/datawarehouse/dw-model/"/>
      <url>/2018/06/24/datawarehouse/dw-model/</url>
      
        <content type="html"><![CDATA[<h3 id="数据仓库模型"><a href="#数据仓库模型" class="headerlink" title="数据仓库模型"></a>数据仓库模型</h3><p>数据仓库中有几种经典的数据模型：范式模型、维度模型、DataVault。</p><p>很多模型的设计都在同构化，而且在工作中也不是单独地用一种模型，会根据业务场景做出选择。</p><h4 id="范式模型"><a href="#范式模型" class="headerlink" title="范式模型"></a>范式模型</h4><p>范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解。</p><p>在数据仓库的模型设计中目前一般采用第三范式。</p><p>一个符合第三范式的关系具有以下三个条件：</p><ul><li>数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值。</li><li>数据库表中的每个实例或行必须可以被唯一区分。</li><li>数据库表中不包含已在其它表中已包含的非主关键字信息。</li></ul><p>范式模型由数据仓库之父Inmon提倡，可以大致地按照OLTP设计中的3NF来理解，它在范式理论上符合3NF，它与OLTP系统中的3NF的区别在于数据仓库中的3NF是站在企业角度面向主题的抽象，而不是针对某个具体业务流程的实体对象关系抽象，它更多的是面向数据的整合和一致性治理。</p><p>关于范式模型，也叫ER模型、实体模型。</p><h4 id="维度模型"><a href="#维度模型" class="headerlink" title="维度模型"></a>维度模型</h4><p>维度模型是Ralph Kimball所倡导，他的-The DataWarehouse Toolkit-The Complete Guide to Dimensona Modeling，是数据仓库工程领域的数据建模经典。按照书中所讲：</p><p><em>维度建模并不要求维度模型必须满足第3范式。数据库中强调的 3NF 主要是为了消除冗余。规范化的 3NF 将数据划分为多个不同的实体，每个实体构成一个关系表。比如说订单数据库，开始可能是每个订单中的一行表示一条记录，到后来为了满足 3NF会变成蜘蛛网状图，也许会包含上百个规范化表。而且对于 BI 查询来讲，规范化模型太复杂，用户会难以理解和记录这些模型的使用。 而维度建模解决了模式过分复杂的问题。</em></p><p>维度模型的典型代表是星形模型，以及一些特殊场景下使用的雪花模型和星座模型。</p><p>维度模型里面有两个非常重要的概念：事实表和维度表。</p><p>事实表：</p><p>发生在现实世界中的操作型事件，其所产生的可度量数值，存储在事实表中。从最低粒度级别来看，事实表行对应一个度量事件。</p><p>维度表：</p><p>每个维度表都包含单一的主键列。维度表的主键可以作为与之关联的任何事实表的外键，当然，维度表行的描述环境应与事实表行完全对应。维度表通常比较宽，是扁平型非规范表，包含大量的低粒度的文本属性。</p><h4 id="Data-Vault"><a href="#Data-Vault" class="headerlink" title="Data Vault"></a>Data Vault</h4><p>Data Vault是Dan Linstedt发起创建的一种模型方法论，现在应该叫做Data Vault 2.0了，它也是一套完整的数据仓库理论，其中也有专门的一部分关于数据模型设计。</p><p>Data Vault 模型，应该说是范式模型和维度模型的一种混合，它兼容了两种模型的优势。</p><p>Data Vault 通常可以分为三种类型：中心体，链接体和附属体。它主要由：Hub（中心表）、Link（链接表）和 Satellite（卫星表） 三部分组成 。</p><p><strong>中心表：</strong></p><p>中心表主要是存储一些日常用的业务关键码，比如客户号，发票号，流水号等。它包括三个要素：</p><ul><li>代理键：这就是一些操作性的组件，包括客户号，发票号等</li><li>装载时间戳：可以理解为ETL进行日加载的时间</li><li>数据源：就是可以追溯到的原系统，比如：CRM、ERP等</li></ul><p><strong>链接表：</strong></p><p>是3NF的多对多关系的物理表现形式，它表现的是多个业务键之间的关系。它和范式模型的最大区别是，将关系作为一个独立单元抽象出来，可以提升模型的扩展性。</p><p>它主要包含以下特征：</p><ul><li>代理键</li><li>代理键间的映射关系</li><li>装载时间戳：可以理解为ETL进行日加载的时间</li><li>数据源：就是可以追溯到的原系统，比如：CRM、ERP等</li></ul><p><strong>卫星表：</strong></p><p>业务领域中的其余信息可能随着时间而变化，所以卫星表必须有能力存储新的或者变化的各种粒度的数据，它们将被存储在卫星表内。</p><p>卫星表是中心表的详细描述内容，一个中心表会有多个卫星表，它由中心表的代理键、装载时间、来源类型、详细的中心表描述信息等组成。</p><p>关于数据模型，在实际的场景中会有很多个性化的设计，有时还不得不做一些反模式的设计。</p><h3 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h3><p>我们在进行维度建模的时候会建一张事实表，这个事实表就是星型模型的中心，然后会有一堆维度表，这些维度表就是向外发散的星星。</p><p><strong>事实表：</strong></p><p>发生在现实世界中的操作型事件，其所产生的可度量数值，存储在事实表中。从最低的粒度级别来看，事实表行对应一个度量事件。</p><p>比如是电商平台中的订单表，每条订单就相当于是一个事实，它在表中的体现就是一条记录。</p><p><strong>维度表：</strong></p><p>每个维度表都包含单一的主键列。维度表的主键可以作为与之关联的任何事实表的外键，当然，维度表行的描述环境应与事实表行完全对应。 维度表通常比较宽，是扁平型非规范表，包含大量的低粒度的文本属性。</p><p>比如关于订单表的商品类别表、供应商表等都属于维度表，这些表都有一个唯一的主键，然后在表中存放了相应维度的详细的数据信息。</p><p><strong>度量值：</strong></p><p>度量值是对一次行为的度量，可以是一个订单的订单金额等。</p><p>维度建模是一种十分优秀的建模方式，它有很多的优点，但是我们在实际工作中也很难完全按照它的方式来实现，都会有所取舍。</p><p>比如说，为了业务，我们还是会需要一些宽表，有时候还会有很多的数据冗余。</p>]]></content>
      
      
      <categories>
          
          <category> DataWarehouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive实现WordCount程序</title>
      <link href="/2018/06/23/hive/hive-wordcount/"/>
      <url>/2018/06/23/hive/hive-wordcount/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive实现WordCount"><a href="#Hive实现WordCount" class="headerlink" title="Hive实现WordCount"></a>Hive实现WordCount</h3><p>Hive的底层执行引擎是MapReduce，既然MR程序可以实现单词统计，那么Hive也可以，而且相对于MR编程实现来说更容易，只需要编写SQL语句即可。</p><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>创建数据库：</p><p><code>create database wordcount;</code></p><p>切换至wordcount数据库：</p><p><code>use wordcount;</code></p><p>建表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">lines</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">line <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">location <span class="string">'/hive_wordcount'</span>;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive_wordcount/words.txt'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">into</span> <span class="keyword">table</span> <span class="keyword">lines</span>;</span></pre></td></tr></table></figure><p>查看表中数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">lines</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|            lines.line            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| hello world                      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| hello hadoop                     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| hello hive                       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| this is hive word count example  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4 rows selected (0.162 seconds)</span></pre></td></tr></table></figure><h4 id="拆分单词"><a href="#拆分单词" class="headerlink" title="拆分单词"></a>拆分单词</h4><p>使用Hive内置的UDTF函数<code>explode(array)</code>，实现行转列：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 先创建表words以存放拆分后的单词</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> words(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">word <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 切分句子</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> words</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(line, <span class="string">" "</span>)) <span class="keyword">as</span> word </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> <span class="keyword">lines</span>;</span></pre></td></tr></table></figure><p>查看words中拆分后的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> words;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| words.word  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| hello       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| world       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| hello       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| hadoop      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| hello       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| hive        |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| this        |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| is          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| hive        |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| word        |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">| count       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">| example     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">12 rows selected (0.128 seconds)</span></pre></td></tr></table></figure><h4 id="实现WordCount"><a href="#实现WordCount" class="headerlink" title="实现WordCount"></a>实现WordCount</h4><p>使用<code>group by</code>实现Hive的wordcount：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> word,<span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">as</span> <span class="keyword">num</span> <span class="keyword">from</span> words <span class="keyword">group</span> <span class="keyword">by</span> word;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|   word   | num  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| count    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| example  | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| hadoop   | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| hello    | 3    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| hive     | 2    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| is       | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| this     | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| word     | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| world    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">9 rows selected (34.373 seconds)</span></pre></td></tr></table></figure><p>思路和MapReduce中的WordCount是一样的：先拆分成一个个的单词，然后对单词进行分组，最后累加。</p><p>上面拆分单词的过程也可以结合<code>lateral view</code>和<code>explode()</code>使用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> word </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> <span class="keyword">lines</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(line, <span class="string">" "</span>)) tmp <span class="keyword">as</span> word;</span></pre></td></tr></table></figure><p><code>lateral view</code>一般与UDTF函数（如<code>explode()</code>）结合使用，将UDTF应用于基表的每一行。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase与Hive、Phoenix的集成&amp;节点的管理</title>
      <link href="/2018/06/21/hbase/hbase-hive-phoenix-integration/"/>
      <url>/2018/06/21/hbase/hbase-hive-phoenix-integration/</url>
      
        <content type="html"><![CDATA[<h3 id="HBase与Hive的集成"><a href="#HBase与Hive的集成" class="headerlink" title="HBase与Hive的集成"></a>HBase与Hive的集成</h3><h4 id="HBase与Hive的对比"><a href="#HBase与Hive的对比" class="headerlink" title="HBase与Hive的对比"></a>HBase与Hive的对比</h4><table><thead><tr><th></th><th>Hive</th><th>HBase</th></tr></thead><tbody><tr><td>特点</td><td>类SQL数据仓库</td><td>NoSQL (key - value)</td></tr><tr><td>适用场景</td><td>离线数据分析和清洗</td><td>适合在线业务</td></tr><tr><td>延迟</td><td>延迟高</td><td>延迟低</td></tr><tr><td>存储位置</td><td>存储在HDFS</td><td>存储在HDFS</td></tr></tbody></table><h4 id="HBase与Hive集成使用"><a href="#HBase与Hive集成使用" class="headerlink" title="HBase与Hive集成使用"></a>HBase与Hive集成使用</h4><p>案例一：建立Hive表，关联HBase表，插入数据到Hive表的同时也能够影响HBase表。</p><ol><li><p>在Hive中创建表的同时关联HBase</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_hbase_emp_table1(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">empno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">ename <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">job <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">mgr <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">hiredate <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">sal <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">comm <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">deptno <span class="built_in">int</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"hbase_emp_table1"</span>);</span></pre></td></tr></table></figure><p>完成之后，可以分别进入Hive和HBase查看，都生成了对应的表。</p></li><li><p>在Hive中创建临时中间表，用于load文件中的数据</p><p>提示：不能将数据直接load进Hive所关联HBase的那张表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">empno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">ename <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">job <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">mgr <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">hiredate <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">sal <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">comm <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">deptno <span class="built_in">int</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure></li><li><p>向Hive中间表中load数据</p><p>hive&gt; <code>load data local inpath &#39;/home/vinx/data/emp.txt&#39; into table emp;</code></p></li><li><p>通过insert命令将中间表中的数据导入到Hive关联HBase的那张表中</p><p>hive&gt; <code>insert into table hive_hbase_emp_table1 select * from emp;</code></p></li><li><p>查看Hive以及关联的HBase表中是否已经成功的同步插入了数据</p><p>Hive：</p><p>hive&gt; <code>select * from hive_hbase_emp_table;</code></p><p>HBase：</p><p>hbase&gt; <code>scan &#39;hbase_emp_table&#39;</code></p></li></ol><p>案例二：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。</p><p>提示：所以完成此案例前，需先完成案例1。</p><ol><li><p>在Hive中创建外部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> relevance_hbase_emp(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">empno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">ename <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">job <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">mgr <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">hiredate <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">sal <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">comm <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">deptno <span class="built_in">int</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="string">":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno"</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"hbase_emp_table1"</span>);</span></pre></td></tr></table></figure></li><li><p>关联后就可以使用Hive函数进行一些分析操作了</p><p>hive (default)&gt; <code>select * from relevance_hbase_emp;</code></p></li></ol><h4 id="Sqoop集成：MySQL-to-HBase"><a href="#Sqoop集成：MySQL-to-HBase" class="headerlink" title="Sqoop集成：MySQL to HBase"></a>Sqoop集成：MySQL to HBase</h4><p>Sqoop supports additional import targets beyond HDFS and Hive. Sqoop can also import records into a table in HBase.</p><p>相关参数：</p><ul><li><p><code>--column-family&lt;family&gt;</code></p><p>Sets the target column family for the import<br>设置导入的目标列族。</p></li><li><p><code>--hbase-create-table</code></p><p>If specified, create missing HBase tables<br>是否自动创建不存在的HBase表（这就意味着，不需要手动提前在HBase中先建立表）</p></li><li><p><code>--hbase-row-key &lt;col&gt;</code></p><p>Specifies which input column to use as the row key.In case, if input table contains composite<br>key, then <col> must be in the form of a comma-separated list of composite key attributes.<br>mysql中哪一列的值作为HBase的rowkey，如果rowkey是个组合键，则以逗号分隔。（注：避免rowkey的重复）</p></li><li><p><code>--hbase-table &lt;table-name&gt;</code></p><p>Specifies an HBase table to use as the target instead of HDFS.<br>指定数据将要导入到HBase中的哪张表中。</p></li><li><p><code>--hbase-bulkload</code></p><p>Enables bulk loading.<br>是否允许bulk形式的导入。</p></li></ul><p>案例：将RDBMS中的数据抽取到HBase中。</p><ol><li><p>配置sqoop-env.sh，添加如下内容：</p><p><code>export HBASE_HOME=/home/vinx/app/hbase-1.3.1</code></p></li><li><p>在MySQL中新建一个数据库db_library，一张表book</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> db_library;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> db_library.book(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) PRIMARY <span class="keyword">KEY</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">name</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">price <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>);</span></pre></td></tr></table></figure></li><li><p>向表中插入一些数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> db_library.book (<span class="keyword">name</span>, price) <span class="keyword">VALUES</span>(<span class="string">'Lie Sporting'</span>, <span class="string">'30'</span>);  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> db_library.book (<span class="keyword">name</span>, price) <span class="keyword">VALUES</span>(<span class="string">'Pride &amp; Prejudice'</span>, <span class="string">'70'</span>);  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> db_library.book (<span class="keyword">name</span>, price) <span class="keyword">VALUES</span>(<span class="string">'Fall of Giants'</span>, <span class="string">'50'</span>);</span></pre></td></tr></table></figure></li><li><p>执行Sqoop导入数据的操作</p><p>手动创建HBase表：</p><p>hbase&gt; <code>create &#39;hbase_book&#39;,&#39;info&#39;</code></p></li><li><p>在HBase中scan这张表得到如下内容</p><p>hbase&gt; <code>scan &#39;hbase_book&#39;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ bin/sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/db_library \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--table book \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--columns <span class="string">"id,name,price"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--column-family <span class="string">"info"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--hbase-create-table \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--hbase-row-key <span class="string">"id"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">--hbase-table <span class="string">"hbase_book"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">--split-by id</span></pre></td></tr></table></figure><p>提示：sqoop1.4.6只支持HBase1.0.1之前的版本的自动创建HBase表的功能。</p></li></ol><h3 id="HBase集成Phoenix"><a href="#HBase集成Phoenix" class="headerlink" title="HBase集成Phoenix"></a>HBase集成Phoenix</h3><h4 id="Phoenix介绍"><a href="#Phoenix介绍" class="headerlink" title="Phoenix介绍"></a>Phoenix介绍</h4><p>可以把Phoenix理解为Hbase的查询引擎，phoenix，由saleforce.com开源的一个项目，后又捐给了Apache。它相当于一个Java中间件，帮助开发者，像使用jdbc访问关系型数据库一些，访问NoSql数据库HBase。</p><p>phoenix，操作的表及数据，存储在hbase上。phoenix只是需要和Hbase进行表关联起来，然后再用工具进行一些读或写操作。</p><p>其实，可以把Phoenix只看成一种代替HBase的语法的一个工具。虽然可以用java可以用jdbc来连接phoenix，然后操作HBase，但是在生产环境中，不可以用在OLTP中。在线事务处理的环境中，需要低延迟，而Phoenix在查询HBase时，虽然做了一些优化，但延迟还是不小。所以依然是用在OLAT中，再将结果返回存储下来。</p><h4 id="Phoenix的使用"><a href="#Phoenix的使用" class="headerlink" title="Phoenix的使用"></a>Phoenix的使用</h4><ul><li><p>启动Phoenix</p><p><code>sqlline.py hadoop001:2181</code></p></li><li><p>展示表</p><p><code>&gt; !table</code></p></li><li><p>创建表</p><p><code>&gt; create table test(id integer not null primary key,name varchar);</code></p></li><li><p>删除表</p><p><code>drop table test;</code></p></li><li><p>插入数据</p><p><code>&gt; upsert into test values(1,&#39;andy&#39;);</code></p><p><code>&gt; upsert into test(name) values(&#39;toms&#39;);</code></p></li><li><p>查询数据</p><p>phoenix &gt; <code>select * from test;</code></p><p>hbase &gt; <code>scan &#39;test&#39;</code></p></li><li><p>退出Phoenix</p><p><code>&gt; !q</code></p></li><li><p>删除数据</p><p><code>delete from test where id=4;</code></p></li><li><p>sum函数的使用</p><p><code>select sum(id) from test;</code></p></li><li><p>增加一列</p><p><code>alter table test add address varchar;</code></p></li><li><p>删除一列</p><p><code>alter table test drop column address;</code></p></li></ul><h4 id="表映射"><a href="#表映射" class="headerlink" title="表映射"></a>表映射</h4><p>hbase创建表：</p><p><code>create &#39;teacher&#39;,&#39;info&#39;,&#39;contact&#39;</code></p><p>插入数据：</p><p><code>put &#39;teacher&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Jack&#39;</code><br><code>put &#39;teacher&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;28&#39;</code><br><code>put &#39;teacher&#39;,&#39;1001&#39;,&#39;info:gender&#39;,&#39;male&#39;</code><br><code>put &#39;teacher&#39;,&#39;1001&#39;,&#39;contact:address&#39;,&#39;shanghai&#39;</code><br><code>put &#39;teacher&#39;,&#39;1001&#39;,&#39;contact:phone&#39;,&#39;13458646987&#39;</code></p><p><code>put &#39;teacher&#39;,&#39;1002&#39;,&#39;info:name&#39;,&#39;Jim&#39;</code><br><code>put &#39;teacher&#39;,&#39;1002&#39;,&#39;info:age&#39;,&#39;30&#39;</code><br><code>put &#39;teacher&#39;,&#39;1002&#39;,&#39;info:gender&#39;,&#39;male&#39;</code><br><code>put &#39;teacher&#39;,&#39;1002&#39;,&#39;contact:address&#39;,&#39;tianjian&#39;</code><br><code>put &#39;teacher&#39;,&#39;1002&#39;,&#39;contact:phone&#39;,&#39;13512436987&#39;</code></p><p>在Phoenix创建映射表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> <span class="string">"teacher"</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="string">"ROW"</span> <span class="built_in">varchar</span> primary <span class="keyword">key</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="string">"contact"</span>.<span class="string">"address"</span> <span class="built_in">varchar</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="string">"contact"</span>.<span class="string">"phone"</span> <span class="built_in">varchar</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="string">"info"</span>.<span class="string">"age"</span>  <span class="built_in">varchar</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="string">"info"</span>.<span class="string">"gender"</span> <span class="built_in">varchar</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="string">"info"</span>.<span class="string">"name"</span> <span class="built_in">varchar</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure><p>在Phoenix查找数据：</p><p><code>select * from &quot;teacher&quot;;</code></p><h3 id="节点的管理"><a href="#节点的管理" class="headerlink" title="节点的管理"></a>节点的管理</h3><h4 id="服役（commissioning）"><a href="#服役（commissioning）" class="headerlink" title="服役（commissioning）"></a>服役（commissioning）</h4><p>当启动regionserver时，regionserver会向HMaster注册并开始接收本地数据，开始的时候，新加入的节点不会有任何数据，平衡器开启的情况下，将会有新的region移动到开启的RegionServer上。如果启动和停止进程是使用ssh和HBase脚本，那么会将新添加的节点的主机名加入到conf/regionservers文件中。</p><ol><li><code>$ ./bin/hbase-daemon.sh stop regionserver</code></li><li>hbase(main)&gt;<code>balance_switch true</code></li></ol><h4 id="退役（decommissioning）"><a href="#退役（decommissioning）" class="headerlink" title="退役（decommissioning）"></a>退役（decommissioning）</h4><p>顾名思义，就是从当前HBase集群中删除某个RegionServer，这个过程分为如下几个过程：</p><p>在0.90.2之前，我们只能通过在要卸载的节点上执行：</p><ol><li><p>停止负载平衡器</p><p>hbase&gt; balance_switch false</p></li><li><p>在退役节点上停止RegionServer</p><p>[vinx@hadoop001 hbase-1.3.1] <code>hbase-daemon.sh stop regionserver</code></p></li><li><p>RegionServer一旦停止，会关闭维护的所有region</p></li><li><p>Zookeeper上的该RegionServer节点消失</p></li><li><p>Master节点检测到该RegionServer下线，开启平衡器</p><p>hbase&gt; <code>balance_switch true</code></p></li><li><p>下线的RegionServer的region服务得到重新分配</p><p>这种方法很大的一个缺点是该节点上的Region会离线很长时间。因为假如该RegionServer上有大量Region的话，因为Region的关闭是顺序执行的，第一个关闭的Region得等到和最后一个Region关闭并Assigned后一起上线。这是一个相当漫长的时间。每个Region Assigned需要4s，也就是说光Assigned就至少需要2个小时。该关闭方法比较传统，需要花费一定的时间，而且会造成部分region短暂的不可用。</p></li></ol><p>另一种方案：自0.90.2之后，HBase添加了一个新的方法，即“graceful_stop”,只需要在HBase Master节点执行</p><ol><li><p>Master节点执行</p><p><code>$ bin/graceful_stop.sh &lt;RegionServer-hostname&gt;</code></p><p>该命令会自动关闭Load Balancer，然后Assigned Region，之后会将该节点关闭。除此之外，你还可以查看remove的过程，已经assigned了多少个Region，还剩多少个Region，每个Region 的Assigned耗时</p></li><li><p>开启负载平衡器</p><p>hbase&gt; <code>balance_switch false</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase的读写流程&amp;HBase API</title>
      <link href="/2018/06/20/hbase/hbase-read-write-process/"/>
      <url>/2018/06/20/hbase/hbase-read-write-process/</url>
      
        <content type="html"><![CDATA[<h3 id="HBase读数据流程"><a href="#HBase读数据流程" class="headerlink" title="HBase读数据流程"></a>HBase读数据流程</h3><ol><li>HRegionServer保存着.META.的这样一张表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面找到.META.表所在的位置信息，即找到这个.META.表在哪个HRegionServer上保存着。</li><li>接着Client通过刚才获取到的HRegionServer的IP来访问.META.表所在的HRegionServer，从而读取到.META.，进而获取到.META.表中存放的元数据。</li><li>Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描(scan)所在HRegionServer的Memstore和Storefile来查询数据。</li></ol><h3 id="HBase写数据流程"><a href="#HBase写数据流程" class="headerlink" title="HBase写数据流程"></a>HBase写数据流程</h3><ol><li>Client也是先访问zookeeper，找到-ROOT-表，进而找到.META.表，并获取.META.表信息。</li><li>确定当前将要写入的数据所对应的RegionServer服务器和Region。</li><li>Client向该RegionServer服务器发起写入数据请求，然后RegionServer收到请求并响应。</li><li>Client先把数据写入到HLog，以防止数据丢失。</li><li>然后将数据写入到Memstore。</li><li>如果Hlog和Memstore均写入成功，则这条数据写入成功。在此过程中，如果Memstore达到阈值，会把Memstore中的数据flush到StoreFile中。</li><li>当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile。当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。</li></ol><p>提示：因为内存空间是有限的，所以说溢写过程必定伴随着大量的小文件产生。</p><h3 id="HBase-API"><a href="#HBase-API" class="headerlink" title="HBase API"></a>HBase API</h3><p>首先需要获取Configuration对象：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Configuration conf;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//使用HBaseConfiguration的单例方法实例化</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    conf = HBaseConfiguration.create();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    conf.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"hadoop001"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    conf.set(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    conf.set(<span class="string">"zookeeper.znode.parent"</span>, <span class="string">"/hbase"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>判断表是否存在：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isTableExist</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> MasterNotRunningException, ZooKeeperConnectionException, IOException</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//在HBase中管理、访问表需要先创建HBaseAdmin对象</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    Connection connection = ConnectionFactory.createConnection(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//HBaseAdmin admin = new HBaseAdmin(conf);</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> admin.tableExists(tableName);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>创建表：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String tableName, String... columnFamily)</span> <span class="keyword">throws</span> MasterNotRunningException, ZooKeeperConnectionException, IOException</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//判断表是否存在</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span>(isTableExist(tableName))&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        System.out.println(<span class="string">"表"</span> + tableName + <span class="string">"已存在"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//System.exit(0);</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    &#125;<span class="keyword">else</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//创建表属性对象,表名需要转字节</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        HTableDescriptor descriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(tableName));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//创建多个列族</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span>(String cf : columnFamily)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">            descriptor.addFamily(<span class="keyword">new</span> HColumnDescriptor(cf));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//根据对表的配置，创建表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        admin.createTable(descriptor);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        System.out.println(<span class="string">"表"</span> + tableName + <span class="string">"创建成功！"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>删除表：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dropTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(isTableExist(tableName))&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">admin.disableTable(tableName);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">admin.deleteTable(tableName);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"表"</span> + tableName + <span class="string">"删除成功！"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;<span class="keyword">else</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"表"</span> + tableName + <span class="string">"不存在！"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>向表中插入数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addRowData</span><span class="params">(String tableName, String rowKey, String columnFamily, String column, String value)</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建HTable对象</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//向表中插入数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//向Put对象中组装数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">hTable.put(put);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">hTable.close();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"插入数据成功"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>删除多行数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteMultiRow</span><span class="params">(String tableName, String... rows)</span> <span class="keyword">throws</span> IOException</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">List&lt;Delete&gt; deleteList = <span class="keyword">new</span> ArrayList&lt;Delete&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(String row : rows)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(row));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">deleteList.add(delete);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">hTable.delete(deleteList);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">hTable.close();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>得到所有数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getAllRows</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> IOException</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">HTable hTable = <span class="keyword">new</span> HTable(conf, tableName);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//得到用于扫描region的对象</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用HTable得到resultcanner实现类的对象</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">ResultScanner resultScanner = hTable.getScanner(scan);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(Result result : resultScanner)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">Cell[] cells = result.rawCells();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(Cell cell : cells)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//得到rowkey</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"行键:"</span> + Bytes.toString(CellUtil.cloneRow(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//得到列族</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"列族"</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"列:"</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"值:"</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>得到某一行所有数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getRow</span><span class="params">(String tableName, String rowKey)</span> <span class="keyword">throws</span> IOException</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//get.setMaxVersions();显示所有版本</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//get.setTimeStamp();显示指定时间戳的版本</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Result result = table.get(get);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"行键:"</span> + Bytes.toString(result.getRow()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"列族"</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"列:"</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"值:"</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"时间戳:"</span> + cell.getTimestamp());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>获取某一行指定<code>列族:列</code>的数据：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getRowQualifier</span><span class="params">(String tableName, String rowKey, String family, String qualifier)</span> <span class="keyword">throws</span> IOException</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Result result = table.get(get);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(Cell cell : result.rawCells())&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"行键:"</span> + Bytes.toString(result.getRow()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"列族"</span> + Bytes.toString(CellUtil.cloneFamily(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"列:"</span> + Bytes.toString(CellUtil.cloneQualifier(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">"值:"</span> + Bytes.toString(CellUtil.cloneValue(cell)));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase的RowKey设计</title>
      <link href="/2018/06/18/hbase/hbase-rowkey/"/>
      <url>/2018/06/18/hbase/hbase-rowkey/</url>
      
        <content type="html"><![CDATA[<h3 id="RowKey的重要性"><a href="#RowKey的重要性" class="headerlink" title="RowKey的重要性"></a>RowKey的重要性</h3><p>RowKey的特点：</p><ul><li>类似于MySQL、Oracle中的主键，用于标示唯一的行；</li><li>完全是由用户指定的一串不重复的字符串；</li><li>HBase中的数据永远是根据 Rowkey 的字典排序来排序的。</li></ul><p>RowKey的作用：</p><ul><li>读写数据时通过RowKey找到对应的Region；</li><li>MemStore中的数据按RowKey字典顺序排序；</li><li>HFile中的数据按RowKey字典顺序排序。</li></ul><p>RowKey对查询的影响：</p><p>如果RowKey设计为uid+phone+name，那么这种设计可以很好的支持以下的场景：</p><ul><li>uid = 111 AND phone = 123 AND name = tom</li><li>uid = 111 AND phone = 123</li><li>uid = 111 AND phone = 12?</li><li>uid = 111</li></ul><p>难以支持的场景：</p><ul><li>phone = 123 AND name = tom</li><li>phone = 123</li><li>name = tom</li></ul><p>RowKey对Region划分的影响：</p><p>HBase 表的数据是按照 Rowkey 来分散到不同 Region，不合理的 Rowkey 设计会导致热点问题。热点问题是大量的 Client 直接访问集群的一个或极少数个节点，而集群中的其他节点却处于相对空闲状态。</p><h3 id="RowKey设计"><a href="#RowKey设计" class="headerlink" title="RowKey设计"></a>RowKey设计</h3><p>一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内。</p><p>设计rowkey的主要目的，就是让数据均匀地分布于所有的region中，在一定程度上防止数据倾斜。</p><h4 id="Salting"><a href="#Salting" class="headerlink" title="Salting"></a>Salting</h4><p>这里的加盐不是密码学中的加盐，而是在rowkey 的前面增加随机数。具体就是给 rowkey 分配一个随机前缀 以使得它和之前排序不同。分配的前缀种类数量应该和你想使数据分散到不同的 region 的数量一致。 如果你有一些 热点 rowkey 反复出现在其他分布均匀的 rwokey 中，加盐是很有用的。考虑下面的例子：它将写请求分散到多个 RegionServers，但是对读造成了一些负面影响。</p><p>假如你有下列 rowkey，你表中每一个 region 对应字母表中每一个字母。 以 ‘a’ 开头是同一个region, ‘b’开头的是同一个region。在表中，所有以 ‘f’开头的都在同一个 region， 它们的 rowkey 像下面这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">foo0001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">foo0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">foo0003</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">foo0004</span></pre></td></tr></table></figure><p>现在，假如你需要将上面这个 region 分散到 4个 region。你可以用4个不同的盐：’a’, ‘b’, ‘c’, ‘d’.在这个方案下，每一个字母前缀都会在不同的 region 中。加盐之后，你有了下面的 rowkey:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">a-foo0003</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">b-foo0001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">c-foo0004</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">d-foo0002</span></pre></td></tr></table></figure><p>所以，你可以向4个不同的 region 写。理论上说，如果这四个 Region 存放在不同的机器上，经过加盐之后你将拥有之前4倍的吞吐量。</p><p>现在，如果再增加一行，它将随机分配a,b,c,d中的一个作为前缀，并以一个现有行作为尾部结束：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">a-foo0003</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">b-foo0001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">c-foo0003</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">c-foo0004</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">d-foo0002</span></pre></td></tr></table></figure><p>因为分配是随机的，所以如果你想要以字典序取回数据，你需要做更多工作。加盐这种方式增加了写时的吞吐量，但是当读时有了额外代价。</p><h4 id="Hashing"><a href="#Hashing" class="headerlink" title="Hashing"></a>Hashing</h4><p>Hashing 的原理是计算 RowKey 的 hash 值，然后取 hash 的部分字符串和原来的 RowKey 进行拼接。这里说的 hash 包含 MD5、sha1、sha256或sha512等算法。比如我们有如下的 RowKey：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">foo0001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">foo0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">foo0003</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">foo0004</span></pre></td></tr></table></figure><p>我们使用 md5 计算这些 RowKey 的 hash 值，然后取前 6 位和原来的 RowKey 拼接得到新的 RowKey：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">95f18cfoo0001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">6ccc20foo0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">b61d00foo0003</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">1a7475foo0004</span></pre></td></tr></table></figure><p>优缺点：可以一定程度打散整个数据集，但是不利于 Scan；比如我们使用 md5 算法，来计算Rowkey的md5值，然后截取前几位的字符串。subString(MD5(设备ID), 0, x) + 设备ID，其中x一般取5或6。</p><h4 id="Reversing"><a href="#Reversing" class="headerlink" title="Reversing"></a>Reversing</h4><p>Reversing 的原理是反转一段固定长度或者全部的键。比如我们有以下 URL ，并作为 RowKey：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">flink.github.com</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">www.github.com</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">hadoop.github.com</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">spark.github.com</span></pre></td></tr></table></figure><p>这些 URL 其实属于同一个域名，但是由于前面不一样，导致数据不在一起存放。我们可以对其进行反转，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">moc.buhtig.knilf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">moc.buhtig.www</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">moc.buhtig.poodah</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">moc.buhtig.kraps</span></pre></td></tr></table></figure><p>经过这个之后，这些 URL 的数据就可以放一起了。</p><h4 id="RowKey的长度"><a href="#RowKey的长度" class="headerlink" title="RowKey的长度"></a>RowKey的长度</h4><p>RowKey 可以是任意的字符串，最大长度64KB（因为 Rowlength 占2字节）。建议越短越好，原因如下：</p><ul><li>数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率；</li><li>MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率；</li><li>目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。</li></ul>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase基本概念和使用</title>
      <link href="/2018/06/16/hbase/hbase-basic/"/>
      <url>/2018/06/16/hbase/hbase-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="HBase简介"><a href="#HBase简介" class="headerlink" title="HBase简介"></a>HBase简介</h3><p>HBase是一个分布式的、面向列的开源数据库，它是一个适合于非结构化数据存储的数据库。</p><p>HBase是基于列的而不是基于行的模式。</p><p>面向列：面向列（族）的存储和权限控制，列（族）独立检索。</p><p>稀疏：对于为空（null）的列，并不占用存储空间，因此，表的设计非常的稀疏。</p><h3 id="HBase的角色"><a href="#HBase的角色" class="headerlink" title="HBase的角色"></a>HBase的角色</h3><h4 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h4><ol><li>监控RegionServer</li><li>处理RegionServer故障转移</li><li>处理元数据的变更</li><li>处理region的分配或移除</li><li>在空闲时间进行数据的负载均衡</li><li>通过zookeeper发布自己的位置给客户端</li></ol><h4 id="HRegionServer"><a href="#HRegionServer" class="headerlink" title="HRegionServer"></a>HRegionServer</h4><ol><li>负责存储HBase的实际数据</li><li>处理分配给它的Region</li><li>刷新缓存到HDFS</li><li>维护HLog</li><li>执行压缩</li><li>负责处理Region分片</li></ol><h5 id="Write-Ahead-Logs"><a href="#Write-Ahead-Logs" class="headerlink" title="Write-Ahead Logs"></a>Write-Ahead Logs</h5><p>HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。</p><p>但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p><h5 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h5><p>这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。</p><h5 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h5><p>HFile存储在Store中，一个Store对应HBase表中的一个列族。</p><h5 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h5><p>内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegionServer会在内存中存储键值对。</p><h5 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h5><p>HBase表的分片，HBase表会根据rowkey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。</p><h3 id="HBase的架构"><a href="#HBase的架构" class="headerlink" title="HBase的架构"></a>HBase的架构</h3><p>一个RegionServer可以包含多个HRegion，每个RegionServer维护一个HLog，和多个HFiles以及其对应的MemStore。RegionServer运行于DataNode上，数量可以与DatNode数量一致，请参考如下架构图：</p><p><img src="https://vinxikk.github.io/img/hbase/hbase-architecture.png" alt="HBase的架构"></p><h3 id="HBase数据模型"><a href="#HBase数据模型" class="headerlink" title="HBase数据模型"></a>HBase数据模型</h3><p>确定一个单元格（cell）的位置，需要如下四个：</p><p>rowkey + column family + column + timestamp(version版本)，数据有版本的概念，即一个单元格可能有多个值，但是只有最新的一个对外显示。</p><ul><li>HBase中有两张特殊的Table，-ROOT-和.META.</li><li>.META.：记录了用户表的Region信息，.META.可以有多个region</li><li>-ROOT-：记录了.META.表的Region信息，-ROOT-只有一个region</li><li>Zookeeper中记录了-ROOT-表的location</li><li>Client访问用户数据之前需要首先访问zookeeper，然后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过client端会做cache缓存（在0.96版本后，Hbase移除了-ROOT-表）。</li></ul><p><img src="https://vinxikk.github.io/img/hbase/hbase-read-data.png" alt="访问数据流程"></p><h4 id="RowKey"><a href="#RowKey" class="headerlink" title="RowKey"></a>RowKey</h4><p>行键，Table的主键，Table中的记录默认按照rowkey升序排序。</p><h4 id="Timestamp"><a href="#Timestamp" class="headerlink" title="Timestamp"></a>Timestamp</h4><p>时间戳，每次数据操作对应的时间戳，可以看作是数据的version number。</p><h4 id="Column-Family"><a href="#Column-Family" class="headerlink" title="Column Family"></a>Column Family</h4><p>列族，Table在水平方向有一个或者多个Column Family组成，一个Column Family中可以由多个Column组成，即Column Family支持动态扩展，无需预先定义Column的数量以及类型，所有Column均以二进制格式存储，用户需要自行进行类型转换。</p><h4 id="Table-amp-Region"><a href="#Table-amp-Region" class="headerlink" title="Table&amp;Region"></a>Table&amp;Region</h4><p>当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由<code>[startkey,endkey)</code>表示，不同的region会被Master分配给相应的RegionServer进行管理。</p><h4 id="HMaster-1"><a href="#HMaster-1" class="headerlink" title="HMaster"></a>HMaster</h4><p>HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作：</p><ol><li>管理用户对Table的增、删、改、查操作</li><li>管理HRegionServer的负载均衡，调整Region分布</li><li>在Region Split后，负责新Region的分配</li><li>在HRegionServer停机后，负责失效HRegionServer上的Regions迁移</li></ol><h4 id="HRegionServer-1"><a href="#HRegionServer-1" class="headerlink" title="HRegionServer"></a>HRegionServer</h4><p>HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。</p><p>每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元，因此最好将具备共同IO特性的column放在一个Column Family中，这样最高效。</p><h4 id="MemStore-amp-StoreFiles"><a href="#MemStore-amp-StoreFiles" class="headerlink" title="MemStore&amp;StoreFiles"></a>MemStore&amp;StoreFiles</h4><p>HStore存储是HBase存储的核心了，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。</p><p>MemStore是Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile），当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要进入内存中就可以立即返回，保证了HBase I/O的高性能。</p><p>当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer上，使得原先1个Region的压力得以分流到2个Region上。</p><h4 id="HLog"><a href="#HLog" class="headerlink" title="HLog"></a>HLog</h4><p>每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中，HLog文件定期会滚动出新的，并删除旧的文件（已持久化到StoreFile中的数据）。</p><p>当HRegionServer意外终止后，HMaster会通过Zookeeper感知到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。</p><h4 id="文件类型"><a href="#文件类型" class="headerlink" title="文件类型"></a>文件类型</h4><p>HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型：</p><ol><li>HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile。</li><li>HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File。</li></ol><p>Zookeeper中hbase的节点的存储信息：</p><ul><li>rs：regionserver节点信息</li><li>table-lock：hbase的除meta以外的所有表</li><li>Table：hbase的所有的表</li></ul><h3 id="HBase的使用"><a href="#HBase的使用" class="headerlink" title="HBase的使用"></a>HBase的使用</h3><h4 id="HBase服务的启动"><a href="#HBase服务的启动" class="headerlink" title="HBase服务的启动"></a>HBase服务的启动</h4><p>启动方式1：</p><p><code>bin/hbase-daemon.sh start master</code></p><p><code>bin/hbase-daemon.sh start regionserver</code></p><p>提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。</p><p>启动方式2：</p><p><code>bin/start-hbase.sh</code></p><p>对应的停止服务：</p><p><code>bin/stop-hbase.sh</code></p><h4 id="查看HBase页面"><a href="#查看HBase页面" class="headerlink" title="查看HBase页面"></a>查看HBase页面</h4><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：</p><p><a href="http://hadoop001:16010" target="_blank" rel="noopener">http://hadoop001:16010</a></p><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4><ul><li><p>进入HBase客户端命令行</p><p><code>bin/hbase shell</code></p></li><li><p>查看帮助命令</p><p>hbase(main)&gt; <code>help</code></p></li><li><p>查看当前数据库中有哪些表</p><p>hbase(main)&gt; <code>list</code></p></li><li><p>查看当前数据库中有哪些命名空间</p><p>hbase(main)&gt; <code>list_namespace</code></p></li></ul><h4 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h4><ul><li><p>创建表</p><p>hbase(main)&gt; <code>create &#39;student&#39;,&#39;info&#39;</code></p><p>hbase(main)&gt; <code>create &#39;iparkmerchant_order&#39;,&#39;smzf&#39;</code></p><p>hbase(main)&gt; <code>create &#39;staff&#39;,&#39;info&#39;</code></p></li><li><p>插入数据到表</p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Thomas&#39;</code></p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:sex&#39;,&#39;male&#39;</code></p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;18&#39;</code></p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1002&#39;,&#39;info:name&#39;,&#39;Janna&#39;</code></p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1002&#39;,&#39;info:sex&#39;,&#39;female&#39;</code></p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1002&#39;,&#39;info:age&#39;,&#39;20&#39;</code></p><p>数据插入后的数据模型：</p><p><img src="https://vinxikk.github.io/img/hbase/hbase-student-data-model.png" alt="student表的数据模型"></p></li><li><p>扫描查看表数据</p><p>hbase(main) &gt; <code>scan &#39;student&#39;</code></p><p>hbase(main) &gt; <code>scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;, STOPROW  =&gt; &#39;1001&#39;}</code></p><p>hbase(main) &gt; <code>scan &#39;student&#39;,{STARTROW =&gt; &#39;1001&#39;}</code>  从哪一个rowkey开始扫描</p></li><li><p>查看表结构</p><p>hbase(main):&gt; <code>desc &#39;student&#39;</code></p></li><li><p>更新指定字段的数据</p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;Nick&#39;</code></p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:age&#39;,&#39;100&#39;</code></p><p>hbase(main) &gt; <code>put &#39;student&#39;,&#39;1001&#39;,&#39;info:isNull&#39;,&#39;&#39;</code>（仅测试空值问题）</p></li><li><p>查看<code>指定行</code>或<code>指定列族:列</code>的数据</p><p>hbase(main) &gt; <code>get &#39;student&#39;,&#39;1001&#39;</code></p><p>hbase(main) &gt; <code>get &#39;student&#39;,&#39;1001&#39;,&#39;info:name&#39;</code></p></li><li><p>删除数据</p><p>删除某rowkey的全部数据：</p><p>hbase(main) &gt; <code>deleteall &#39;student&#39;,&#39;1001&#39;</code></p></li><li><p>清空表数据</p><p>hbase(main) &gt; <code>truncate &#39;student&#39;</code></p><p>提示：清空表的操作顺序为先disable，然后再truncate。</p></li><li><p>删除表</p><p>首先需要先让该表为disable状态：</p><p>hbase(main) &gt; <code>disable &#39;student&#39;</code></p><p>检查这个表是否被禁用：</p><p>hbase(main) &gt; <code>is_enabled &#39;hbase_book&#39;</code></p><p>hbase(main) &gt; <code>is_disabled &#39;hbase_book&#39;</code></p><p>然后才能drop这个表：</p><p>hbase(main) &gt; <code>drop &#39;student&#39;</code></p><p>提示：如果直接drop表，会报错：Drop the named table. Table must first be disabled<br>ERROR: Table student is enabled. Disable it first.</p><p>恢复被禁用的表：</p><p><code>enable &#39;student&#39;</code></p></li><li><p>统计表数据行数</p><p>hbase(main) &gt; <code>count &#39;student&#39;</code></p></li><li><p>变更表信息</p><p>将info列族中的数据存放3个版本：</p><p>hbase(main) &gt; <code>alter &#39;student&#39;,{NAME=&gt;&#39;info&#39;,VERSIONS=&gt;3}</code></p><p>查看student的最新版本的数据：</p><p>hbase(main) &gt; <code>get &#39;student&#39;,&#39;1001&#39;</code></p><p>查看HBase中的多版本：</p><p>hbase(main) &gt; <code>get &#39;student&#39;,&#39;1001&#39;,{COLUMN=&gt;&#39;info:name&#39;,VERSIONS=&gt;10}</code></p></li></ul><h4 id="常用shell操作"><a href="#常用shell操作" class="headerlink" title="常用shell操作"></a>常用shell操作</h4><ul><li><p>status：显示服务器状态</p><p>hbase&gt; <code>status &#39;hadoop001&#39;</code></p></li><li><p>exists：检查表是否存在，适用于表量特别多的情况</p><p>hbase&gt; <code>exists &#39;hbase_book&#39;</code></p></li><li><p>is_enabled/is_disabled：检查表是否启用或禁用</p><p>hbase&gt; <code>is_enabled &#39;hbase_book&#39;</code></p><p>hbase&gt; <code>is_disabled &#39;hbase_book&#39;</code></p></li><li><p>alter：该命令可以改变表和列族的模式</p><p>为当前表增加列族：</p><p>hbase&gt; <code>alter &#39;hbase_book&#39;, NAME =&gt; &#39;CF2&#39;, VERSIONS =&gt; 2</code></p><p>为当前表删除列族：</p><p>hbase&gt; <code>alter &#39;hbase_book&#39;, &#39;delete&#39; =&gt; &#39;CF2&#39;</code></p></li><li><p>drop：删除一张表</p><p>hbase&gt; <code>disable &#39;hbase_book&#39;</code></p><p>hbase&gt; <code>drop &#39;hbase_book&#39;</code></p></li><li><p>delete：删除一行中一个单元格的值</p><p>hbase&gt; <code>delete &#39;hbase_book&#39;, &#39;rowKey&#39;, &#39;CF:C&#39;</code></p></li><li><p>truncate：清空表数据，即禁用表 =&gt; 删除表 =&gt; 创建表</p><p>hbase&gt; <code>truncate &#39;hbase_book&#39;</code></p></li><li><p>create：创建多个列族</p><p>hbase&gt; <code>create &#39;t1&#39;, {NAME =&gt; &#39;f1&#39;}, {NAME =&gt; &#39;f2&#39;}, {NAME =&gt; &#39;f3&#39;}</code></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume之Log4J Appender：采集log4j日志到控制台</title>
      <link href="/2018/06/14/flume/flume-log4j-appender/"/>
      <url>/2018/06/14/flume/flume-log4j-appender/</url>
      
        <content type="html"><![CDATA[<h4 id="Flume采集log4j日志"><a href="#Flume采集log4j日志" class="headerlink" title="Flume采集log4j日志"></a>Flume采集log4j日志</h4><p>目标：使用flume采集log4j日志，在logger控制台输出，即log4j日志 =&gt; Log4J Appender。</p><h4 id="Mock代码及log4j-properties"><a href="#Mock代码及log4j-properties" class="headerlink" title="Mock代码及log4j.properties"></a>Mock代码及log4j.properties</h4><p>Java Mock数据的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ivinx.flume;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMock</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = Logger.getLogger(LogMock<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">int</span> num = <span class="number">1000</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        logMock(num);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">logMock</span><span class="params">(<span class="keyword">int</span> num)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">10</span>; i &lt; num; i ++) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">            LOGGER.info(<span class="string">"LogMock Info ["</span> + i + <span class="string">"]"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">            Thread.sleep(<span class="number">1000</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>pom.xml引入如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.10<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume.flume-ng-clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-log4jappender<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span></pre></td></tr></table></figure><p>本地log4j配置文件如下（resources文件夹下）：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 日志级别优先级排序: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootCategory</span>=<span class="string">info, console</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Log4J Appender</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.flume</span> = <span class="string">org.apache.flume.clients.log4jappender.Log4jAppender</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.flume.Hostname</span> = <span class="string">hadoop002</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.flume.Port</span> = <span class="string">41414</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.flume.UnsafeMode</span> = <span class="string">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.flume.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.flume.layout.ConversionPattern</span>=<span class="string">%d&#123;yyyy-MM-dd HH:mm:ss&#125; %p [%c:%L] - %m%n</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># configure a class's logger to output to the flume appender</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.logger.com.ivinx.flume</span> = <span class="string">DEBUG,flume</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># appender console</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.console</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.console.target</span>=<span class="string">System.out</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.console.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.appender.console.layout.ConversionPattern</span>=<span class="string">%d [%-5p] [%t] - [%l] %m%n</span></span></pre></td></tr></table></figure><p>备注：其中hostname为flume安装的服务器IP，port为端口与下面的flume的监听端口相对应。</p><h4 id="配置flume-agent"><a href="#配置flume-agent" class="headerlink" title="配置flume agent"></a>配置flume agent</h4><p>新建配置文件<code>vi flume-log4j-logger.conf</code>，编辑以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">a1.sources&#x3D;source1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a1.channels&#x3D;channel1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a1.sinks&#x3D;sink1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">a1.sources.source1.type&#x3D;avro</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">a1.sources.source1.bind&#x3D;hadoop002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">a1.sources.source1.port&#x3D;41414</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">a1.sources.source1.channels &#x3D; channel1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">a1.channels.channel1.type&#x3D;memory</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">a1.channels.channel1.capacity&#x3D;10000</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">a1.channels.channel1.transactionCapacity&#x3D;1000</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">a1.channels.channel1.keep-alive&#x3D;30</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">a1.sinks.sink1.type &#x3D; logger</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">a1.sinks.sink1.channel &#x3D; channel1</span></pre></td></tr></table></figure><p>如上配置，flume服务器运行在hadoop002上，并且监听的端口为41414，在log4j中只需要将日志发送到hadoop002的41414端口就能成功的发送到flume上。</p><p>flume会监听并收集该端口上的数据信息，然后将它转化成logger event，直接在控制台输出。</p><h4 id="启动flume并测试"><a href="#启动flume并测试" class="headerlink" title="启动flume并测试"></a>启动flume并测试</h4><p>测试步骤：</p><ol><li><p>启动flume：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--name a1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--conf <span class="variable">$FLUME_HOME</span>/conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--conf-file <span class="variable">$FLUME_HOME</span>/script/flume-log4j-logger.conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-Dflume.root.logger=INFO,console</span></pre></td></tr></table></figure></li><li><p>运行LogMock类的main方法打印日志</p></li><li><p>在hadoop002的控制台查看sink的数据</p></li></ol><hr><p>参考：</p><p><a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#log4j-appender" target="_blank" rel="noopener">http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#log4j-appender</a></p><p><a href="https://www.cnblogs.com/dreammyle/p/6595693.html" target="_blank" rel="noopener">https://www.cnblogs.com/dreammyle/p/6595693.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume的概念、部署和使用</title>
      <link href="/2018/06/12/flume/flume-getting-started/"/>
      <url>/2018/06/12/flume/flume-getting-started/</url>
      
        <content type="html"><![CDATA[<h3 id="Flume简介"><a href="#Flume简介" class="headerlink" title="Flume简介"></a>Flume简介</h3><p>官网：<a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p><p>官方描述：</p><p>Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application.</p><p>Flume介绍：</p><ol><li>Flume提供一个分布式的，可靠的，对大数据量的日志进行高效收集、聚集、移动的服务，Flume只能在Unix环境下运行。</li><li>Flume基于流式架构，容错性强，也很灵活简单。</li><li>Flume、Kafka用来实时进行数据收集，Spark、Flink用来实时处理数据，impala用来实时查询。</li></ol><h3 id="Flume的角色"><a href="#Flume的角色" class="headerlink" title="Flume的角色"></a>Flume的角色</h3><p><img src="https://vinxikk.github.io/img/flume/flume-architecture.png" alt="Flume的数据流模型"></p><p><strong>Source：</strong></p><p>用于采集数据，Source是产生数据流的地方，同时Source会将产生的数据流传输到Channel，这个有点类似于Java IO部分的Channel。</p><p><strong>Channel：</strong></p><p>用于桥接Sources和Sinks，类似于一个队列。</p><p><strong>Sink：</strong></p><p>从Channel收集数据，将数据写到目标源(可以是下一个Source，也可以是HDFS或者HBase)。</p><p><strong>Event：</strong></p><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。</p><h4 id="Flume传输过程"><a href="#Flume传输过程" class="headerlink" title="Flume传输过程"></a>Flume传输过程</h4><p>source监控某个文件或数据流，数据源产生新的数据，拿到该数据后，将数据封装在一个Event中，并put到channel后commit提交，channel队列先进先出，sink去channel队列中拉取数据，然后写入到HDFS中。</p><h3 id="Flume部署"><a href="#Flume部署" class="headerlink" title="Flume部署"></a>Flume部署</h3><p>Flume版本选择：<code>wget http://archive.cloudera.com/cdh5/cdh/5/flume-ng-1.6.0-cdh5.16.2.tar.gz</code></p><p>解压：<code>tar -zxvf flume-ng-1.6.0-cdh5.16.2.tar.gz -C ~/app/</code></p><p>设置软连接：<code>ln -s apache-flume-1.6.0-cdh5.16.2-bin flume</code></p><p>修改配置文件：</p><ul><li><code>cp flume-env.sh.template flume-env.sh</code></li><li><code>echo $JAVA_HOME</code></li><li><code>vi flume-env.sh</code></li></ul><p>追加以下内容：</p><p><code>export JAVA_HOME=/usr/java/jdk1.8.0_91</code></p><p>配置环境变量：<code>vi .bashrc</code></p><p>追加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">export FLUME_HOME&#x3D;&#x2F;home&#x2F;vinx&#x2F;app&#x2F;flume</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">export PATH&#x3D;$FLUME_HOME&#x2F;bin:$PATH</span></pre></td></tr></table></figure><p>生效环境变量：<code>source .bashrc</code></p><p>Flume部署至此完成。</p><h3 id="Flume的使用"><a href="#Flume的使用" class="headerlink" title="Flume的使用"></a>Flume的使用</h3><p>Flume官方文档：</p><p><a href="http://flume.apache.org/releases/content/1.6.0/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/releases/content/1.6.0/FlumeUserGuide.html</a></p><p><a href="https://cwiki.apache.org/confluence/display/FLUME/Getting+Started" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/FLUME/Getting+Started</a></p><h4 id="案例一：监控端口数据"><a href="#案例一：监控端口数据" class="headerlink" title="案例一：监控端口数据"></a>案例一：监控端口数据</h4><p>目标：Flume监控一端Console，另一端Console发送消息，使被监控端实时显示。</p><p>安装telnet（root用户）：<code>yum install telnet -y</code></p><p>配置Agent：</p><p><code>vi flume-telnet.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r1   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a1.sinks &#x3D; k1    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">a1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"># Describe&#x2F;configure the source   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.type &#x3D; netcat</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.port &#x3D; 44444</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"># Use a channel which buffers events in memory </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">a1.channels.c1.type &#x3D; memory</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"># Describe the sink  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.type &#x3D; logger</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"># Bind the source and sink to the channel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.channel &#x3D; c1</span></pre></td></tr></table></figure><p>判断端口44444是否被占用：</p><ul><li><code>netstat -tunlp | grep 44444</code> - root用户</li><li><code>netstat -a | grep 44444</code> - 普通用户</li></ul><p>启动Agent：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--name a1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--conf <span class="variable">$FLUME_HOME</span>/conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--conf-file <span class="variable">$FLUME_HOME</span>/script/flume-telnet.conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-Dflume.root.logger=INFO,console</span></pre></td></tr></table></figure><p>jps查看进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 ~]$ jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">4354 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">4249 Application</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 ~]$ ps -ef | grep 4249</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">vinx      4249  3690  3 21:08 pts/1    00:00:04 /usr/java/jdk1.8.0_91/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp /home/vinx/app/flume/conf:/home/vinx/app/flume/lib/*:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/etc/hadoop:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/common/lib/*:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/common/*:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs/lib/*:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/hdfs/*:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/yarn/lib/*:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/yarn/*:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/mapreduce/lib/*:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/share/hadoop/mapreduce/*:/home/vinx/app/hadoop/contrib/capacity-scheduler/*.jar:/home/vinx/app/hive/lib/* -Djava.library.path=:/home/vinx/app/hadoop-2.6.0-cdh5.16.2/lib/native org.apache.flume.node.Application --name a1 --conf-file /home/vinx/app/flume/script/flume-telnet.conf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">vinx      4370  4331  0 21:10 pts/2    00:00:00 grep 4249</span></pre></td></tr></table></figure><p>打开另一个session，使用telnet向本机的44444端口发送内容：<code>telnet hadoop002 44444</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 ~]$ telnet hadoop002 44444</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Trying 192.168.137.xxx...</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Connected to hadoop002.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Escape character is <span class="string">'^]'</span>.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">hello</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">world</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">hello   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">spark</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr></table></figure><p>此时，原来session的控制台输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">2018-02-13 21:13:35,122 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 0D                               hello. &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">2018-02-13 21:13:36,568 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 77 6F 72 6C 64 0D                               world. &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">2018-02-13 21:13:46,800 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 0D                               hello. &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">2018-02-13 21:13:49,500 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 73 70 61 72 6B 0D                               spark. &#125;</span></pre></td></tr></table></figure><p>其中，Event是数据传输的基本单元：</p><p> <code>Event: { headers:{} body: 68 65 6C 6C 6F 0D                               hello. }</code></p><p>其中，header是可选的，body是字节数组。</p><h4 id="案例二：实时读取本地文件到HDFS"><a href="#案例二：实时读取本地文件到HDFS" class="headerlink" title="案例二：实时读取本地文件到HDFS"></a>案例二：实时读取本地文件到HDFS</h4><p>官网：<a href="http://flume.apache.org/releases/content/1.6.0/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="noopener">http://flume.apache.org/releases/content/1.6.0/FlumeUserGuide.html#hdfs-sink</a></p><p>agent的配置文件：<code>vi flume-hdfs.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r1   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a1.sinks &#x3D; k1    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">a1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"># Describe&#x2F;configure the source   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.type &#x3D; exec</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;home&#x2F;vinx&#x2F;data&#x2F;flume&#x2F;data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.shell &#x3D; &#x2F;bin&#x2F;sh -c</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"># Use a channel which buffers events in memory </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">a1.channels.c1.type &#x3D; memory</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"># Describe the sink  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.type &#x3D; hdfs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop002:8020&#x2F;flume&#x2F;data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.batchSize &#x3D; 10</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"># Bind the source and sink to the channel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.channel &#x3D; c1</span></pre></td></tr></table></figure><p>HDFS上创建对应的目录：<code>hadoop fs -mkdir -p /flume/data</code></p><p>启动agent：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--name a1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--conf <span class="variable">$FLUME_HOME</span>/conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--conf-file <span class="variable">$FLUME_HOME</span>/script/flume-hdfs.conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-Dflume.root.logger=INFO,console</span></pre></td></tr></table></figure><p>此时出现connection refuse异常，初步判断是flume agent连接不上HDFS。</p><p>验证：<code>hadoop fs -ls hdfs://hadoop002:8020/</code>，发现连不上，猜测可能是端口错误。</p><p>查看core-site.xml中的参数（<code>vi core-site.xml</code>），发现：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop002:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/vinx/tmp/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr></table></figure><p>确认为端口错误。</p><p>修改flume-hdfs.conf，并保存：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r1   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a1.sinks &#x3D; k1    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">a1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"># Describe&#x2F;configure the source   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.type &#x3D; exec</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;home&#x2F;vinx&#x2F;data&#x2F;flume&#x2F;data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.shell &#x3D; &#x2F;bin&#x2F;sh -c</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"># Use a channel which buffers events in memory </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">a1.channels.c1.type &#x3D; memory</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"># Describe the sink  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.type &#x3D; hdfs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop002:9000&#x2F;flume&#x2F;data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.batchSize &#x3D; 10</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"># Bind the source and sink to the channel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.channel &#x3D; c1</span></pre></td></tr></table></figure><p>再次启动flume agent，连接成功。</p><p>测试数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> hello &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> world &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> hello &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> flume &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> hello &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> flume &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> hello &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> flume &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> hello &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> flume &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> hello &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> flume &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> hello &gt;&gt; data.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop002 flume]$ <span class="built_in">echo</span> flume &gt;&gt; data.log</span></pre></td></tr></table></figure><p>控制HDFS文件滚动的参数：</p><table><thead><tr><th>name</th><th>default</th><th>description</th></tr></thead><tbody><tr><td>hdfs.rollInterval</td><td>30</td><td>Number of seconds to wait before rolling current file (0 = never roll based on time interval)</td></tr><tr><td>hdfs.rollSize</td><td>1024</td><td>File size to trigger roll, in bytes (0: never roll based on file size)</td></tr><tr><td>hdfs.rollCount</td><td>10</td><td>Number of events written to file before it rolled (0 = never roll based on number of events)</td></tr></tbody></table><h4 id="案例三：实时读取目录文件到HDFS"><a href="#案例三：实时读取目录文件到HDFS" class="headerlink" title="案例三：实时读取目录文件到HDFS"></a>案例三：实时读取目录文件到HDFS</h4><p>目标：使用Flume监听整个目录的文件</p><p>配置文件flume-spool-hdfs.conf：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r1   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a1.sinks &#x3D; k1    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">a1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"># Describe&#x2F;configure the source   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.type &#x3D; spooldir</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.spoolDir &#x3D; &#x2F;home&#x2F;vinx&#x2F;data&#x2F;flume&#x2F;spool</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"># Use a channel which buffers events in memory </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">a1.channels.c1.type &#x3D; memory</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"># Describe the sink  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.type &#x3D; hdfs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;hadoop002:9000&#x2F;flume&#x2F;spool&#x2F;%Y%m%d%H%M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.batchSize &#x3D; 10</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; spool-</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"># Bind the source and sink to the channel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.channel &#x3D; c1</span></pre></td></tr></table></figure><p>启动agent：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--name a1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--conf <span class="variable">$FLUME_HOME</span>/conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--conf-file <span class="variable">$FLUME_HOME</span>/script/flume-spool-hdfs.conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-Dflume.root.logger=INFO,console</span></pre></td></tr></table></figure><p>测试，向本地spool文件夹添加文件。</p><p>注意，使用Spooling Directory Source时：</p><ol><li>不要在监控目录中创建并持续修改文件</li><li>上传完成的文件会以.COMPLETED结尾</li><li>被监控文件夹每500毫秒扫描一次文件变动</li></ol><h4 id="案例四：监控多个文件-文件夹"><a href="#案例四：监控多个文件-文件夹" class="headerlink" title="案例四：监控多个文件/文件夹"></a>案例四：监控多个文件/文件夹</h4><p>官方文档：<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#taildir-source" target="_blank" rel="noopener">http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#taildir-source</a></p><p>配置文件flume-taildir-logger.conf：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r1   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a1.sinks &#x3D; k1    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">a1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"># Describe&#x2F;configure the source   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.type &#x3D; TAILDIR</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.filegroups &#x3D; f1 f2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.positionFile &#x3D; &#x2F;home&#x2F;vinx&#x2F;data&#x2F;flume&#x2F;taildir_position.json</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.filegroups.f1 &#x3D; &#x2F;home&#x2F;vinx&#x2F;data&#x2F;flume&#x2F;taildir&#x2F;test01&#x2F;example.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.headers.f1.headerKey1 &#x3D; value1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.filegroups.f2 &#x3D; &#x2F;home&#x2F;vinx&#x2F;data&#x2F;flume&#x2F;taildir&#x2F;test02&#x2F;.*log.*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.headers.f2.headerKey1 &#x3D; value2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.headers.f2.headerKey2 &#x3D; value2-2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"># Use a channel which buffers events in memory </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">a1.channels.c1.type &#x3D; memory</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"># Describe the sink  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.type &#x3D; logger</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"># Bind the source and sink to the channel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">a1.sources.r1.channels &#x3D; c1  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.channel &#x3D; c1</span></pre></td></tr></table></figure><p>其中，taildir_position.json记录了消费偏移量。</p><p>启动agent：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--name a1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--conf <span class="variable">$FLUME_HOME</span>/conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--conf-file <span class="variable">$FLUME_HOME</span>/script/flume-taildir-logger.conf \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-Dflume.root.logger=INFO,console</span></pre></td></tr></table></figure><p>测试：</p><ol><li>在本地分别创建test01、test02两个文件夹，其中test01中创建example.log文件</li><li>向example.log中追加日志数据，查看Console消费情况</li><li>向test02文件夹分别copy后缀名为.txt和.log的文件，查看消费情况</li><li>观察offset的变化</li></ol><p>断点还原测试：</p><ol><li><p>启动flume agent，并向test01/example.log追加数据<code>echo flume &gt;&gt; example.log</code>，查看taildir_position.json中pos的值；</p></li><li><p>kill掉flume进程，并再次<code>echo hello &gt;&gt; example.log</code>；</p></li><li><p>重新启动flume agent，查看offset的变化：</p><p>[{“inode”:1978955,”pos”:54,”file”:”/home/vinx/data/flume/taildir/test01/example.log”}]</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce编程实现JSON格式数据的ETL清洗</title>
      <link href="/2018/06/11/hadoop/hadoop-json-etl/"/>
      <url>/2018/06/11/hadoop/hadoop-json-etl/</url>
      
        <content type="html"><![CDATA[<h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>使用MR编程，实现对JSON格式的数据进行ETL。</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>此处使用Gson解析JSON格式的数据。</p><p>Gson是Google公司发布的一个开放源代码的Java库，主要用途为序列化Java对象为JSON字符串，或反序列化JSON字符串成Java对象。</p><p>Gson的Github地址：<a href="https://github.com/google/gson" target="_blank" rel="noopener">https://github.com/google/gson</a></p><p>ETL主代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.google.gson.Gson;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.ivinx.hadoop.utils.FileUtils;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hdfs.web.JsonUtil;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 目标：对json格式数据进行ETL</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> *</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 输入的数据格式是json</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * &#123;"id":10,"name":"tom","address":"beijing"&#125;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * ...</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> *</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 数据ETL：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 输出：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 10tombeijing</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * ...</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonETL</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">        System.setProperty(<span class="string">"hadoop.home.dir"</span>, <span class="string">"/C:/softwarePath/winutils"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">        String input = <span class="string">"data/people.txt"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">        String output = <span class="string">"out/json"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">        Job job = Job.getInstance(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">        FileUtils.deleteOutput(conf, output);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">        job.setJarByClass(JsonETL<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">        job.setMapperClass(MyMapper<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">        job.setNumReduceTasks(<span class="number">0</span>); <span class="comment">// 不需要reduce</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        job.setOutputKeyClass(Info<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(input));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(output));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Info</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">        Gson gson = <span class="keyword">new</span> Gson();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">            Info info = gson.fromJson(value.toString(), Info<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">            context.write(info, NullWritable.get());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>自定义序列化类：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataInput;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Info</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> Long id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String address;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Info</span><span class="params">()</span> </span>&#123;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Info</span><span class="params">(Long id, String name, String address)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.id = id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.name = name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.address = address;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> id + <span class="string">"\t"</span> + name + <span class="string">"\t"</span> + address;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        out.writeLong(id);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        out.writeUTF(name);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        out.writeUTF(address);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.id = in.readLong();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.name = in.readUTF();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.address = in.readUTF();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getId</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(Long id)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.id = id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.name = name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getAddress</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> address;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAddress</span><span class="params">(String address)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.address = address;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce编程实现Group By</title>
      <link href="/2018/06/10/hadoop/hadoop-group-by/"/>
      <url>/2018/06/10/hadoop/hadoop-group-by/</url>
      
        <content type="html"><![CDATA[<h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>假设数据源为emp表，现在要求将表中的数据按照deptno来分组。</p><p>如果是在Hive上的话，可以用SQL轻易的解决，但是今天我们另辟蹊径，尝试使用MR代码的方式实现。</p><p>emp表字段：empno,ename,job,mgr,hiredate,sal,comm,deptno</p><p>输出格式：deptno  empno,ename…;empno,ename…</p><p>即key=deptno, value=records</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>Mapper、Reducer、Driver类实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.ivinx.hadoop.utils.FileUtils;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 数据源：emp.txt</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> *</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 目标：对deptno列分组</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> *</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 输出格式：deptno  empno,ename...;empno,ename...</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 即key=deptno, value=records</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> *</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 备用选项：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 当然也可以考虑自定义分区，按key输出为不同的文件（但是当分组数过多时，会产生很多小文件）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GroupByMR</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        System.setProperty(<span class="string">"hadoop.home.dir"</span>, <span class="string">"/C:/softwarePath/winutils"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">        String in = <span class="string">"data/emp.txt"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">        String out = <span class="string">"out/groupby"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">        Job job = Job.getInstance(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">        FileUtils.deleteOutput(conf, out);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">        job.setJarByClass(GroupByMR<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">        job.setMapperClass(MyMapper<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">        job.setReducerClass(MyReducer<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">        job.setMapOutputKeyClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">        job.setMapOutputValueClass(Emp<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">        job.setOutputKeyClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">        job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(in));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(out));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Emp</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">int</span> mgr;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">double</span> comm;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">int</span> deptNo;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">            String[] splits = value.toString().split(<span class="string">"\t"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">            System.out.println(splits.length);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> (<span class="number">6</span> == splits.length) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">                mgr = Integer.parseInt(splits[<span class="number">3</span>].trim());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">                comm = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">                deptNo = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">            &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">if</span> (<span class="string">""</span>.equals(splits[<span class="number">3</span>].trim())) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">                    mgr = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">                &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">                    mgr = Integer.parseInt(splits[<span class="number">3</span>].trim());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">                &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">if</span> (<span class="string">""</span>.equals(splits[<span class="number">6</span>].trim())) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line">                    comm = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line">                &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">                    comm = Double.parseDouble(splits[<span class="number">6</span>].trim());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line">                &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">if</span> (<span class="string">""</span>.equals(splits[<span class="number">7</span>].trim())) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line">                    deptNo = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line">                &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line">                    deptNo = Integer.parseInt(splits[<span class="number">7</span>].trim());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line">                &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">            Emp emp = <span class="keyword">new</span> Emp(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">                    Integer.parseInt(splits[<span class="number">0</span>].trim()), splits[<span class="number">1</span>], splits[<span class="number">2</span>], mgr,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">                    splits[<span class="number">4</span>], Double.parseDouble(splits[<span class="number">5</span>].trim()), comm, deptNo</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line">            );</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line">            context.write(<span class="keyword">new</span> IntWritable(deptNo), emp);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">IntWritable</span>, <span class="title">Emp</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(IntWritable key, Iterable&lt;Emp&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">for</span> (Emp emp: values) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">104</span></pre></td><td class="code"><pre><span class="line">                builder.append(emp.toString());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">105</span></pre></td><td class="code"><pre><span class="line">                builder.append(<span class="string">";"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">106</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">107</span></pre></td><td class="code"><pre><span class="line">            String empStr = builder.toString();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">108</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">109</span></pre></td><td class="code"><pre><span class="line">            context.write(key, <span class="keyword">new</span> Text(empStr));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">110</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">111</span></pre></td><td class="code"><pre><span class="line">            builder.delete(<span class="number">0</span>, builder.length());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">112</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">113</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">114</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>自定义序列化类Emp：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataInput;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Emp</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> Integer empno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String ename;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> Integer mgr;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String hiredate;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> Double sal;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> Double comm;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> Integer deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Emp</span><span class="params">()</span> </span>&#123;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Emp</span><span class="params">(Integer empno, String ename, String job, Integer mgr, String hiredate, Double sal, Double comm, Integer deptno)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.empno = empno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.ename = ename;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.job = job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.mgr = mgr;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.hiredate = hiredate;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.sal = sal;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.comm = comm;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.deptno = deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> empno + <span class="string">","</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">                + ename + <span class="string">","</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">                + job + <span class="string">","</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">                + mgr + <span class="string">","</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">                + hiredate + <span class="string">","</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">                + sal + <span class="string">","</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">                + comm + <span class="string">","</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">                + deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        out.writeInt(empno);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">        out.writeUTF(ename);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">        out.writeUTF(job);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">        out.writeInt(mgr);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">        out.writeUTF(hiredate);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">        out.writeDouble(sal);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">        out.writeDouble(comm);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">        out.writeInt(deptno);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.empno = in.readInt();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.ename = in.readUTF();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.job = in.readUTF();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.mgr = in.readInt();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.hiredate = in.readUTF();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.sal = in.readDouble();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.comm = in.readDouble();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.deptno = in.readInt();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getEmpno</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> empno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setEmpno</span><span class="params">(Integer empno)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.empno = empno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getEname</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> ename;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setEname</span><span class="params">(String ename)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.ename = ename;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getJob</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setJob</span><span class="params">(String job)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.job = job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getMgr</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> mgr;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setMgr</span><span class="params">(Integer mgr)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.mgr = mgr;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getHiredate</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> hiredate;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHiredate</span><span class="params">(String hiredate)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.hiredate = hiredate;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">104</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">105</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">106</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> Double <span class="title">getSal</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">107</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> sal;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">108</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">109</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">110</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSal</span><span class="params">(Double sal)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">111</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.sal = sal;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">112</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">113</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">114</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> Double <span class="title">getComm</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">115</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> comm;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">116</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">117</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">118</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setComm</span><span class="params">(Double comm)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">119</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.comm = comm;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">120</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">121</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">122</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getDeptno</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">123</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">124</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">125</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">126</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDeptno</span><span class="params">(Integer deptno)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">127</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">this</span>.deptno = deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">128</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">129</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce编程解决DataSkew数据倾斜问题</title>
      <link href="/2018/06/09/hadoop/hadoop-data-skew/"/>
      <url>/2018/06/09/hadoop/hadoop-data-skew/</url>
      
        <content type="html"><![CDATA[<h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>假设数据文件如下：</p><p>hello    hadoop<br>hello    spark<br>hello    spark<br>hello    hadoop<br>hello    flink<br>hello    hadoop<br>hello    world<br>hello    flink<br>…</p><p>当要实现WordCount时，就容易产生数据倾斜，下面探讨使用MapReducer编程解决由此产生的数据热点问题。</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>思路：主要分两步，加盐和去盐</p><ol><li>加盐，每一个单词后面加一个随机数，将热点词打散到不同的reduce进行计算</li><li>去盐，去掉单词后的随机数，做真正的WordCount</li></ol><p>第一个MR，加盐：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.ivinx.hadoop.utils.FileUtils;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Random;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 加盐 Salt</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SaltDataSkew</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        System.setProperty(<span class="string">"hadoop.home.dir"</span>, <span class="string">"/C:/softwarePath/winutils"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        String input = <span class="string">"data/skew/skewdata.txt"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        String output = <span class="string">"out/skew01"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        Job job = Job.getInstance(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        FileUtils.deleteOutput(conf, output);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">        job.setJarByClass(SaltDataSkew<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">        job.setMapperClass(MyMapper<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">        job.setReducerClass(MyReducer<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(input));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(output));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">        Random rand = <span class="keyword">new</span> Random();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">            String[] splits = value.toString().split(<span class="string">"\t"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> saltNum = rand.nextInt(<span class="number">10</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">for</span> (String word: splits) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">                context.write(<span class="keyword">new</span> Text(word + <span class="string">"_"</span> + saltNum), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">for</span> (IntWritable value: values) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">                count += value.get();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(count));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>第二个MR，去盐并WordCount：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.ivinx.hadoop.utils.FileUtils;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 去盐 Desalt</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DesaltDataSkew</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        System.setProperty(<span class="string">"hadoop.home.dir"</span>, <span class="string">"/C:/softwarePath/winutils"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        String input = <span class="string">"out/skew01"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        String output = <span class="string">"out/skew02"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        Job job = Job.getInstance(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        FileUtils.deleteOutput(conf, output);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        job.setJarByClass(DesaltDataSkew<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">        job.setMapperClass(MyMapper<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">        job.setReducerClass(MyReducer<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(input));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(output));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">        String desaltWord;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">int</span> count;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">            String[] splits = value.toString().split(<span class="string">"\t"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">            desaltWord = splits[<span class="number">0</span>].split(<span class="string">"_"</span>)[<span class="number">0</span>];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">            count = Integer.parseInt(splits[<span class="number">1</span>]);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">            context.write(<span class="keyword">new</span> Text(desaltWord), <span class="keyword">new</span> IntWritable(count));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">for</span> (IntWritable value: values) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">                count += value.get();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(count));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h4 id="ChainMapper-ChainReducer代码实现"><a href="#ChainMapper-ChainReducer代码实现" class="headerlink" title="ChainMapper+ChainReducer代码实现"></a>ChainMapper+ChainReducer代码实现</h4><p>可以使用ChainMapper+ChainReducer一步解决数据倾斜，思路和上面类似，但是只需要一个类。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.ivinx.hadoop.utils.FileUtils;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainMapper;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.chain.ChainReducer;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Random;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * MR解决数据倾斜 data skew</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> *</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * Chain 实现加盐 去盐过程</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ChainMRDriver</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        System.setProperty(<span class="string">"hadoop.home.dir"</span>, <span class="string">"/C:/softwarePath/winutils"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        String in = <span class="string">"data/skew/skewdata.txt"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        String out1 = <span class="string">"out/skew01"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        String out2 = <span class="string">"out/skew02"</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">        Job job1 = Job.getInstance(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">        Job job2 = Job.getInstance(conf);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        FileUtils.deleteOutput(conf, out1);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">        FileUtils.deleteOutput(conf, out2);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">        job1.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">        job2.setJarByClass(ChainMRDriver<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">        ChainMapper.addMapper(job1, SaltMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">        ChainReducer.setReducer(job1, SaltReducer<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">        ChainMapper.addMapper(job2, DesaltMapper<span class="class">.<span class="keyword">class</span>, <span class="title">LongWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        ChainReducer.setReducer(job2, DesaltReducer<span class="class">.<span class="keyword">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">Text</span>.<span class="title">class</span>, <span class="title">IntWritable</span>.<span class="title">class</span>, <span class="title">conf</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">        FileInputFormat.setInputPaths(job1, <span class="keyword">new</span> Path(in));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">        FileOutputFormat.setOutputPath(job1, <span class="keyword">new</span> Path(out1));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">        FileInputFormat.setInputPaths(job2, <span class="keyword">new</span> Path(out1));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">        FileOutputFormat.setOutputPath(job2, <span class="keyword">new</span> Path(out2));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//提交job</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">        System.out.println(<span class="string">"======= 加盐 ======="</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">boolean</span> result1 = job1.waitForCompletion(<span class="keyword">true</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (result1) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">            System.out.println(<span class="string">"======= 去盐 ======="</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">boolean</span> result2 = job2.waitForCompletion(<span class="keyword">true</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">            System.exit(result2 ? <span class="number">0</span> : <span class="number">1</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">        System.exit(<span class="number">1</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DesaltMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">        String desaltWord;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">int</span> count;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">            String[] splits = value.toString().split(<span class="string">"\t"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">            desaltWord = splits[<span class="number">0</span>].split(<span class="string">"_"</span>)[<span class="number">0</span>];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">            count = Integer.parseInt(splits[<span class="number">1</span>]);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">            context.write(<span class="keyword">new</span> Text(desaltWord), <span class="keyword">new</span> IntWritable(count));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DesaltReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">for</span> (IntWritable value: values) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line">                count += value.get();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(count));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SaltMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line">        Random rand = <span class="keyword">new</span> Random();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line">            String[] splits = value.toString().split(<span class="string">"\t"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> saltNum = rand.nextInt(<span class="number">10</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">for</span> (String word: splits) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line">                context.write(<span class="keyword">new</span> Text(word + <span class="string">"_"</span> + saltNum), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">104</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">105</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">106</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SaltReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">107</span></pre></td><td class="code"><pre><span class="line">        <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">108</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">109</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> count = <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">110</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">111</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">for</span> (IntWritable value: values) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">112</span></pre></td><td class="code"><pre><span class="line">                count += value.get();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">113</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">114</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">115</span></pre></td><td class="code"><pre><span class="line">            context.write(key, <span class="keyword">new</span> IntWritable(count));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">116</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">117</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">118</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop的API使用：文件重命名</title>
      <link href="/2018/06/08/hadoop/hadoop-rename-file/"/>
      <url>/2018/06/08/hadoop/hadoop-rename-file/</url>
      
        <content type="html"><![CDATA[<h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><p>假设HDFS上日志文件的形式如下：</p><p>/logs/20171011/188.txt<br>/logs/20171011/2sf.txt<br>/logs/20171011/36t.txt<br>/logs/20171012/1ns.txt<br>/logs/20171012/2sfs.txt<br>/logs/20171012/37hjd.txt</p><p>现在要求，使用HDFS API实现日志文件的重命名，输出格式如下：</p><p>/logs/20171011/20171011-0.txt<br>/logs/20171011/20171011-1.txt<br>/logs/20171011/20171011-2.txt<br>/logs/20171012/20171012-0.txt<br>/logs/20171012/20171012-1.txt<br>/logs/20171012/20171012-2.txt</p><p>也就是实现日志文件命名的规则化。</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.net.URI;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.List;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RenameFileApp</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">static</span> FileSystem fileSystem;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">(String day)</span> <span class="keyword">throws</span> IOException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//TODO...</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        Path filePath = <span class="keyword">new</span> Path(<span class="string">"/logs/"</span> + day);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(filePath, <span class="keyword">true</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        List&lt;String&gt; srcList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">while</span> (files.hasNext()) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">            LocatedFileStatus fileStatus = files.next();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">            String src = fileStatus.getPath().toString();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">            srcList.add(src);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; srcList.size(); i++) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">            Path dst = <span class="keyword">new</span> Path(<span class="string">"/logs/"</span> + day + <span class="string">"/"</span> + day + <span class="string">"-"</span> + i + <span class="string">".txt"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">            fileSystem.rename(<span class="keyword">new</span> Path(srcList.get(i)), dst);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        setUp();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">        rename(<span class="string">"20171011"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">        rename(<span class="string">"20171012"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">        tearDown();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setUp</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        URI uri = <span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop002:9000"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">        conf.set(<span class="string">"dfs.replication"</span>, <span class="string">"1"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">        fileSystem = FileSystem.get(uri, conf, <span class="string">"vinx"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">tearDown</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != fileSystem) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">            fileSystem.close();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce在Windows上运行的坑：winutils.exe和hadoop.dll</title>
      <link href="/2018/06/07/hadoop/hadoop-winutils-error/"/>
      <url>/2018/06/07/hadoop/hadoop-winutils-error/</url>
      
        <content type="html"><![CDATA[<h3 id="Hadoop在Windows上的坑：winutils-exe和hadoop-dll"><a href="#Hadoop在Windows上的坑：winutils-exe和hadoop-dll" class="headerlink" title="Hadoop在Windows上的坑：winutils.exe和hadoop.dll"></a>Hadoop在Windows上的坑：winutils.exe和hadoop.dll</h3><p>在Windows电脑上运行MapReduce程序，出现以下错误：</p><p><em>java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries</em></p><p>解决方法：</p><ol><li>下载winutils.exe(<a href="http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe" target="_blank" rel="noopener">http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe</a>)</li><li>创建一个目录，比如<code>C:\softwarePath\winutils\bin</code>，将winutils.exe放入上述创建好的目录下（注意不能缺少bin目录）</li><li>设置<code>HADOOP_HOME=C:\softwarePath\winutils</code>环境变量，并将其放入PATH变量中，例如<code>%HADOOP_HOME%\bin</code>；或者直接在程序中加入<code>System.setProperty(&quot;hadoop.home.dir&quot;, &quot;/C:/softwarePath/winutils&quot;);</code>。这里采用的是后者。</li></ol><p>当运行程序的时候，又出现了以下错误：</p><p><em>Exception in thread “main” java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z</em></p><p>解决方法：</p><p>下载winutils.exe和hadoop.dll(<a href="https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin" target="_blank" rel="noopener">https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin</a>)<br>放这两个在<code>%HADOOP_HOME%\bin</code>下，同时hadoop.dll也放在C:\Windows\System32目录下。</p><p>至此，问题解决。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop：重新格式化HDFS</title>
      <link href="/2018/06/04/hadoop/hadoop-reformat/"/>
      <url>/2018/06/04/hadoop/hadoop-reformat/</url>
      
        <content type="html"><![CDATA[<h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p>使用start-dfs.sh启动HDFS，但是jps看不到NameNode进程，并且NN的日志报错。</p><p>问题原因：</p><p>格式化HDFS时，没有指定<code>hadoop.tmp.dir</code>，默认使用的是<code>/tmp</code>目录。由于Linux系统<code>/tmp</code>目录的自动删除机制，致使<code>file://${hadoop.tmp.dir}/dfs/name</code>目录丢失，所以NameNode启动失败。</p><h3 id="重新格式化HDFS"><a href="#重新格式化HDFS" class="headerlink" title="重新格式化HDFS"></a>重新格式化HDFS</h3><p>hdfs-default.xml的相关参数：</p><table><thead><tr><th>name</th><th>value</th></tr></thead><tbody><tr><td>dfs.namenode.name.dir</td><td>file://${hadoop.tmp.dir}/dfs/name</td></tr><tr><td>dfs.datanode.data.dir</td><td>file://${hadoop.tmp.dir}/dfs/data</td></tr><tr><td>dfs.namenode.checkpoint.dir</td><td>file://${hadoop.tmp.dir}/dfs/namesecondary</td></tr></tbody></table><p><code>vi $HADOOP_HOME/etc/hadoop/hdfs-site.xml</code>，追加以下参数：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/vinx/tmp/hadoop/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/vinx/tmp/hadoop/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/vinx/tmp/hadoop/dfs/namesecondary<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>core-default.xml的相关参数：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>hadoop.tmp.dir</td><td>/tmp/hadoop-${user.name}</td><td>A base for other temporary directories.</td></tr></tbody></table><p>可以看到，core-site.xml中的<code>hadoop.tmp.dir</code>其实就是上述hdfs-site.xml中参数的根路径，所以其实只需要修改core-site.xml中的<code>hadoop.tmp.dir</code>参数即可。</p><p><code>vi $HADOOP_HOME/etc/hadoop/core-site.xml</code>，追加以下参数：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/vinx/tmp/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>执行重新格式化命令<code>hdfs namenode -format</code>，出现：</p><p><em>Re-format filesystem in Storage Directory /home/vinx/tmp/hadoop/dfs/name ? (Y or N) Y</em></p><p>继续执行，出现：</p><p><em>18/06/04 15:06:07 INFO common.Storage: Storage directory /home/vinx/tmp/hadoop/dfs/name has been successfully formatted.</em></p><p>说明重新格式化成功。此时，在<code>/home/vinx/tmp/hadoop/dfs</code>目录会出现name文件夹。</p><p>start-dfs启动HDFS，可以看到NameNode进程已经起来。</p><p>查看web UI界面：</p><p><a href="http://hadoop002:50070/" target="_blank" rel="noopener">http://hadoop002:50070/</a></p><p>注意：这种重新格式化的方法，会将原来HDFS中的数据全部清空。当然原来的数据并没有删除，只是指定了一个新的NameNode路径，并在指定的目录维护HDFS文件和块的信息。</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive常用的日期函数</title>
      <link href="/2018/06/02/hive/hive-date-function/"/>
      <url>/2018/06/02/hive/hive-date-function/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive日期函数"><a href="#Hive日期函数" class="headerlink" title="Hive日期函数"></a>Hive日期函数</h3><p>Hive官网地址：<a href="http://hive.apache.org/" target="_blank" rel="noopener">http://hive.apache.org/</a></p><h4 id="前置：Hive查看函数使用方法"><a href="#前置：Hive查看函数使用方法" class="headerlink" title="前置：Hive查看函数使用方法"></a>前置：Hive查看函数使用方法</h4><ol><li><p>查看month相关的函数：</p><p><code>show functions like &#39;*month*&#39;</code></p></li><li><p>查看add_months函数的用法：</p><p><code>desc function add_months;</code></p></li><li><p>查看add_months函数的详细说明和例子：</p><p><code>desc function extended add_months;</code></p></li></ol><h4 id="常用的日期函数"><a href="#常用的日期函数" class="headerlink" title="常用的日期函数"></a>常用的日期函数</h4><p><code>unix_timestamp()</code>：获取当前unix的时间戳</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 1527917075  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2018-06-02 20:24:35'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 1527942275  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr></table></figure><p><code>from_unixtime()</code>：转化unix时间戳到当前时区的时间，以指定格式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">unix_timestamp</span>(), <span class="string">'yyyy-MM-dd'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2018-06-02  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1527917075</span>, <span class="string">'yyyyMMdd'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|    _c0    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 20180602  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+--+</span></span></pre></td></tr></table></figure><p><code>to_date()</code>：时间转日期格式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">to_date</span>(from_unixtime(<span class="keyword">unix_timestamp</span>(), <span class="string">'yyyy-MM-dd'</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2018-06-02  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">to_date</span>(<span class="string">'2018-06'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|  _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr></table></figure><p><code>select to_date(unix_timestamp());</code>会报错如下：<br><em>Error: Error while compiling statement: FAILED: SemanticException [Error 10014]: Line 1:7 Wrong arguments ‘unix_timestamp’: TO_DATE() only takes STRING/TIMESTAMP/DATEWRITABLE types, got LONG (state=42000,code=10014)</em></p><p><code>date_sub(start_date, num_days)</code> - Returns the date that is num_days before start_date.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_sub</span>(<span class="string">'2018-06-02'</span>, <span class="number">30</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2018-05-03  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_sub</span>(<span class="string">'2018-06'</span>, <span class="number">30</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|  _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_sub</span>(<span class="string">'20180602'</span>, <span class="number">30</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|  _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">| NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+--+</span></span></pre></td></tr></table></figure><p><code>date_format(date/timestamp/string, fmt)</code> - converts a date/timestamp/string to a value of string in the format specified by the date format fmt.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2018-06-02 20:24:35'</span>, <span class="string">'yyyy-MM-dd'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2018-06-02  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2018-06-02 20:24:35'</span>, <span class="string">'yyyy-MM'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|   _c0    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 2018-06  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+--+</span></span></pre></td></tr></table></figure><p>返回时间中的年、月、日等：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">year</span>(<span class="string">'2018-06-02 20:24:35'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">month</span>(<span class="string">'2018-06-02 20:24:35'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(<span class="string">'2018-06-02 20:24:35'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">hour</span>(<span class="string">'2018-06-02 20:24:35'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">minute</span>(<span class="string">'2018-06-02 20:24:35'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">second</span>(<span class="string">'2018-06-02 20:24:35'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 返回时间在整年中的周数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">weekofyear</span>(<span class="string">'2018-06-02 20:24:35'</span>);</span></pre></td></tr></table></figure><p><code>datediff()</code>：返回开始日期减去结束日期的天数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">'2018-06-02'</span>, <span class="string">'2018-05-21'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 12   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 取最近30天数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dws__bill <span class="keyword">where</span> <span class="keyword">datediff</span>(<span class="keyword">CURRENT_TIMESTAMP</span>, createtime) &lt;= <span class="number">30</span>;</span></pre></td></tr></table></figure><p><code>date_add()</code>：返回给定日期后n天的日期</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">'2018-06-02'</span>, <span class="number">10</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2018-06-12  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr></table></figure><p>yyyyMMdd和yyyy-MM-dd日期之间的切换：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建dual伪表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual(dummy <span class="keyword">string</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/dual.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| dual.dummy  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| X           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 20180602转成2018-06-02</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">unix_timestamp</span>(<span class="string">'20180602'</span>,<span class="string">'yyyyMMdd'</span>),<span class="string">'yyyy-MM-dd'</span>) <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 2018-06-02转成20180602</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="keyword">unix_timestamp</span>(<span class="string">'2018-06-02'</span>,<span class="string">'yyyy-mm-dd'</span>),<span class="string">'yyyymmdd'</span>) <span class="keyword">from</span> dual;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming基础</title>
      <link href="/2018/05/29/spark/spark-streaming-basic/"/>
      <url>/2018/05/29/spark/spark-streaming-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="Spark-Streaming简介"><a href="#Spark-Streaming简介" class="headerlink" title="Spark Streaming简介"></a>Spark Streaming简介</h3><p>Spark Streaming是核心Spark API的扩展，可实现可扩展、高吞吐量、可容错的实时数据流处理。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等众多来源获取，并且可以使用由高级函数（如map，reduce，join和window）开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。而且还可以在数据流上应用Spark提供的机器学习和图处理算法。</p><p><img src="https://vinxikk.github.io/img/spark/what-is-spark-streaming.png" alt="Spark Streaming是什么"></p><h3 id="Spark-Streaming的内部结构"><a href="#Spark-Streaming的内部结构" class="headerlink" title="Spark Streaming的内部结构"></a>Spark Streaming的内部结构</h3><p>在内部，它的工作原理如下。Spark Streaming接收实时输入数据流，并将数据切分成批，然后由Spark引擎对其进行处理，最后生成“批”形式的结果流。</p><p><img src="https://vinxikk.github.io/img/spark/streaming-batch-process.png" alt="Spark Streaming工作原理"></p><p>Spark Streaming将连续的数据流抽象为discretizedstream或DStream。在内部，DStream 由一个RDD序列表示。</p><h3 id="StreamingContext对象"><a href="#StreamingContext对象" class="headerlink" title="StreamingContext对象"></a>StreamingContext对象</h3><p>初始化<code>StreamingContext</code>：</p><p>方式一，从<code>SparkConf</code>对象中创建：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个Context对象：StreamingContext</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"MyNetworkWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//指定批处理的时间间隔</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span></pre></td></tr></table></figure><p>方式二，从现有的<code>SparkContext</code>实例中创建：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span></pre></td></tr></table></figure><p>说明：</p><ul><li>appName参数是应用程序在集群UI上显示的名称。 </li><li>master是Spark，Mesos或YARN集群的URL，或者一个特殊的<code>“local [*]”</code>字符串来让程序以本地模式运行。</li><li>当在集群上运行程序时，不需要在程序中硬编码master参数，而是使用spark-submit提交应用程序并将master的URL以脚本参数的形式传入。但是，对于本地测试和单元测试，您可以通过<code>“local[*]”</code>来运行Spark Streaming程序（请确保本地系统中的cpu核心数够用）。 </li><li><code>StreamingContext</code>会内在的创建一个<code>SparkContext</code>的实例（所有Spark功能的起始点），你可以通过<code>ssc.sparkContext</code>访问到这个实例。</li><li>批处理的时间窗口长度必须根据应用程序的延迟要求和可用的集群资源进行设置。</li></ul><p>注意：</p><ul><li>一旦一个<code>StreamingContext</code>开始运作，就不能设置或添加新的流计算。</li><li>一旦一个上下文被停止，它将无法重新启动。</li><li>同一时刻，一个JVM中只能有一个<code>StreamingContext</code>处于活动状态。</li><li><code>StreamingContext</code>上的<code>stop()</code>方法也会停止<code>SparkContext</code>。 要仅停止<code>StreamingContext</code>（保持<code>SparkContext</code>活跃），请将<code>stop()</code> 方法的可选参数<code>stopSparkContext</code>设置为<code>false</code>。</li><li>只要前一个<code>StreamingContext</code>在下一个<code>StreamingContext</code>被创建之前停止（不停止<code>SparkContext</code>），<code>SparkContext</code>就可以被重用来创建多个<code>StreamingContext</code>。</li></ul><h3 id="离散流（DStream）"><a href="#离散流（DStream）" class="headerlink" title="离散流（DStream）"></a>离散流（DStream）</h3><p><code>DiscretizedStream</code>或<code>DStream</code> 是Spark Streaming对流式数据的基本抽象。它表示连续的数据流，这些连续的数据流可以是从数据源接收的输入数据流，也可以是通过对输入数据流执行转换操作而生成的经处理的数据流。在内部，<code>DStream</code>由一系列连续的RDD表示，如下图：</p><p><img src="https://vinxikk.github.io/img/spark/streaming-dstream-1.png" alt="DStream由一系列连续的RDD组成"></p><p>在之前的NetworkWordCount的例子中，我们将一行行文本组成的流转换为单词流，具体做法为：将<code>flatMap</code>操作应用于名为lines的 DStream中的每个RDD上，以生成words DStream的RDD。如下图所示：</p><p><img src="https://vinxikk.github.io/img/spark/streaming-dstream-2.png" alt="lines DStream转化为words DStream"></p><p>但是DStream和RDD也有区别，下面画图说明：</p><p><img src="https://vinxikk.github.io/img/spark/streaming-dstream-3.png" alt="RDD的结构"></p><p><img src="https://vinxikk.github.io/img/spark/streaming-dstream-4.png" alt="DStream的结构"></p><h4 id="DStream中的转换操作（transformation）"><a href="#DStream中的转换操作（transformation）" class="headerlink" title="DStream中的转换操作（transformation）"></a>DStream中的转换操作（transformation）</h4><p><img src="https://vinxikk.github.io/img/spark/streaming-dstream-transformation.png" alt="DStream的transformation"></p><p><code>transform(func)</code>：</p><ul><li><p>通过RDD-to-RDD函数作用于源DStream中的各个RDD，可以是任意的RDD操作，从而返回一个新的RDD。</p></li><li><p>举例：在NetworkWordCount中，也可以使用transform来生成元组对</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        .setAppName(<span class="string">"NetworkWordCount"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        .setMaster(<span class="string">"local[2]"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//创建一个DStream，处理数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">            <span class="string">"192.168.xxx.xxx"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">            <span class="number">7788</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">            <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//执行wordcount</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//生成元组</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> wordPair = words.transform(x =&gt; x.map(x =&gt; (x, <span class="number">1</span>)))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        wordPair.print()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//启动StreamingContext</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        ssc.start()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//等待计算完成</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        ssc.awaitTermination()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li></ul><p><code>updateStateByKey(func)</code>：</p><ul><li><p>操作允许不断用新信息更新它的同时保持任意状态。</p><p>定义状态，状态可以是任何的数据类型</p><p>定义状态更新函数，怎样利用更新前的状态和从输入流里面获取的新值更新状态</p></li><li><p>重写NetworkWordCount程序，累计每个单词出现的频率</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyTotalNetworkWordCount</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      .setAppName(<span class="string">"MyNetworkWordCount"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">5</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//设置检查点</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    ssc.checkpoint(<span class="string">"hdfs://192.168.xxx.xxx:9000/spark/checkpoint"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//创建一个DStream，处理数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> lines  = ssc.socketTextStream(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="string">"192.168.xxx.xxx"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="number">7788</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">    )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//执行wordcount</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//定义函数用于累计每个单词的总频率</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> addFunc = (currValues: <span class="type">Seq</span>[<span class="type">Int</span>], prevValueState: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//通过Spark内部的reduceByKey按key规约，然后这里传入某key当前批次的Seq/List,再计算当前批次的总和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> currentCount = currValues.sum</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">// 已累加的值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> previousCount = prevValueState.getOrElse(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">// 返回累加后的结果，是一个Option[Int]类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      <span class="type">Some</span>(currentCount + previousCount)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> totalWordCounts = pairs.updateStateByKey[<span class="type">Int</span>](addFunc)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    totalWordCounts.print()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">    ssc.start()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">    ssc.awaitTermination()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li></ul><h4 id="窗口操作"><a href="#窗口操作" class="headerlink" title="窗口操作"></a>窗口操作</h4><p>Spark Streaming还提供了窗口计算功能，允许您在数据的滑动窗口上应用转换操作。下图说明了滑动窗口的工作方式：</p><p><img src="https://vinxikk.github.io/img/spark/streaming-dstream-window.png" alt="DStream的窗口计算"></p><p>如图所示，每当窗口滑过<code>originalDStream</code>时，落在窗口内的源RDD被组合并被执行操作以产生<code>windowedDStream</code>的RDD。在上面的例子中，操作应用于最近3个时间单位的数据，并以2个时间单位滑动。这表明任何窗口操作都需要指定两个参数：</p><ul><li><p>窗口长度（windowlength） - 窗口的时间长度（上图的示例中为：3）。</p></li><li><p>滑动间隔（slidinginterval） - 两次相邻的窗口操作的间隔（即每次滑动的时间长度）（上图示例中为：2）。</p><p>这两个参数必须是源DStream的批间隔的倍数（上图示例中为：1）。</p></li></ul><p>我们以一个例子来说明窗口操作。 对之前的单词计数的示例进行扩展，每10秒钟对过去30秒的数据进行wordcount。为此，我们必须在最近30秒的pairs DStream数据中对(word, 1)键值对应用<code>reduceByKey</code>操作。这是通过使用<code>reduceByKeyAndWindow</code>操作完成的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 执行wordcount</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordPair = words.map(x =&gt; (x, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//val wordCountResult = wordPair.reduceByKey(_ + _)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordCountResult = wordPair.reduceByKeyAndWindow(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    (a: <span class="type">Int</span>, b: <span class="type">Int</span>) =&gt; (a + b), <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr></table></figure><p>一些常见的窗口操作如下表所示。所有这些操作都用到了上述两个参数：<code>windowLength</code>和<code>slideInterval</code>。</p><ul><li><p><code>window(windowLength, slideInterval)</code></p><p>基于源DStream产生的窗口化的批数据计算一个新的DStream</p></li><li><p><code>countByWindow(windowLength, slideInterval)</code></p><p>返回流中元素的一个滑动窗口数</p></li><li><p><code>reduceByWindow(func, windowLength, slideInterval)</code></p><p>返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数必须是相关联的以使计算能够正确的并行计算。</p></li><li><p><code>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</code></p><p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数</p></li><li><p><code>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</code></p><p>上述<code>reduceByKeyAndWindow()</code>的更高效的版本，其中使用前一窗口的reduce计算结果递增地计算每个窗口的reduce值。这是通过对进入滑动窗口的新数据进行reduce操作，以及“逆减（inverse reducing）”离开窗口的旧数据来完成的。一个例子是当窗口滑动时对键对应的值进行“一加一减”操作。但是，它仅适用于“可逆减函数（invertible reduce functions）”，即具有相应“反减”功能的减函数（作为参数invFunc）。 像<code>reduceByKeyAndWindow</code>一样，通过可选参数可以配置reduce任务的数量。 请注意，使用此操作必须启用检查点。</p></li><li><p><code>countByValueAndWindow(windowLength, slideInterval, [numTasks])</code></p><p>应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率。</p></li></ul><h4 id="输入DStream和接收器"><a href="#输入DStream和接收器" class="headerlink" title="输入DStream和接收器"></a>输入DStream和接收器</h4><p>输入DStreams表示从数据源获取输入数据流的DStreams。在NetworkWordCount例子中，lines表示输入DStream，它代表从netcat服务器获取的数据流。每一个输入流DStream和一个Receiver对象相关联，这个Receiver从源中获取数据，并将数据存入内存中用于处理。</p><p>输入DStreams表示从数据源获取的原始数据流。Spark Streaming拥有两类数据源：</p><ul><li>基本本源（Basic sources）：这些源在StreamingContext API中直接可用。例如文件系统、套接字连接、Akka的actor等。</li><li>高级源（Advanced sources）：这些源包括Kafka,Flume,Kinesis,Twitter等等。</li></ul><p>下面通过具体的案例，详细说明：</p><p><strong>文件流</strong>：通过监控文件系统的变化，若有新文件添加，则将它读入并作为数据流</p><p>需要注意的是：</p><ul><li>这些文件具有相同的格式</li><li>这些文件通过原子移动或重命名文件的方式在dataDirectory创建</li><li>如果在文件中追加内容，这些追加的新数据也不会被读取</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FileStreaming</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        .setMaster(<span class="string">"local[2]"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        .setAppName(<span class="string">"FileStreaming"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">2</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//从本地目录中读取数据：如果有新文件产生，就会读取进来</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> lines = ssc.textFileStream(<span class="string">"c:\\data\\files"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//打印结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        lines.print()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        ssc.start()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        ssc.awaitTermination()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p><strong>RDD队列流</strong>：使用<code>streamingContext.queueStream(queueOfRDD)</code>创建基于RDD队列的DStream，用于调试Spark Streaming应用程序。</p><p><strong>套接字流</strong>：通过监听Socket端口来接收数据。</p><h4 id="DStream的输出操作"><a href="#DStream的输出操作" class="headerlink" title="DStream的输出操作"></a>DStream的输出操作</h4><p>输出操作允许DStream的操作推到如数据库、文件系统等外部系统中。因为输出操作实际上是允许外部系统消费转换后的数据，它们触发的实际操作是DStream转换。</p><p>目前，定义了下面几种输出操作：</p><p><img src="https://vinxikk.github.io/img/spark/streaming-dstream-output.png" alt="DStream的输出操作"></p><h5 id="foreachRDD的设计模式"><a href="#foreachRDD的设计模式" class="headerlink" title="foreachRDD的设计模式"></a>foreachRDD的设计模式</h5><p><code>DStream.foreachRDD</code>是一个强大的原语，发送数据到外部系统中。</p><p>第一步：创建连接，将数据写入外部数据库</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//输出结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//wordCountResult.print()</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">wordCountResult.foreachRDD(rdd =&gt;&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    rdd.foreachPartition(partitionRecord =&gt;&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">var</span> conn:<span class="type">Connection</span> = <span class="literal">null</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">var</span> pst:<span class="type">PreparedStatement</span> = <span class="literal">null</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">try</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">            conn = <span class="type">DriverManager</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">            .getConnection(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">                    <span class="string">"jdbc:oracle:thin:@192.168.xxx.xxx:1521/orcl.example.com"</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">                    <span class="string">"vinx"</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">                    <span class="string">"123456"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">                )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">            partitionRecord.foreach(record =&gt; &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">                pst = conn.prepareStatement(<span class="string">"insert into myresult values(?,?)"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">                pst.setString(<span class="number">1</span>, record._1)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">                pst.setInt(<span class="number">2</span>, record._2)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">//执行</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">                pst.executeUpdate()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">            &#125;)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        &#125;<span class="keyword">catch</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">case</span> e1:<span class="type">Exception</span> =&gt; println(<span class="string">"Some Error: "</span> + e1.getMessage)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span>(pst != <span class="literal">null</span>) pst.close()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span>(conn != <span class="literal">null</span>) conn.close()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    &#125;)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">&#125;)</span></pre></td></tr></table></figure><h4 id="DataFrame和SQL操作"><a href="#DataFrame和SQL操作" class="headerlink" title="DataFrame和SQL操作"></a>DataFrame和SQL操作</h4><p>我们可以很方便地使用DataFrames和SQL操作来处理流数据。必须使用当前的StreamingContext对应的SparkContext创建一个SparkSession。此外，必须这样做的另一个原因是使得应用可以在driver程序故障时得以重新启动，这是通过创建一个可以延迟实例化的单例SparkSession来实现的。</p><p>在下面的示例中，我们使用DataFrame和SQL来修改之前的wordcount示例并对单词进行计数。我们将每个RDD转换为DataFrame，并注册为临时表，然后在这张表上执行SQL查询。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用Spark SQL来查询Spark Streaming处理的数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">words.foreachRDD &#123; rdd =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//使用单例模式，创建SparkSession对象</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    .config(rdd.sparkContext.getConf)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    .getOrCreate()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">import</span> spark.implicits._</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// 将RDD[String]转换为DataFrame</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> wordsDataFrame = rdd.toDF(<span class="string">"word"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// 创建临时视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// 执行SQL</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> wordCountsDataFrame = spark.sql(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="string">"select word, count(*) as total from words group by word"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    wordCountsDataFrame.show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL性能优化</title>
      <link href="/2018/05/27/spark/spark-sql-optimize/"/>
      <url>/2018/05/27/spark/spark-sql-optimize/</url>
      
        <content type="html"><![CDATA[<h3 id="在内存中缓存数据"><a href="#在内存中缓存数据" class="headerlink" title="在内存中缓存数据"></a>在内存中缓存数据</h3><p>性能调优主要是将数据放入内存中操作。通过<code>spark.cacheTable(&quot;tableName&quot;)</code>或者<code>dataFrame.cache()</code>。使用<code>spark.uncacheTable(&quot;tableName&quot;)</code>来从内存中去除table。</p><p><strong>Demo案例：</strong></p><p>从Oracle数据库中读取数据，生成DataFrame：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> oracleDF = spark.read.format(<span class="string">"jdbc"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    .option(<span class="string">"url"</span>,<span class="string">"jdbc:oracle:thin:@192.168.xxx.xxx:1521/orcl.example.com"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    .option(<span class="string">"dbtable"</span>,<span class="string">"vinx.emp"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    .option(<span class="string">"user"</span>,<span class="string">"vinx"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    .option(<span class="string">"password"</span>,<span class="string">"123456"</span>).load</span></pre></td></tr></table></figure><p>将DataFrame注册成表：</p><p><code>oracleDF.registerTempTable(&quot;emp&quot;)</code></p><p>执行查询，并通过Web Console监控执行的时间：</p><p><code>spark.sql(&quot;select * from emp&quot;).show</code></p><p><img src="https://vinxikk.github.io/img/spark/sql-optimize-1.png" alt="查询耗时"></p><p>将表进行缓存，并查询两次，并通过Web Console监控执行的时间：</p><p><code>spark.sqlContext.cacheTable(&quot;emp&quot;)</code></p><p><img src="https://vinxikk.github.io/img/spark/sql-optimize-2.png" alt="cache后的查询耗时"></p><p>清空缓存：</p><p><code>spark.sqlContext.cacheTable(&quot;emp&quot;)</code></p><p><code>spark.sqlContext.clearCache</code></p><h3 id="性能优化相关参数"><a href="#性能优化相关参数" class="headerlink" title="性能优化相关参数"></a>性能优化相关参数</h3><p>将数据缓存到内存中的相关优化参数：</p><ul><li><p><code>spark.sql.inMemoryColumnarStorage.compressed</code></p><p>默认为true</p><p>Spark SQL 将会基于统计信息自动地为每一列选择一种压缩编码方式。</p></li><li><p><code>spark.sql.inMemoryColumnarStorage.batchSize</code></p><p>默认值：10000</p><p>缓存批处理大小。缓存数据时, 较大的批处理大小可以提高内存利用率和压缩率，但同时也会带来 OOM(Out Of Memory)的风险。</p></li></ul><p>其他性能相关的配置选项（不过不推荐手动修改，可能在后续版本自动的自适应修改）：</p><ul><li><p><code>spark.sql.files.maxPartitionBytes</code></p><p>默认值：128MB</p><p>读取文件时单个分区可容纳的最大字节数</p></li><li><p><code>spark.sql.files.openCostInBytes</code></p><p>默认值：4M</p><p>打开文件的估算成本, 按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度)。</p></li><li><p><code>spark.sql.autoBroadcastJoinThreshold</code></p><p>默认值：10M</p><p>用于配置一个表在执行 join 操作时能够广播给所有 worker 节点的最大字节大小。通过将这个值设置为 -1 可以禁用广播。注意，当前数据统计仅支持已经运行了 <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code> 命令的 Hive Metastore 表。</p></li><li><p><code>spark.sql.shuffle.partitions</code></p><p>默认值：200</p><p>用于配置 join 或聚合操作混洗（shuffle）数据时使用的分区数。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL常用数据源</title>
      <link href="/2018/05/25/spark/spark-sql-datasource/"/>
      <url>/2018/05/25/spark/spark-sql-datasource/</url>
      
        <content type="html"><![CDATA[<h3 id="通用的Load-Save函数"><a href="#通用的Load-Save函数" class="headerlink" title="通用的Load/Save函数"></a>通用的Load/Save函数</h3><h4 id="什么是parquet文件"><a href="#什么是parquet文件" class="headerlink" title="什么是parquet文件"></a>什么是parquet文件</h4><p>Parquet是列式存储格式的一种文件类型，列式存储有以下的核心：</p><ul><li>可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。</li><li>压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如，Run Length Encoding和Delta Encoding），进一步节约存储空间。</li><li>只读取需要的列，支持向量运算，能够获取更好的扫描性能。</li><li>Parquet格式是Spark SQL的默认数据源，可通过<code>spark.sql.sources.default</code>配置</li></ul><h4 id="Load-Save函数"><a href="#Load-Save函数" class="headerlink" title="Load/Save函数"></a>Load/Save函数</h4><p>读取parquet文件：</p><p><code>val usersDF = spark.read.load(&quot;/root/data/users.parquet&quot;)</code></p><p>查询schema和数据：</p><p><img src="https://vinxikk.github.io/img/spark/sql-datasource-schema-show.png" alt="查看schema和数据"></p><p>查询用户的name和喜好颜色，并保存：</p><p><code>usersDF.select($&quot;name&quot;,$&quot;favorite_color&quot;).write.save(&quot;/root/result/parquet&quot;)</code></p><p>验证结果：</p><p><img src="https://vinxikk.github.io/img/spark/sql-datasource-verify-result.png" alt="读取保存的parquet文件"></p><h4 id="显式加载json格式文件"><a href="#显式加载json格式文件" class="headerlink" title="显式加载json格式文件"></a>显式加载json格式文件</h4><p>直接加载：<code>val usersDF = spark.read.load(&quot;/home/vinx/data/people.json&quot;)</code>，会出错。</p><p>正确方式：<code>val usersDF = spark.read.format(&quot;json&quot;).load(&quot;/home/vinx/data/people.json&quot;)</code></p><h4 id="存储模式（Save-Mode）"><a href="#存储模式（Save-Mode）" class="headerlink" title="存储模式（Save Mode）"></a>存储模式（Save Mode）</h4><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。</p><p>SaveMode详细介绍如下表：</p><p><img src="https://vinxikk.github.io/img/spark/sql-datasource-save-mode.png" alt="Save Mode"></p><p>示例：</p><p><code>usersDF.select($&quot;name&quot;).write.save(&quot;/root/result/parquet1&quot;)</code>，出错，因为/root/result/parquet1已经存在。</p><p>正确方式：<code>usersDF.select($&quot;name&quot;).write.mode(&quot;overwrite&quot;).save(&quot;/root/result/parquet1&quot;)</code></p><h4 id="将结果保存为表"><a href="#将结果保存为表" class="headerlink" title="将结果保存为表"></a>将结果保存为表</h4><p><code>usersDF.select($&quot;name&quot;).write.saveAsTable(&quot;table1&quot;)</code></p><p>也可以进行分区、分桶等操作：<code>partitionBy</code>、<code>bucketBy</code>。</p><h3 id="parquet文件"><a href="#parquet文件" class="headerlink" title="parquet文件"></a>parquet文件</h3><p>Parquet是一个列格式而且用于多个数据处理系统中。Spark SQL提供支持对于Parquet文件的读写，也就是自动保存原始数据的schema。当写Parquet文件时，所有的列被自动转化为nullable，因为兼容性的缘故。</p><p>案例：</p><p>读入json格式的数据，将其转换成parquet格式，并创建相应的表来使用SQL进行查询。</p><p><img src="https://vinxikk.github.io/img/spark/sql-datasource-parquet-demo.png" alt="parquet文件案例"></p><h4 id="schema的合并"><a href="#schema的合并" class="headerlink" title="schema的合并"></a>schema的合并</h4><p>parquet支持schema evolution（schema演变，即：合并）。用户可以先定义一个简单的schema，然后逐渐的向schema中增加列描述。通过这种方式，用户可以获取多个有不同schema但相互兼容的parquet文件。</p><p><img src="https://vinxikk.github.io/img/spark/sql-datasource-parquet-schema-evolution.png" alt="parquet文件schema的合并"></p><h3 id="JSON-Dataset"><a href="#JSON-Dataset" class="headerlink" title="JSON Dataset"></a>JSON Dataset</h3><p>Spark SQL能自动解析JSON数据集的schema，读取JSON数据集为DataFrame格式。读取JSON数据集方法为<code>SQLContext.read().json()</code>。该方法将string格式的RDD或JSON文件转换为DataFrame。</p><p>需要注意的是，这里的JSON文件不是常规的JSON格式。JSON文件每一行必须包含一个独立的、自满足有效的JSON对象。如果用多行描述一个JSON对象，会导致读取出错。读取JSON数据集示例如下：</p><p>定义路径：</p><p><code>val path =&quot;/home/vinx/data/people.json&quot;</code></p><p>读取json文件，生成DataFrame：</p><p><code>val peopleDF = spark.read.json(path)</code></p><p>打印schema结构信息：</p><p><code>peopleDF.printSchema()</code></p><p>创建临时视图：</p><p><code>peopleDF.createOrReplaceTempView(&quot;people&quot;)</code></p><p>执行查询：</p><p><code>spark.sql(&quot;SELECT name FROM people WHERE age=19&quot;).show</code></p><h3 id="使用JDBC"><a href="#使用JDBC" class="headerlink" title="使用JDBC"></a>使用JDBC</h3><p>Spark SQL同样支持通过JDBC读取其他数据库的数据作为数据源。</p><p>Demo演示：使用Spark SQL读取Oracle数据库中的表。</p><p>启动Spark Shell的时候，指定Oracle数据库的驱动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">spark-shell \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--master spark://rshost001:7077 \       </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--jars /home/vinx/jars/ojdbc6.jar \        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--driver-class-path /home/vinx/jars/ojdbc6.jar</span></pre></td></tr></table></figure><p>读取Oracle数据库中的数据：</p><p><img src="https://vinxikk.github.io/img/spark/sql-datasource-oracle-property.png" alt="Oracle属性说明"></p><p><strong>方式一：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> oracleDF = spark.read.format(<span class="string">"jdbc"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    .option(<span class="string">"url"</span>,<span class="string">"jdbc:oracle:thin:@192.168.xxx.xxx:1521/orcl.example.com"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    .option(<span class="string">"dbtable"</span>,<span class="string">"vinx.emp"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    .option(<span class="string">"user"</span>,<span class="string">"vinx"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    .option(<span class="string">"password"</span>,<span class="string">"123456"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    .load</span></pre></td></tr></table></figure><p><strong>方式二：</strong></p><p>导入需要的类：</p><p><code>import java.util.Properties</code>   </p><p>定义属性：</p><p><code>val oracleprops = new Properties()</code></p><p><code>oracleprops.setProperty(&quot;user&quot;,&quot;vinx&quot;)</code></p><p><code>oracleprops.setProperty(&quot;password&quot;,&quot;123456&quot;)</code></p><p>读取数据：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> oracleEmpDF = spark.read</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">.jdbc(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">        <span class="string">"jdbc:oracle:thin:@192.168.xxx.xxx:1521/orcl.example.com"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="string">"vinx.emp"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        oracleprops</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    )</span></pre></td></tr></table></figure><h3 id="使用Hive-Table"><a href="#使用Hive-Table" class="headerlink" title="使用Hive Table"></a>使用Hive Table</h3><p>首先，搭建好Hive的环境（需要Hadoop）。</p><p>配置Spark SQL支持Hive：</p><ul><li><p>将以下文件拷贝到<code>$SPARK_HOME/conf</code>的目录下</p><p><code>$HIVE_HOME/conf/hive-site.xml</code></p><p><code>$HADOOP_CONF_DIR/core-site.xml</code></p><p><code>$HADOOP_CONF_DIR/hdfs-site.xml</code></p></li></ul><p>使用Spark Shell操作Hive：</p><ul><li><p>启动Spark Shell的时候，需要使用–jars指定mysql的驱动程序</p></li><li><p>创建表</p><p><code>spark.sql(&quot;create table test (key INT, value STRING) row format delimited fields terminated by &#39;,&#39;&quot;)</code></p></li><li><p>导入数据</p><p><code>spark.sql(&quot;load data local path &#39;/home/vinx/data/test.txt&#39; into table test&quot;)</code></p></li><li><p>查询数据</p><p><code>spark.sql(&quot;select * from test&quot;).show</code></p></li></ul><p>使用spark-sql操作Hive：</p><ul><li><p>启动spark-sql的时候，需要使用–jars指定mysql的驱动程序</p></li><li><p>操作Hive</p><p><code>show tables;</code></p><p><code>select * from tb_name;</code></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL基本概念&amp;DataFrame&amp;Dataset</title>
      <link href="/2018/05/24/spark/spark-sql/"/>
      <url>/2018/05/24/spark/spark-sql/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是Spark-SQL"><a href="#什么是Spark-SQL" class="headerlink" title="什么是Spark SQL"></a>什么是Spark SQL</h3><p><img src="https://vinxikk.github.io/img/spark/what-is-spark-sql.png" alt="Spark SQL是什么"></p><p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。</p><p>Hive是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性。</p><p>由于MapReduce这种计算模型执行效率比较慢，所以Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！同时Spark SQL也支持从Hive中读取数据。</p><h4 id="Dataset和DataFrame"><a href="#Dataset和DataFrame" class="headerlink" title="Dataset和DataFrame"></a>Dataset和DataFrame</h4><p><strong>DataFrame：</strong></p><p>DataFrame是组织成命名列的数据集，它在概念上等同于关系数据库中的表，但在底层具有更丰富的优化。DataFrame可以从各种来源构建，例如：</p><ul><li>结构化数据文件</li><li>Hive中的表</li><li>外部数据库或现有RDD</li></ul><p><img src="https://vinxikk.github.io/img/spark/sql-rdd-dataframe.png" alt="RDD和DataFrame"></p><p>从上图可以看出，DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合，DataFrame是分布式的Row对象的集合。</p><p>DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化。</p><p><strong>Dataset：</strong></p><p>Dataset是数据的分布式集合。Dataset是在Spark 1.6中添加的一个新接口，是DataFrame之上更高一级的抽象。它提供了RDD的优点（强类型化，使用强大的lambda函数的能力）以及Spark SQL优化后的执行引擎的优点。一个Dataset 可以从JVM对象构造，然后使用函数转换（map， flatMap，filter等）去操作。 Dataset API 支持Scala和Java。 Python不支持Dataset API。</p><h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><h4 id="通过case-class创建DataFrame"><a href="#通过case-class创建DataFrame" class="headerlink" title="通过case class创建DataFrame"></a>通过case class创建DataFrame</h4><ol><li><p>定义case class（相当于表的结构schema）</p><p><code>case class Emp(empno: Int, ename: String, job: String, mgr: String, hiredate: String, sal: Int, comm: String, deptno: Int)</code></p><p>由于mgr和comm列中包含null值，简单起见，将对应的case class类型定义为String</p></li><li><p>将HDFS上的数据读入RDD，并将RDD与case class关联</p><p><code>val lines = sc.textFile(&quot;hdfs://rshost001:9000/date/emp.csv&quot;).map(_.split(&quot;,&quot;))</code></p><p><code>val allEmp = lines.map(x =&gt; Emp(x(0).toInt, x(1), x(2), x(3), x(4), x(5).toInt, x(6), x(7).toInt))</code></p></li><li><p>将RDD转换成DataFrame</p><p><code>val allEmpDF = allEmp.toDF</code></p></li><li><p>通过DataFrame查询数据</p><p><img src="https://vinxikk.github.io/img/spark/sql-dataframe-example.png" alt="DataFrame示例"></p></li></ol><h4 id="使用SparkSession"><a href="#使用SparkSession" class="headerlink" title="使用SparkSession"></a>使用SparkSession</h4><p>Apache Spark 2.0引入了SparkSession，其为用户提供了一个统一的切入点来使用Spark的各项功能，并且允许用户通过它调用DataFrame和Dataset相关API来编写Spark程序。最重要的是，它减少了用户需要了解的一些概念，使得我们可以很容易地与Spark交互。</p><p>在2.0版本之前，与Spark交互之前必须先创建SparkConf和SparkContext。然而在Spark 2.0中，我们可以通过SparkSession来实现同样的功能，而不需要显式地创建SparkConf, SparkContext 以及 SQLContext，因为这些对象已经封装在SparkSession中。</p><p><img src="https://vinxikk.github.io/img/spark/sql-sparksession.png" alt="SparkSession"></p><ol><li><p>创建StructType，来定义schema结构信息</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> myschema = <span class="type">StructType</span>(<span class="type">List</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"empno"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"ename"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"job"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"mgr"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"hiredate"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"sal"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"comm"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"deptno"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    ))</span></pre></td></tr></table></figure><p>需要：<code>import org.apache.spark.sql.types._</code></p></li><li><p>读入数据并切分数据</p><p><code>val empRDD = sc.textFile(&quot;hdfs://rshost001:9000/data/emp.csv&quot;).map(_.split(&quot;,&quot;))</code></p></li><li><p>将RDD中的数据映射成Row</p><p><code>val rowRDD = empRDD.map(line =&gt; Row(line(0).toInt, line(1), line(2), line(3), line(4), line(5).toInt, line(6), line(7).toInt))</code></p><p>需要：<code>import org.apache.spark.sql.Row</code></p></li><li><p>创建DataFrame</p><p><code>val df = spark.createDataFrame(rowRDD, myschema)</code></p></li></ol><h4 id="使用json文件来创建DataFrame"><a href="#使用json文件来创建DataFrame" class="headerlink" title="使用json文件来创建DataFrame"></a>使用json文件来创建DataFrame</h4><p>源文件：<code>$SPARK_HOME/examples/src/main/resources/people.json</code></p><p><code>val df = spark.read.json(path)</code></p><p>查看数据和schema信息：</p><p><img src="https://vinxikk.github.io/img/spark/sql-json-dataframe.png" alt="通过json创建DF"></p><h4 id="DataFrame操作"><a href="#DataFrame操作" class="headerlink" title="DataFrame操作"></a>DataFrame操作</h4><p>DataFrame操作也称作无类型的Dataset操作。</p><p>查询所有员工的姓名：</p><p><img src="https://vinxikk.github.io/img/spark/sql-df-sql-1.png" alt="查询所有员工的姓名"></p><p>查询所有员工的姓名和薪水，并给薪水加100块钱：</p><p><img src="https://vinxikk.github.io/img/spark/sql-df-sql-2.png" alt="查询所有员工的姓名和薪水"></p><p>查询工资大于2000的员工：</p><p><img src="https://vinxikk.github.io/img/spark/sql-df-sql-3.png" alt="查询工资大于2000的员工"></p><p>求每个部门的员工人数：</p><p><img src="https://vinxikk.github.io/img/spark/sql-df-sql-4.png" alt="求每个部门的员工人数"></p><p>更多请参考：</p><p><strong><a href="http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.sql.Dataset</a></strong></p><p>在DataFrame中使用SQL语句：</p><ol><li><p>将DataFrame注册成表（视图）：<code>df.createOrReplaceTempView(&quot;emp&quot;)</code></p></li><li><p>执行查询：</p><p><code>spark.sql(&quot;select * from emp&quot;).show</code></p><p><code>spark.sql(&quot;select * from emp where deptno=10&quot;).show</code></p><p><code>spark.sql(&quot;select deptno,sum(sal) from emp group by deptno&quot;).show</code></p></li></ol><h4 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h4><p>上面使用的是一个在Session生命周期中的临时views。在Spark SQL中，如果你想拥有一个临时的view，并想在不同的Session中共享，而且在application的运行周期内可用，那么就需要创建一个全局的临时view。并记得使用的时候加上global_temp作为前缀来引用它，因为全局的临时view是绑定到系统保留的数据库global_temp上。</p><ol><li><p>创建一个普通的view和全局的view</p><p><code>df.createOrReplaceTempView(&quot;emp1&quot;)</code><br><code>df.createGlobalTempView(&quot;emp2&quot;)</code></p></li><li><p>在当前会话中执行查询，均可查询出结果</p><p><code>spark.sql(&quot;select * from emp1&quot;).show</code><br><code>spark.sql(&quot;select * from global_temp.emp2&quot;).show</code></p></li><li><p>开启一个新的会话，执行同样的查询</p><p><code>spark.newSession.sql(&quot;select * from emp1&quot;).show</code>     （运行出错）<br><code>spark.newSession.sql(&quot;select * from global_temp.emp2&quot;).show</code></p></li></ol><h3 id="创建Dataset"><a href="#创建Dataset" class="headerlink" title="创建Dataset"></a>创建Dataset</h3><p>DataFrame的引入，可以让Spark更好的处理结构数据的计算，但其中一个主要的问题是：缺乏编译时类型安全。为了解决这个问题，Spark采用新的Dataset API (DataFrame API的类型扩展)。</p><p><img src="https://vinxikk.github.io/img/spark/sql-dataset.png" alt="Dataset与RDD、DataFrame的关系"></p><p>Dataset是一个分布式的数据收集器。这是在Spark1.6之后新加的一个接口，兼顾了RDD的优点（强类型，可以使用功能强大的lambda）以及Spark SQL的执行器高效性的优点。所以可以把DataFrames看成是一种特殊的Datasets，即：Dataset(Row)。</p><h4 id="使用序列创建Dataset"><a href="#使用序列创建Dataset" class="headerlink" title="使用序列创建Dataset"></a>使用序列创建Dataset</h4><p>定义case class：</p><p><code>case class People(id: Int, name: String)</code></p><p>生成序列，并创建Dataset：</p><p><code>val ds = Seq(People(1, &quot;Tom&quot;), People(2, &quot;Mary&quot;)).toDS</code></p><p>查看结果：</p><p>scala&gt; <code>ds.show</code><br>+—+—-+<br>| id|name|<br>+—+—-+<br>|  1| Tom|<br>|  2|Mary|<br>+—+—-+</p><p>scala&gt; <code>ds.collect</code><br><code>res1: Array[People] = Array(People(1,Tom), People(2,Mary))</code></p><h4 id="使用json创建Dataset"><a href="#使用json创建Dataset" class="headerlink" title="使用json创建Dataset"></a>使用json创建Dataset</h4><p>定义case class：</p><p><code>case class Person(name: String, gender: String)</code></p><p>通过json数据生成DataFrame：</p><p><code>val df = spark.read.json(sc.parallelize(&quot;&quot;&quot;{&quot;gender&quot;: &quot;Male&quot;, &quot;name&quot;: &quot;Tom&quot;}&quot;&quot;&quot; :: Nil))</code></p><p>将DataFrame转成Dataset：</p><p><code>df.as[Person].show</code></p><p><code>df.as[Person].collect</code></p><h4 id="使用HDFS数据创建Dataset"><a href="#使用HDFS数据创建Dataset" class="headerlink" title="使用HDFS数据创建Dataset"></a>使用HDFS数据创建Dataset</h4><p>读取HDFS数据，并创建Dataset：</p><p><code>val linesDS = spark.read.text(&quot;hdfs://rshost001:9000/data/data.txt&quot;).as[String]</code></p><p>对Dataset进行操作，分词后，查询长度大于3的单词：</p><p><code>val words = linesDS.flatMap(_.split(&quot; &quot;)).filter(_.length &gt; 3)</code></p><p><code>words.show</code></p><p><code>words.collect</code></p><p>执行WordCount程序：</p><p><code>val result = linesDS.flatMap(_.split(&quot; &quot;)).map((_,1)).groupByKey(x =&gt; x._1).count</code></p><p><code>result.show</code></p><p>排序：</p><p><code>result.orderBy($&quot;value&quot;).show</code></p><h4 id="Dataset案例"><a href="#Dataset案例" class="headerlink" title="Dataset案例"></a>Dataset案例</h4><p>使用emp.json生成DataFrame：</p><p><code>val empDF = spark.read.json(&quot;/home/vinx/data/emp.json&quot;)</code></p><p>查询工资大于3000的员工：</p><p><code>empDF.where($&quot;sal&quot; &gt;= 3000).show</code></p><p>创建case class：</p><p><code>case class Emp(empno:Long,ename:String,job:String,hiredate:String,mgr:String,sal:Long,comm:String,deptno:Long)</code></p><p>生成Dataset，并查询数据：</p><p><code>val empDS = empDF.as[Emp]</code></p><p>查询工资大于3000的员工：</p><p><code>empDS.filter(_.sal &gt; 3000).show</code></p><p>查看10号部门的员工：</p><p><code>empDS.filter(_.deptno == 10).show</code></p><p>多表查询：</p><ol><li><p>创建部门表</p><p><code>val deptRDD=sc.textFile(&quot;/home/vinx/data/dept.csv&quot;).map(_.split(&quot;,&quot;))</code></p><p><code>case class Dept(deptno:Int,dname:String,loc:String)</code></p><p><code>val deptDS = deptRDD.map(x=&gt;Dept(x(0).toInt,x(1),x(2))).toDS</code></p></li><li><p>创建员工表</p><p><code>val empRDD = sc.textFile(&quot;/home/vinx/data/emp.csv&quot;).map(_.split(&quot;,&quot;))</code></p><p><code>case class Emp(empno:Int,ename:String,job:String,mgr:String,hiredate:String,sal:Int,comm:String,deptno:Int)</code></p><p><code>val empDS = empRDD.map(x =&gt; Emp(x(0).toInt,x(1),x(2),x(3),x(4),x(5).toInt,x(6),x(7).toInt)).toDS</code></p></li><li><p>执行多表查询，等值连接</p><p><code>val result = deptDS.join(empDS,&quot;deptno&quot;)</code></p><p>另一种写法：</p><p><code>val result = deptDS.joinWith(empDS,deptDS(&quot;deptno&quot;)=== empDS(&quot;deptno&quot;))</code></p><p><code>joinWith</code>和<code>join</code>的区别是连接后的新Dataset的schema会不一样</p></li><li><p>查看执行计划：<code>result.explain</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD的血缘关系：窄依赖和宽依赖&amp;Spark任务中的Stage</title>
      <link href="/2018/05/22/spark/spark-dependency/"/>
      <url>/2018/05/22/spark/spark-dependency/</url>
      
        <content type="html"><![CDATA[<h3 id="RDD的依赖关系"><a href="#RDD的依赖关系" class="headerlink" title="RDD的依赖关系"></a>RDD的依赖关系</h3><p>RDD和它依赖的父RDD(s)的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p><p><img src="https://vinxikk.github.io/img/spark/rdd-narrow-wide-dependency.png" alt="RDD的窄依赖和宽依赖"></p><p>窄依赖：每个parent RDD的partition最多被child RDD的一个partition使用。</p><p>宽依赖：每个parent RDD的partition被多个child RDD的partition使用。</p><p>窄依赖每个child RDD的partition的生成操作都是可以并行的，而宽依赖则需要所有的parent RDD partition shuffle结果得到后再进行。</p><h3 id="org-apache-spark-Dependency-scala源码"><a href="#org-apache-spark-Dependency-scala源码" class="headerlink" title="org.apache.spark.Dependency.scala源码"></a>org.apache.spark.Dependency.scala源码</h3><p><code>Dependency</code>是一个抽象类：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dependency.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Dependency</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>它有两个子类：<code>NarrowDependency</code> 和 <code>ShuffleDenpendency</code>，分别对应窄依赖和宽依赖。</p><h4 id="NarrowDependency抽象类"><a href="#NarrowDependency抽象类" class="headerlink" title="NarrowDependency抽象类"></a>NarrowDependency抽象类</h4><p>定义了抽象方法<code>getParents</code>，输入<code>partitionId</code>，用于获得child RDD 的某个partition依赖的parent RDD的所有 partitions。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dependency.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">_rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">T</span>] </span>&#123;  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * Get the parent partitions for a child partition.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * @param partitionId a partition of the child RDD</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * @return the partitions of the parent RDD that the child partition depends upon</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">Seq</span>[<span class="type">Int</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>] = _rdd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>窄依赖有两种具体的实现：<code>OneToOneDependency</code>和<code>RangeDependency</code>。</p><p><code>OneToOneDependency</code>指child RDD的partition只依赖于parent RDD 的一个partition，产生<code>OneToOneDependency</code>的算子有map，filter，flatMap等。</p><p>可以看到<code>getParents</code>实现很简单，就是传进去一个<code>partitionId</code>，再把<code>partitionId</code>放在List里面传出去。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(partitionId)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p><code>RangeDependency</code>指child RDD partition在一定的范围内一对一的依赖于parent RDD partition，主要用于union。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//inStart表示parent RDD的开始索引，outStart表示child RDD 的开始索引</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RangeDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>], inStart: <span class="type">Int</span>, outStart: <span class="type">Int</span>, length: <span class="type">Int</span></span>)  </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = &#123;    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="type">List</span>(partitionId - outStart + inStart) <span class="comment">//表示于当前索引的相对位置</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="type">Nil</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h4 id="ShuffleDependency宽依赖"><a href="#ShuffleDependency宽依赖" class="headerlink" title="ShuffleDependency宽依赖"></a>ShuffleDependency宽依赖</h4><p>表示一个parent RDD的partition会被child RDD的partition使用多次，需要经过shuffle才能形成。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],    </span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val partitioner: <span class="type">Partitioner</span>,    </span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] </span>&#123;  <span class="comment">//shuffle都是基于PairRDD进行的，所以传入的RDD要是key-value类型的</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] = _rdd.asInstanceOf[<span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> keyClassName: <span class="type">String</span> = reflect.classTag[<span class="type">K</span>].runtimeClass.getName</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> valueClassName: <span class="type">String</span> = reflect.classTag[<span class="type">V</span>].runtimeClass.getName</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> combinerClassName: <span class="type">Option</span>[<span class="type">String</span>] =</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="type">Option</span>(reflect.classTag[<span class="type">C</span>]).map(_.runtimeClass.getName)  <span class="comment">//获取shuffleId</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> shuffleId: <span class="type">Int</span> = _rdd.context.newShuffleId()  <span class="comment">//向shuffleManager注册shuffle信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    shuffleId, _rdd.partitions.length, <span class="keyword">this</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(<span class="keyword">this</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>由于shuffle涉及到网络传输，所以要有序列化serializer，为了减少网络传输，可以map端聚合，通过mapSideCombine和aggregator控制，还有key排序相关的keyOrdering，以及重输出的数据如何分区的partitioner，还有一些class信息。Partition之间的关系在shuffle处戛然而止，因此shuffle是划分stage的依据。</p><h4 id="两种依赖的区分"><a href="#两种依赖的区分" class="headerlink" title="两种依赖的区分"></a>两种依赖的区分</h4><p>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。例如，逐个元素地执行map、然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点之间进行Shuffle，这与MapReduce类似。</p><p>第二，窄依赖能够更有效地进行失效节点的恢复，即只需重新计算丢失RDD分区的父分区，而且不同节点之间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。</p><h3 id="Spark任务中的Stage"><a href="#Spark任务中的Stage" class="headerlink" title="Spark任务中的Stage"></a>Spark任务中的Stage</h3><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。</p><p><img src="https://vinxikk.github.io/img/spark/rdd-stage.png" alt="RDD的Stage"></p><p>RDD存在着依赖关系，这些依赖关系形成了有向无环图DAG，DAG通过DAGScheduler进行Stage的划分，并基于每个Stage生成了TaskSet，提交给TaskScheduler。</p><h4 id="作业的提交"><a href="#作业的提交" class="headerlink" title="作业的提交"></a>作业的提交</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// SparkContext.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">progressBar.foreach(_.finishAll())</span></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span></pre></td></tr></table></figure><p>可以看到，SparkContext的<code>runjob</code>方法调用了DAGScheduler的<code>runjob</code>方法正式向集群提交任务，最终调用了<code>submitJob</code>方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    callSite: <span class="type">CallSite</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach &#123; p =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">            <span class="string">"Attempting to access a non-existent partition: "</span> + p + <span class="string">". "</span> +</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            <span class="string">"Total number of partitions: "</span> + maxPartitions)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">// Return immediately if the job is running 0 tasks</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">    assert(partitions.size &gt; <span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//给eventProcessLoop发送JobSubmitted消息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        jobId, rdd, func2, partitions.toArray, callSite, waiter,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        <span class="type">SerializationUtils</span>.clone(properties)))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    waiter</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>这里向<code>eventProcessLoop</code>对象发送了<code>JobSubmitted</code>消息。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[scheduler] <span class="keyword">val</span> eventProcessLoop = <span class="keyword">new</span> <span class="type">DAGSchedulerEventProcessLoop</span>(<span class="keyword">this</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  eventProcessLoop是<span class="type">DAGSchedulerEventProcessLoop</span>类的一个对象。</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">StageCancelled</span>(stageId) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleStageCancellation(stageId)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">JobCancelled</span>(jobId) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleJobCancellation(jobId)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">JobGroupCancelled</span>(groupId) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleJobGroupCancelled(groupId)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">AllJobsCancelled</span> =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.doCancelAllJobs()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorAdded</span>(execId, host) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleExecutorAdded(execId, host)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorLost</span>(execId, reason) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> filesLost = reason <span class="keyword">match</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">case</span> <span class="type">SlaveLost</span>(_, <span class="literal">true</span>) =&gt; <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="literal">false</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleExecutorLost(execId, filesLost)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">BeginEvent</span>(task, taskInfo) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleBeginEvent(task, taskInfo)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">GettingResultEvent</span>(taskInfo) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleGetTaskResult(taskInfo)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> completion: <span class="type">CompletionEvent</span> =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleTaskCompletion(completion)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">TaskSetFailed</span>(taskSet, reason, exception) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">ResubmitFailedStages</span> =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.resubmitFailedStages()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure><p><code>DAGSchedulerEventProcessLoop</code>对接收到的消息进行处理，在<code>doOnReceive</code>方法中形成一个event loop。<br>接下来将调用<code>submitStage()</code>方法进行stage的划分。</p><h4 id="stage的划分"><a href="#stage的划分" class="headerlink" title="stage的划分"></a>stage的划分</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> jobId = activeJobForStage(stage)<span class="comment">//查找该Stage的所有激活的job</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (jobId.isDefined) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      logDebug(<span class="string">"submitStage("</span> + stage + <span class="string">")"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)<span class="comment">//得到Stage的父Stage，并排序</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        logDebug(<span class="string">"missing: "</span> + missing)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (missing.isEmpty) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">          logInfo(<span class="string">"Submitting "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">"), which has no missing parents"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">          submitMissingTasks(stage, jobId.get)<span class="comment">//如果Stage没有父Stage，则提交任务集</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">for</span> (parent &lt;- missing) &#123;<span class="comment">//如果有父Stage，递归调用submiStage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            submitStage(parent)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">          waitingStages += stage<span class="comment">//将其标记为等待状态，等待下次提交</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">      abortStage(stage, <span class="string">"No active job for stage "</span> + stage.id, <span class="type">None</span>)<span class="comment">//如果该Stage没有激活的job，则丢弃该Stage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure><p>在<code>submitStage</code>方法中判断Stage的父Stage有没有被提交，直到所有父Stage都被提交，只有等父Stage完成后才能调度子Stage。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>] <span class="comment">//用于存放父Stage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]] <span class="comment">//用于存放已访问过的RDD</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (!visited(rdd)) &#123; <span class="comment">//如果RDD没有被访问过，则进行访问</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        visited += rdd <span class="comment">//添加到已访问RDD的HashSet中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123; <span class="comment">//获取该RDD的依赖</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">            dep <span class="keyword">match</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">              <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;<span class="comment">//若为宽依赖，则该RDD依赖的RDD所在的stage为父stage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)<span class="comment">//生成父Stage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">if</span> (!mapStage.isAvailable) &#123;<span class="comment">//若父Stage不存在，则添加到父Stage的HashSET中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">                  missing += mapStage</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">                &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">              <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt;<span class="comment">//若为窄依赖，则继续访问父RDD</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">                waitingForVisit.push(narrowDep.rdd)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">    waitingForVisit.push(stage.rdd)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;<span class="comment">//循环遍历所有RDD</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      visit(waitingForVisit.pop())</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    missing.toList</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure><p><code>getmissingParentStages()</code>方法为核心方法。</p><p>Stage是通过shuffle划分的，所以每一个Stage都是以shuffle开始的，若一个RDD是宽依赖，则必然说明该RDD的父RDD在另一个Stage中，若一个RDD是窄依赖，则该RDD所依赖的父RDD还在同一个Stage中，我们可以根据这个逻辑，找到该Stage的父Stage。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD高级算子</title>
      <link href="/2018/05/21/spark/spark-high-level-operation/"/>
      <url>/2018/05/21/spark/spark-high-level-operation/</url>
      
        <content type="html"><![CDATA[<h3 id="RDD的高级算子"><a href="#RDD的高级算子" class="headerlink" title="RDD的高级算子"></a>RDD的高级算子</h3><h4 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h4><p>把每个partition中的分区号和对应的值拿出来</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//...</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr></table></figure><p>接收一个函数参数：</p><ul><li>第一个参数：分区号</li><li>第二个参数：分区中的元素</li></ul><p>示例：将每个分区中的元素和分区号打印出来。</p><p><code>val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)</code></p><p>创建一个函数返回RDD中的每个分区号和元素：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func1</span></span>(index:<span class="type">Int</span>, iter:<span class="type">Iterator</span>[<span class="type">Int</span>]):<span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">   iter.toList.map( x =&gt; <span class="string">"[PartID:"</span> + index + <span class="string">", value="</span> + x + <span class="string">"]"</span> ).iterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>调用：<code>rdd1.mapPartitionsWithIndex(func1).collect</code></p><p>结果：</p><p><code>res18: Array[String] = Array([PartID:0, value=1], [PartID:0, value=2], [PartID:0, value=3], [PartID:0, value=4], [PartID:1, value=5], [PartID:1, value=6], [PartID:1, value=7], [PartID:1, value=8], [PartID:1, value=9])</code></p><h4 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h4><p>先对局部聚合，再对全局聚合。</p><p><img src="https://vinxikk.github.io/img/spark/rdd-aggregate.png" alt="aggregate算子"></p><p><strong>示例1（数字）：</strong></p><p><code>val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)</code></p><p>查看每个分区中的元素：</p><p>scala&gt; <code>rdd1.mapPartitionsWithIndex(func1).collect</code><br><code>res19: Array[String] = Array([PartID:0, value=1], [PartID:0, value=2], [PartID:1, value=3], [PartID:1, value=4], [PartID:1, value=5])</code></p><p>将每个分区中的最大值求和（初始值是0）：</p><p>scala&gt; <code>rdd1.aggregate(0)(math.max(_,_),_+_)</code><br><code>res20: Int = 7</code></p><p>如果初始值是10，则结果为30：</p><p>scala&gt; <code>rdd1.aggregate(10)(math.max(_,_),_+_)</code><br><code>res21: Int = 30</code></p><p>如果是求和（初始值为0）：</p><p>scala&gt; <code>rdd1.aggregate(0)(_+_,_+_)</code><br><code>res22: Int = 15</code></p><p>如果初始值是10，则结果是45：</p><p>scala&gt; <code>rdd1.aggregate(10)(_+_,_+_)</code><br><code>res23: Int = 45</code></p><p><strong>示例2（字符串）：</strong></p><p><code>val rdd2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;),2)</code></p><p>修改查看分区元素的函数：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func2</span></span>(index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  iter.toList.map(x =&gt; <span class="string">"[partID:"</span> +  index + <span class="string">", val: "</span> + x + <span class="string">"]"</span>).iterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>两个分区中的元素：</p><p><code>[partID:0, val: a], [partID:0, val: b], [partID:0, val: c],</code><br><code>[partID:1, val: d], [partID:1, val: e], [partID:1, val: f]</code></p><p>运行结果：</p><p>scala&gt; <code>rdd2.aggregate(&quot;&quot;)(_+_,_+_)</code><br><code>res25: String = abcdef</code></p><p>scala&gt; <code>rdd2.aggregate(&quot;|&quot;)(_+_,_+_)</code><br><code>res26: String = ||abc|def</code></p><p><strong>示例3：</strong></p><p><code>val rdd3 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;4567&quot;),2)</code></p><p><code>rdd3.aggregate(&quot;&quot;)((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y)</code></p><p>结果可能是：”24”，也可能是：”42”</p><p><code>val rdd4 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;&quot;),2)</code></p><p><code>rdd4.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)</code></p><p>结果是：”10”，也可能是”01”</p><p>原因：注意有个初始值””，其长度0，然后0.toString变成字符串</p><p><code>val rdd5 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;&quot;,&quot;345&quot;),2)</code></p><p><code>rdd5.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)</code></p><p>结果是：”11”，原因同上。</p><h4 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h4><p>准备数据：</p><p><code>val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 2)</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func3</span></span>(index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Int</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  iter.toList.map(x =&gt; <span class="string">"[partID:"</span> +  index + <span class="string">", val: "</span> + x + <span class="string">"]"</span>).iterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>两个分区中的元素：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-aggregate-example.png" alt="分区中的元素"></p><p>将每个分区中的动物最多的个数求和：</p><p>scala&gt; <code>pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect</code><br><code>res69: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))</code></p><p>将每种动物个数求和：</p><p>scala&gt; <code>pairRDD.aggregateByKey(0)(_ + _, _ + _).collect</code><br><code>res71: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))</code></p><p>也可以使用<code>reduceByKey</code>：</p><p>scala&gt; <code>pairRDD.reduceByKey(_ + _).collect</code><br><code>res73: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))</code></p><h4 id="coalesce与repartition"><a href="#coalesce与repartition" class="headerlink" title="coalesce与repartition"></a>coalesce与repartition</h4><p>都是将RDD中的分区进行重分区。</p><p>区别是，coalesce默认不会进行shuffle(false)，而repartition会进行shuffle(true)，即会将数据真正通过网络进行重分区。</p><p>示例：</p><p><code>val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func4</span></span>(index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">Int</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  iter.toList.map(x =&gt; <span class="string">"[partID:"</span> +  index + <span class="string">", val: "</span> + x + <span class="string">"]"</span>).iterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>下面两句话是等价的：</p><p><code>val rdd2 = rdd1.repartition(3)</code><br><code>val rdd3 = rdd1.coalesce(3,true)</code> =&gt;如果是false，查看RDD的length依然是2</p><h3 id="aggregate和aggregateByKey的区别"><a href="#aggregate和aggregateByKey的区别" class="headerlink" title="aggregate和aggregateByKey的区别"></a>aggregate和aggregateByKey的区别</h3><h4 id="aggregate-1"><a href="#aggregate-1" class="headerlink" title="aggregate"></a>aggregate</h4><p>aggregate函数的签名如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span></span></pre></td></tr></table></figure><p>这个函数是一个柯里化的方法，输入参数分为两个部分：<code>(zeroValue: U)</code>和<code>(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U)</code>。</p><p>aggregate先对每个分区的元素做聚合，然后对所有分区的结果做聚合，聚合过程中，使用的是给定的聚集函数以及初始值<code>zeroValue</code>。</p><p>这个函数能返回一个与原始RDD不同的类型U，因此，需要一个合并RDD类型T到结果类型U的函数，还需要一个合并类型U的函数。这两个函数都可以修改和返回他们的第一个参数，而不是重新构建一个U类型的参数以避免重新分配内存。</p><p><code>zeroValue</code>：seqOp的每个分区的累积结果的初始值以及combOp的不同分区的组合结果的初始值，这通常将是初始元素（例如，”Nil”表的列表连接或”0”表示求和）。</p><p><code>seqOp</code>：每个分区累积结果的聚合函数。</p><p><code>combOp</code>：用于组合不同分区的结果。</p><p>seqOp操作会聚合各分区中的元素，然后combOp操作把所有分区的聚合结果再次聚合，两个操作的初始值都是zeroValue。</p><p>seqOp的操作是遍历分区中的所有元素(T)，第一个T跟zeroValue做操作，结果再作为与第二个T做操作的zeroValue，直到遍历完整个分区。</p><p>combOp操作是把各分区聚合的结果，再聚合。</p><p>aggregate函数返回一个跟RDD不同类型的值。因此，需要一个操作seqOp来把分区中的元素T合并成一个U，另外一个操作combOp把所有U聚合。</p><p><strong>示例1（求平均值）：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> (mul, sum, count) = sc.parallelize(list, <span class="number">2</span>).aggregate((<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>))(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    (acc, number) =&gt; (acc._1 * number, acc._2 + number, acc._3 + <span class="number">1</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    (x, y) =&gt; (x._1 * y._1, x._2 + y._2, x._3 + y._3)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(sum / count, mul)</span></pre></td></tr></table></figure><p>sum是求和，count是累积元素的个数，mul是求各元素的乘积。</p><p>具体过程：</p><ol><li>初始值是(1, 0, 0)</li><li>number是函数中的T，也就是list中的元素，此时类型为Int。而acc的类型为(Int, Int, Int)。<code>acc._1 * num</code>是各元素相乘(初始值为1)，<code>acc._2 + number</code>为各元素相加，<code>acc._3 + 1</code>为累积元素的个数。</li><li><code>sum / count</code>用来计算平均值</li></ol><p><strong>示例2：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> raw = <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"d"</span>, <span class="string">"f"</span>, <span class="string">"g"</span>, <span class="string">"h"</span>, <span class="string">"o"</span>, <span class="string">"q"</span>, <span class="string">"x"</span>, <span class="string">"y"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> (biggerthanf, lessthanf) = sc.parallelize(raw, <span class="number">1</span>).aggregate((<span class="number">0</span>, <span class="number">0</span>))(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    (cc, str) =&gt; &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">var</span> biggerf = cc._1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">var</span> lessf = cc._2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (str.compareTo(<span class="string">"f"</span>) &gt;= <span class="number">0</span>) biggerf = cc._1 + <span class="number">1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(str.compareTo(<span class="string">"f"</span>) &lt; <span class="number">0</span>) lessf = cc._2 + <span class="number">1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        (biggerf, lessf)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    (x, y) =&gt; (x._1 + y._1, x._2 + y._2)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr></table></figure><p>统计在raw这个List中，比”f”大与比”f”小的元素分别有多少个。</p><h4 id="aggregateByKey-1"><a href="#aggregateByKey-1" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h4><p><code>aggregate</code>是针对序列的操作，<code>aggregateByKey</code>则是针对&lt;k,v&gt;对的操作。顾名思义，<code>aggregateByKey</code>就是针对key做<code>aggregate</code>操作。</p><p>对PairRDD中相同的Key值进行聚合操作，在聚合过程中同样使用了一个中立的初始值。</p><p>和<code>aggregate</code>函数类似，<code>aggregateByKey</code>返回值的类型不需要和RDD中value的类型一致。</p><p>因为<code>aggregateByKey</code>是对相同Key中的值进行聚合操作，所以<code>aggregateByKey</code>函数最终返回的类型还是PairRDD，对应的结果是Key和聚合后的值，而<code>aggregate</code>函数直接返回的是非RDD的结果。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)] = self.withScope &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    aggregateByKey(zeroValue, defaultPartitioner(self))(seqOp, combOp)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>针对&lt;k,v&gt;对的操作，Spark中还有一个<code>combineByKey</code>的函数：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(<span class="literal">null</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p><code>aggregateByKey</code>的实现：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>, partitioner: <span class="type">Partitioner</span>)(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)] = self.withScope &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Serialize the zero value to a byte array so that we can get a new clone of it on each key</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> zeroBuffer = <span class="type">SparkEnv</span>.get.serializer.newInstance().serialize(zeroValue)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> zeroArray = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Byte</span>](zeroBuffer.limit)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    zeroBuffer.get(zeroArray)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> cachedSerializer = <span class="type">SparkEnv</span>.get.serializer.newInstance()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> createZero = () =&gt; cachedSerializer.deserialize[<span class="type">U</span>](<span class="type">ByteBuffer</span>.wrap(zeroArray))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// We will clean the combiner closure later in `combineByKey`</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> cleanedSeqOp = self.context.clean(seqOp)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    combineByKeyWithClassTag[<span class="type">U</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        (v: <span class="type">V</span>) =&gt; cleanedSeqOp(createZero(), v),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        cleanedSeqOp, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        combOp, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        partitioner</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>从源码可以看出，<code>aggregateByKey</code>调用的就是<code>combineByKey</code>方法。</p><p><code>seqOp</code>方法就是<code>mergeValue</code>，<code>combOp</code>方法则是<code>mergeCombiners</code>，<code>cleanedSeqOp(createZero(), v)</code>是<code>createCombiner</code>, 也就是传入的<code>seqOp</code>函数, 只不过其中一个值是传入的zeroValue而已。</p><p>因此, 当<code>createCombiner</code>和<code>mergeValue</code>函数的操作相同, <code>aggregateByKey</code>更为合适。</p><p><strong>示例：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AggregateByKeyOp</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>])&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      .setAppName(<span class="string">"AggregateByKey"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      .setMaster(<span class="string">"local"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> data=<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> rdd=sc.parallelize(data, <span class="number">2</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//合并不同partition中的值，a，b得数据类型为zeroValue的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">combOp</span></span>(a:<span class="type">String</span>,b:<span class="type">String</span>):<span class="type">String</span>=&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">          println(<span class="string">"combOp: "</span>+a+<span class="string">"\t"</span>+b)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">          a+b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//合并在同一个partition中的值，a的数据类型为zeroValue的数据类型，b的数据类型为原value的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">seqOp</span></span>(a:<span class="type">String</span>,b:<span class="type">Int</span>):<span class="type">String</span>=&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">          println(<span class="string">"SeqOp:"</span>+a+<span class="string">"\t"</span>+b)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">          a+b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">      rdd.foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//zeroValue:中立值,定义返回value的类型，并参与运算</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//seqOp:用来在同一个partition中合并值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//combOp:用来在不同partiton中合并值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> aggregateByKeyRDD=rdd.aggregateByKey(<span class="string">"100"</span>)(seqOp, combOp)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">      sc.stop()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>运行过程：</p><p>分区一数据：</p><p>((1,3)<br>(1,2)<br>分区二数据：</p><p>(1,4)<br>(2,3)</p><p>分区一相同key的数据进行合并<br>seq: 100     3   //(1,3)开始和中立值进行合并  合并结果为 1003<br>seq: 1003     2   //(1,2)再次合并 结果为 10032</p><p>分区二相同key的数据进行合并<br>seq: 100     4  //(1,4) 开始和中立值进行合并 1004<br>seq: 100     3  //(2,3) 开始和中立值进行合并 1003</p><p>将两个分区的结果进行合并<br>key为2的，只在一个分区存在，不需要合并 (2,1003)<br>(2,1003)</p><p>key为1的, 在两个分区存在，并且数据类型一致，合并<br>comb: 10032     1004<br>(1,100321004)</p><h3 id="reduceByKey与groupByKey的区别"><a href="#reduceByKey与groupByKey的区别" class="headerlink" title="reduceByKey与groupByKey的区别"></a>reduceByKey与groupByKey的区别</h3><p>在Spark中，一切的操作都是基于RDD的。RDD中有一种特殊的format：pair RDD，即RDD的每一行是一个(key, value)的格式。</p><p><code>reduceByKey(func, numPartitions=None)</code></p><p><em>Merge the values for each key using an associative reduce function. This will also perform the merginglocally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce. Output will be hash-partitioned with numPartitions partitions, or the default parallelism level if numPartitions is not specified.</em></p><p><code>reduceByKey</code>用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。</p><p><code>groupByKey(numPartitions=None)</code></p><p><em>Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions. Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will provide much better performance.</em></p><p><code>groupByKey</code>也是对每个key进行操作，但只生成一个sequence。</p><p>如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择<code>reduceByKey</code>/<code>aggregateByKey</code>更好。这是因为<code>groupByKey</code>不能自定义函数，我们需要先用<code>groupByKey</code>生成RDD，然后才能对此RDD通过map进行自定义函数操作。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> words = <span class="type">Array</span>(<span class="string">"one"</span>, <span class="string">"two"</span>, <span class="string">"two"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordCountsWithReduce = wordPairsRDD.reduceByKey(_ + _)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordCountsWithGroup = wordPairsRDD.groupByKey().map(t =&gt; (t._1, t._2.sum))</span></pre></td></tr></table></figure><p>上面得到的<code>wordCountsWithReduce</code>和<code>wordCountsWithGroup</code>是完全一样的，但是，它们的内部运算过程是不同的。</p><p>当采用<code>reduceByKey</code>时，Spark可以在每个分区移动数据之前将待输出数据与一个共用的key结合。借助下图可以理解在<code>reduceByKey</code>里究竟发生了什么。 注意在数据对被搬移前同一机器上同样的key是怎样被组合的(<code>reduceByKey</code>中的lamdba函数)。然后lamdba函数在每个区上被再次调用来将所有值reduce成一个最终结果。整个过程如下：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-reduce-by-key.png" alt="reduceByKey"></p><p>当采用<code>groupByKey</code>时，由于它不接收函数，spark只能先将所有的键值对(key-value pair)都移动，这样的后果是集群节点之间的开销很大，导致传输延时。整个过程如下：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-group-by-key.png" alt="groupByKey"></p><p>因此，在对大数据进行复杂计算时，<code>reduceByKey</code>优于<code>groupByKey</code>。</p><p>另外，如果仅仅是group处理，那么以下函数应该优于<code>groupByKey</code>：</p><ol><li><code>combineByKey</code>：组合数据，但是组合之后的数据类型与输入时值的类型不一样。</li><li><code>foldByKey</code>：合并每一个key的所有值，在级联函数和“零值”中使用。</li></ol><p><code>reduceByKey</code>是先在单台机器中计算，再将结果进行shuffle，减少运算量。</p><p><code>groupByKey</code>是对RDD中的所有数据做shuffle,根据不同的Key映射到不同的partition中再进行aggregate。</p><p><code>aggregateByKey</code>是先对每个partition中的数据根据不同的Key进行aggregate，然后将结果进行shuffle，完成各个partition之间的aggregate。</p><h3 id="reduceByKey和aggregateByKey"><a href="#reduceByKey和aggregateByKey" class="headerlink" title="reduceByKey和aggregateByKey"></a>reduceByKey和aggregateByKey</h3><p>假设有一系列元组，以用户ID为key，以用户在某一时间点访问的网站为value：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> userAccesses = sc.parallelize(<span class="type">Array</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u1"</span>, <span class="string">"site1"</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u2"</span>, <span class="string">"site1"</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u1"</span>, <span class="string">"site1"</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u2"</span>, <span class="string">"site3"</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u2"</span>, <span class="string">"site4"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">))</span></pre></td></tr></table></figure><p>要对这个列表进行处理，获得某个用户访问过且去重后的所有站点。</p><p>因为<code>groupByKey</code>运算量较大，可选方案有<code>reduceByKey</code>、<code>aggregateByKey</code>。</p><p><code>reduceByKey</code>代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> mapedUserAccess = userAccesses.map(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    userSite =&gt; (userSite._1, <span class="type">Set</span>(userSite._2))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> distinctSite = mapedUserAccess.reduceByKey(_++_)</span></pre></td></tr></table></figure><p>但上述代码的问题是，RDD的每个值都将创建一个Set，如果处理一个巨大的RDD,这些对象将大量吞噬内存，并且对垃圾回收造成压力。</p><p>如果使用<code>aggregateByKey</code>：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> zeroValue = collecyion.mutable.set[<span class="type">String</span>]()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> aggregated = userAccesses.aggregateByKey(zeroValue)(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    (set,v) =&gt; set += v, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    (setOne, setTwo) =&gt; setOne ++= setTwo</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr></table></figure><p>为避免<code>reduceByKey</code>内存问题，可用<code>aggregateByKey</code>。</p><p><code>aggregateByKey</code>函数的使用，需为它提供以下三个参数：</p><ol><li><p>零值（zero）：即聚合的初始值</p></li><li><p>函数f:(U, V)</p><p>把值V合并到数据结构U， 该函数在分区内合并值时使用</p></li><li><p>函数 g:(U, U)</p><p>合并两个数据结构U，在分区间合并值时调用此函数。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark架构和部署&amp;WordCount原理</title>
      <link href="/2018/05/20/spark/spark-basic/"/>
      <url>/2018/05/20/spark/spark-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h3><p>Spark官网：</p><p><a href="http://spark.apache.org/" target="_blank" rel="noopener">http://spark.apache.org/</a></p><p><img src="https://vinxikk.github.io/img/spark/what-is-spark.png" alt="Spark是什么"></p><p>Spark是一个针对大规模数据处理的快速通用引擎。</p><p>Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。</p><h3 id="Spark的架构"><a href="#Spark的架构" class="headerlink" title="Spark的架构"></a>Spark的架构</h3><p><img src="https://vinxikk.github.io/img/spark/spark-cluster-architecture.png" alt="Spark集群架构"></p><p><img src="https://vinxikk.github.io/img/spark/spark-master-worker.png" alt="Spark主从结构"></p><h3 id="Spark部署"><a href="#Spark部署" class="headerlink" title="Spark部署"></a>Spark部署</h3><p>Spark的部署有以下几种模式：</p><ul><li>Standalone</li><li>YARN</li><li>Mesos</li><li>Amason EC2</li></ul><h4 id="Spark-Standalone伪分布式部署"><a href="#Spark-Standalone伪分布式部署" class="headerlink" title="Spark Standalone伪分布式部署"></a>Spark Standalone伪分布式部署</h4><p>配置文件con/spark-env.sh：</p><p><code>export JAVA_HOME=/usr/java/jdk1.8.0_121</code></p><p><code>export SPARK_MASTER_HOST=rshost001</code></p><p><code>export SPARK_MASTER_PORT=7077</code></p><p>下面的可以不写，默认<br><code>export SPARK_WORKER_CORES=1</code><br><code>export SPARK_WORKER_MEMORY=1024m</code></p><p>配置文件conf/slave：</p><p><code>rshost001</code></p><h4 id="Spark-Standalone全分布式部署"><a href="#Spark-Standalone全分布式部署" class="headerlink" title="Spark Standalone全分布式部署"></a>Spark Standalone全分布式部署</h4><p>配置文件con/spark-env.sh：</p><p><code>export JAVA_HOME=/usr/java/jdk1.8.0_121</code></p><p><code>export SPARK_MASTER_HOST=rshost001</code></p><p><code>export SPARK_MASTER_PORT=7077</code></p><p>下面的可以不写，默认<br><code>export SPARK_WORKER_CORES=1</code><br><code>export SPARK_WORKER_MEMORY=1024m</code></p><p>配置文件conf/slave：</p><p><code>rshost002</code></p><p><code>rshost001</code></p><p>启动Spark集群：<code>start-all.sh</code></p><p>Web UI界面：<a href="http://rshost001:8080/" target="_blank" rel="noopener">http://rshost001:8080/</a></p><h3 id="Spark-Demo"><a href="#Spark-Demo" class="headerlink" title="Spark Demo"></a>Spark Demo</h3><h4 id="Spark-Example"><a href="#Spark-Example" class="headerlink" title="Spark Example"></a>Spark Example</h4><p>示例jar包路径：<code>$SPARK_HOME/examples/jars/spark-examples_2.11-2.1.0.jar</code></p><p>示例程序源码：<code>$SPARK_HOME/examples/src/main</code></p><p>Demo蒙特卡洛求PI：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--master spark://rshost001:7077 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--class org.apache.spark.examples.SparkPi \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">/home/vinx/app/spark-2.1.1-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.1.1.jar \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">100</span></pre></td></tr></table></figure><h4 id="使用Spark-Shell"><a href="#使用Spark-Shell" class="headerlink" title="使用Spark Shell"></a>使用Spark Shell</h4><p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</p><p>启动spark shell：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">./bin/spark-shell \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--master spark://rshost001:7077 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--executor-memory 2g \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--total-executor-cores 1</span></pre></td></tr></table></figure><p>参数说明：<br><code>--master spark://rshost001:7077</code> 指定Master的地址<br><code>--executor-memory 2g</code> 指定每个worker可用内存为2G<br><code>--total-executor-cores 1</code> 指定整个集群使用的cup核数为1个</p><p>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。</p><p>在Spark Shell中编写WordCount程序：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"/home/vinx/data/words.txt"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">.flatMap(_.split(<span class="string">" "</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">.map((_,<span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">.reduceByKey(_+_)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">.saveAsTextFile(<span class="string">"/home/vinx/data/wordcount"</span>)</span></pre></td></tr></table></figure><p>说明：<br>sc是SparkContext对象，该对象是提交spark程序的入口<br><code>textFile(&quot;/home/vinx/data/words.txt&quot;)</code>读取本地数据<br><code>flatMap(_.split(&quot; &quot;))</code> 先map再压平<br><code>map((_,1))</code> 将单词和1构成元组<br><code>reduceByKey(_+_)</code> 按照key进行reduce，并将value累加<br><code>saveAsTextFile(&quot;/home/vinx/data/wordcount&quot;)</code>将结果写入到本地目录</p><h3 id="Spark运行机制"><a href="#Spark运行机制" class="headerlink" title="Spark运行机制"></a>Spark运行机制</h3><h4 id="WordCount执行的流程"><a href="#WordCount执行的流程" class="headerlink" title="WordCount执行的流程"></a>WordCount执行的流程</h4><p><img src="https://vinxikk.github.io/img/spark/spark-wordcount.png" alt="WordCount执行过程"></p><h4 id="Spark提交任务的流程"><a href="#Spark提交任务的流程" class="headerlink" title="Spark提交任务的流程"></a>Spark提交任务的流程</h4><p><img src="https://vinxikk.github.io/img/spark/spark-job.png" alt="Spark任务提交过程"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD的常用算子&amp;persist/cache缓存机制&amp;checkpoint容错机制</title>
      <link href="/2018/05/19/spark/spark-core/"/>
      <url>/2018/05/19/spark/spark-core/</url>
      
        <content type="html"><![CDATA[<h3 id="RDD的基本概念"><a href="#RDD的基本概念" class="headerlink" title="RDD的基本概念"></a>RDD的基本概念</h3><h4 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h4><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</p><p>RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。</p><p>RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><h4 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h4><p><img src="https://vinxikk.github.io/img/spark/rdd-properties.png" alt="RDD的属性"></p><ul><li><p>一组分片（Partition）</p><p>即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p></li><li><p>一个计算每个分区的函数</p><p>Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p></li><li><p>RDD之间的依赖关系</p><p>RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p></li><li><p>一个Partitioner，即RDD的分片函数</p><p>当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于key-value的RDD，才会有Partitioner，非key-value的RDD的Partitioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p></li><li><p>一个列表</p><p>存储每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p></li></ul><h4 id="RDD的创建方式"><a href="#RDD的创建方式" class="headerlink" title="RDD的创建方式"></a>RDD的创建方式</h4><ul><li><p>通过外部的数据文件创建，如HDFS</p><p><code>val rdd1 = sc.textFile(“hdfs://192.168.xxx.xxx:9000/data/data.txt”)</code></p></li><li><p>通过<code>sc.parallelize</code>进行创建</p><p><code>val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))</code></p></li><li><p>RDD的类型</p><p>Transformation和Action</p></li></ul><h4 id="RDD的基本原理"><a href="#RDD的基本原理" class="headerlink" title="RDD的基本原理"></a>RDD的基本原理</h4><p><img src="https://vinxikk.github.io/img/spark/rdd-parallelize.png" alt="RDD的并行原理"></p><h3 id="RDD的算子"><a href="#RDD的算子" class="headerlink" title="RDD的算子"></a>RDD的算子</h3><h4 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h4><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。</p><p>相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。</p><p>只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。</p><p>这种设计让Spark更加有效率地运行。</p><ul><li><p><code>map(func)</code></p><p>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</p></li><li><p><code>filter(func)</code></p><p>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</p></li><li><p><code>flatMap(func)</code></p><p>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</p></li><li><p><code>mapPartitions(func)</code></p><p>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</p></li><li><p><code>mapPartitionsWithIndex(func)</code></p><p>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</p></li><li><p><code>sample((withReplacement, fraction, seed)</code></p><p>根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</p></li><li><p><code>union(otherDataset)</code></p><p>对源RDD和参数RDD求并集后返回一个新的RDD</p></li><li><p><code>intersection(otherDataset)</code></p><p>对源RDD和参数RDD求交集后返回一个新的RDD</p></li><li><p><code>distinct([numTasks])</code></p><p>对源RDD进行去重后返回一个新的RDD</p></li><li><p><code>groupByKey([numTasks])</code></p><p>在一个(K,V)的RDD上调用，返回一个(K,Iterator[V])的RDD</p></li><li><p><code>reduceByKey(func,[numTasks])</code></p><p>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与<code>groupByKey</code>类似，reduce任务的个数可以通过第二个可选的参数来设置</p></li><li><p><code>aggregateByKey(zeroValue)(seqOp,combOp,[numTasks])</code></p></li><li><p><code>sortByKey([ascending], [numTasks])</code></p><p>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</p></li><li><p><code>sortBy(func,[ascending], [numTasks])</code></p><p>与sortByKey类似，但是更灵活</p></li><li><p><code>join(otherDataset, [numTasks])</code></p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</p></li><li><p><code>cogroup(otherDataset, [numTasks])</code></p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个<code>(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))</code>类型的RDD</p></li><li><p><code>cartesian(otherDataset)</code></p><p>笛卡尔积</p></li><li><p><code>pipe(command, [envVars])</code></p></li><li><p><code>coalesce(numPartitions)</code></p></li><li><p><code>repartition(numPartitions)</code></p></li><li><p><code>repartitionAndSortWithinPartitions(partitioner)</code></p></li></ul><h4 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h4><ul><li><p><code>reduce(func)</code></p><p>通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的</p></li><li><p><code>collect()</code></p><p>在驱动程序中，以数组的形式返回数据集的所有元素</p></li><li><p><code>count()</code></p><p>返回RDD的元素个数</p></li><li><p><code>first()</code></p><p>返回RDD的第一个元素（类似于take(1)）</p></li><li><p><code>take(n)</code></p><p>返回一个由数据集的前n个元素组成的数组</p></li><li><p><code>takeSample(withReplacement,num, [seed])</code></p><p>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</p></li><li><p><code>takeOrdered(n, [ordering])</code></p></li><li><p><code>saveAsTextFile(path)</code></p><p>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark会调用toString方法，将它转换为文件中的文本</p></li><li><p><code>saveAsSequenceFile(path)</code></p><p>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以是HDFS或其他Hadoop支持的文件系统</p></li><li><p><code>saveAsObjectFile(path)</code></p></li><li><p><code>countByKey()</code></p><p>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数</p></li><li><p><code>foreach(func)</code></p><p>在数据集的每一个元素上，运行函数func进行更新</p></li></ul><h4 id="常用算子实例"><a href="#常用算子实例" class="headerlink" title="常用算子实例"></a>常用算子实例</h4><p>Transformation算子：</p><ul><li><p><code>map(func)</code>:  将输入的每个元素重写组合成一个元组</p><p>通过sc.parallelize创建RDD：</p><p><code>val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))</code></p><p>返回一个(K,*)格式的元组：</p><p><code>val rdd2 = rdd1.map((_,&quot;*&quot;))</code></p><p>每个元素乘以10：</p><p><code>val rdd2 = rdd1.map(_ * 10)</code></p><p>每个元素加上10：</p><p><code>val rdd2 = rdd1.map((x:Int) =&gt; x + 10)</code></p></li><li><p><code>filter(func)</code>:  返回一个新的RDD，该RDD是经过func运算后返回true的元素</p><p><code>val rdd3 = rdd1.filter(_ &gt; 5)</code></p></li><li><p><code>flatMap(func)</code>:  压平操作</p><p><code>val books = sc.parallelize(List(&quot;Hadoop&quot;,&quot;Hive&quot;,&quot;HDFS&quot;))</code></p><p><code>books.flatMap(_.toList).collect</code></p><p>结果：</p><p><code>res12: Array[Char] = Array(H, a, d, o, o, p, H, i, v, e, H, D, F, S)</code></p></li><li><p><code>union(otherDataset)</code>:  并集运算，类型要一致</p><p><code>val rdd4 = sc.parallelize(List(4,5,6,4,7))</code></p><p><code>val rdd5 = sc.parallelize(List(1,2,3,4))</code></p><p><code>val rdd6 = rdd4.union(rdd5)</code></p></li><li><p><code>intersection(otherDataset)</code>:  交集</p><p><code>val rdd7 = rdd5.intersection(rdd4)</code></p></li><li><p><code>distinct([numTasks])</code>: 去掉重复数据</p><p><code>val rdd8 = sc.parallelize(List(5,6,7,5,5,5))</code></p><p><code>rdd8.distinct.collect</code></p><p>结果：</p><p><code>res15: Array[Int] = Array(6, 7, 5)</code></p></li><li><p><code>groupByKey([numTasks])</code>:  对于一个&lt;k,v&gt;的RDD，按照key进行分组</p><p><code>val rdd = sc.parallelize(Array((&quot;I&quot;,1),(&quot;love&quot;,2),(&quot;I&quot;,3)))</code></p><p><code>rdd.groupByKey.collect</code></p><p>结果：</p><p><code>res16: Array[(String, Iterable[Int])] = Array((love,CompactBuffer(2)), (I,CompactBuffer(1, 3)))</code></p></li><li><p><code>flatMap</code> + <code>groupByKey</code></p><p><code>val strings = sc.parallelize(List(&quot;I love Beijing&quot;,&quot;I love China&quot;,&quot;Beijing is the capital of China&quot;))</code></p><p><code>strings.flatMap(_.split(&quot; &quot;)).map((_,1)).groupByKey.collect</code></p><p>结果：</p><p><code>res17: Array[(String, Iterable[Int])] = Array((is,CompactBuffer(1)), (love,CompactBuffer(1, 1)), (capital,CompactBuffer(1)), (Beijing,CompactBuffer(1, 1)), (China,CompactBuffer(1, 1)), (I,CompactBuffer(1, 1)), (of,CompactBuffer(1)), (the,CompactBuffer(1)))</code></p></li><li><p>reduceByKey(func, [numTasks]):  类似于groupByKey，区别是reduceByKey会有一个combiner的过程，对每个分区上的数据先做一次合并，所以效率更高</p></li><li><p><code>cartesian</code>:  笛卡尔积</p><p><code>val rdd1 = sc.parallelize(List(&quot;tom&quot;, &quot;jerry&quot;))</code></p><p><code>val rdd2 = sc.parallelize(List(&quot;tom&quot;, &quot;kitty&quot;, &quot;shuke&quot;))</code></p><p><code>val rdd3 = rdd1.cartesian(rdd2)</code></p></li></ul><p>Action算子：</p><p><code>val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)</code></p><ul><li><p><code>collect</code></p><p><code>rdd1.collect</code></p></li><li><p><code>reduce</code></p><p><code>val rdd2 = rdd1.reduce(_+_)</code></p></li><li><p><code>count</code></p><p><code>rdd1.count</code></p></li><li><p><code>top</code></p><p><code>rdd1.top(2)</code></p></li><li><p><code>take</code></p><p><code>rdd1.take(2)</code></p></li><li><p><code>first:  similar to take(1)</code></p><p><code>rdd1.first</code></p></li><li><p><code>takeOrdered</code></p><p><code>rdd1.takeOrdered(3)</code></p></li></ul><h3 id="RDD的缓存机制"><a href="#RDD的缓存机制" class="headerlink" title="RDD的缓存机制"></a>RDD的缓存机制</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><p><img src="https://vinxikk.github.io/img/spark/rdd-persist-cache.png" alt="RDD的persist和cache"></p><p>查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在<code>object StorageLevel</code>中定义。</p><p><img src="https://vinxikk.github.io/img/spark/storage-level.png" alt="存储级别"></p><p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。</p><p>通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><p>示例说明：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-cache-demo.png" alt="cache实例应用"></p><p>通过Web UI进行监控：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-cache-demo-ui.png" alt="cache Job的UI界面"></p><h3 id="RDD的Checkpoint容错机制"><a href="#RDD的Checkpoint容错机制" class="headerlink" title="RDD的Checkpoint容错机制"></a>RDD的Checkpoint容错机制</h3><p>检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage（血统）做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。</p><p>设置checkpoint的目录，可以是本地的文件夹、也可以是HDFS。一般是在具有容错能力，高可靠的文件系统上(比如HDFS, S3等)设置一个检查点路径，用于保存检查点数据。</p><p>本地目录（这种模式，需要将spark-shell运行在本地模式上）：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-checkpoint-local.png" alt="基于本地目录的checkpoint"></p><p>HDFS的目录（这种模式，需要将spark-shell运行在集群模式上）：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-checkpoint-hdfs.png" alt="基于HDFS的checkpoint"></p><p>checkpoint源码：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-checkpoint-source.png" alt="checkpoint源码"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive SQL的底层执行原理</title>
      <link href="/2018/05/17/hive/hive-ql-principle/"/>
      <url>/2018/05/17/hive/hive-ql-principle/</url>
      
        <content type="html"><![CDATA[<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><p><code>select * from tb_name where col_1 = &#39;2018&#39; and col_2 = &#39;hive&#39;;</code></p><p>执行select语句时，只有map阶段，没有shuffle和reduce。</p><p>map：根据split个数开启几个map，每个map task会接收到一个split文件，每个map函数会逐行对输入的文件进行检测，筛选出col_1为2018、col_2为hive的数据，保存到本地。</p><p>map的个数可以这样理解：如果文件为256M（hadoop2.x每个块128M），会开启2个map；289M就会开启3个map。</p><p>这些map都是并行执行的，当数据量特别大时开启的map会越多，理论上还会并行执行。</p><p>select数据时一定要加上where条件，避免盲目查询。</p><h3 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h3><p><code>select col_1,count(*) from tb_name where col_2 = &quot;hive&quot; group by col_1;</code></p><p>map：分完片的文件会产生相应数量的map，每个map会逐行检测col_2是否为hive，如果是hive它会生成键值对<code>&lt;col_1,1&gt;</code>。</p><p>combine：该操作发生在对应的文件中，即map分了几个，就有几个combine，它会把map端产生的键值对相同的key对进行累加，如：<code>&lt;col_1,3&gt;</code>。</p><p>shuffle：该操作分为partition, sort, spill, copy, merge。最重要的是分区和合并的过程。</p><p>map生成的task会通过对每个键取hash值，使map task按照相同的键均匀分配到reduce上，这个过程就是分区，分配到同一个reduce上的task会经过合并过程，生成键值对<code>&lt;col_1,{3,1}&gt;</code>，做完reduce task输入。</p><p>reduce：调用函数累加，3+1=4</p><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p><code>select t1.col_1,t2.col_2 from (select col_1,col_3 from tb_1) t1 join (select col_2,col_3 from tb_2) t2 on t1.col_3 = t2.col_3;</code></p><p>会开启3个MR任务。</p><p>第一个执行<code>select col_1,col_3 from tb_1</code>的操作，第二个执行<code>select col_2,col_3 from tb_2</code>的操作，第三个会将第一个和第二个的结果进行关联合并，然后输出。</p><p>split：join的时候会把第一个MR和第二个MR任务的输出文件输入该次任务，首先会对前两个任务进行分片，hadoop大于128M分一个片。</p><p>map：hadoop集群根据split出来的结果开启相应个数的map task。</p><p>shuffle：join时主要是分区操作，join的列进行数据的重分布和分发过程，分区的类为col_3，于是所有的map task都根据col_3进行发布。相同join就会发生到同一个reducer上。</p><p>reduce：shuffle发送的col_1, col_3; col_2, col_3会根据col_3发送到reduce task上。这个时候根据col_3将他们的值变成一行，保留到本地输出文件中。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的压缩格式</title>
      <link href="/2018/05/16/hive/hive-compress/"/>
      <url>/2018/05/16/hive/hive-compress/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive的压缩格式"><a href="#Hive的压缩格式" class="headerlink" title="Hive的压缩格式"></a>Hive的压缩格式</h3><p>test</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-Compress </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的数据倾斜及性能调优</title>
      <link href="/2018/05/15/hive/hive-skewindata/"/>
      <url>/2018/05/15/hive/hive-skewindata/</url>
      
        <content type="html"><![CDATA[<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><p>数据倾斜是指，MR程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长。这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。</p><p>表现：</p><p>任务进度长时间维持在99%（或者100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。</p><h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><table><thead><tr><th>类型</th><th>keyword</th><th>情形</th><th>后果</th><th>解决方法</th></tr></thead><tbody><tr><td>数据倾斜</td><td>join</td><td>其中一个表是小表（1000条以下记录或1G容量以下）</td><td>分发到某几个reduce上的数据远高于平均值</td><td>SMB Join,  Map Join, SMB Map Join</td></tr><tr><td>数据倾斜</td><td></td><td>大表与大表join，但是关联值0或null过多</td><td>空值会由一个reduce处理，非常慢</td><td>SMB Join, 空值特殊处理, 调参(skewjoin)</td></tr><tr><td>数据倾斜</td><td></td><td>小表不小不大</td><td>-</td><td>嵌套Map Join</td></tr><tr><td>数据倾斜</td><td></td><td>不同数据类型关联</td><td>默认的Hash操作会按int型的id来进行分配，所有string类型id的记录都分配到一个reducer中</td><td>类型转换</td></tr><tr><td>数据倾斜</td><td>group by</td><td>某些维度值数据过多</td><td>处理某值的reduce耗时</td><td>调参(skewindata)</td></tr><tr><td>数据倾斜</td><td>count distinct</td><td>某些值过多</td><td>处理某值的reduce耗时</td><td>count distinct优化</td></tr><tr><td>数据量大</td><td>on … where …</td><td>过滤在where条件</td><td>大数据量join</td><td>where优化</td></tr><tr><td>Job数多</td><td>union all</td><td>union属于嵌套查询</td><td>生成过多的job</td><td>union all优化</td></tr></tbody></table><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h4><p>对于group by引起的倾斜，设置下面的参数：</p><p><code>set hive.map.aggr = true</code></p><p><code>set hive.groupby.skewindata=true</code></p><p>有数据倾斜的时候进行负载均衡，当选项设定为true，生成的查询计划会有两个MR Job。</p><p>第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的。</p><p>第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><p>skew join优化：</p><p><code>set hive.optimize.skewjoin = true;</code></p><p>skew join，其原理是把这种user_id=0的特殊值先不在Reduce端计算掉，而是先写入hdfs，然后启动一轮Map join专门做这个特殊值的计算，期望能提高计算这部分值的处理速度。还有要告诉Hive如何判断特殊值，根据<code>hive.skewjoin.key</code>设置的数量Hive可以知道，比如默认值是100000，那么超过100000条记录的值就是特殊值。</p><h4 id="Sort-Merge-Bucket-Join-SMB-Join"><a href="#Sort-Merge-Bucket-Join-SMB-Join" class="headerlink" title="Sort Merge Bucket Join (SMB Join)"></a>Sort Merge Bucket Join (SMB Join)</h4><p>Hive桶：</p><p>对于每一个表（table）或者分区，Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p><p>SMB Join:</p><ol><li>解决大表与小表间的join问题。小表的number_buckets必须是大表的倍数。</li><li>解决大表与大表间的join问题。这一优化方法并不一定要求两个表必须桶的个数相同，两个表的桶个数是倍数关系也可以。</li></ol><p><img src="https://vinxikk.github.io/img/hive/hive-smb-join.png" alt="SMB Join"></p><p>例子：</p><p><img src="https://vinxikk.github.io/img/hive/hive-smb-join-example.png" alt="SMB Join例子"></p><h4 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h4><p>对于大小表关联查询产生的数据倾斜，一般的做法是使用Map Join将其中做连接的小表（全量数据）分发到所有map task端进行join，从而避免reduce task，前提要求是内存足以装下该全量数据。</p><p>以大表a和小表b为例，所有的map task节点都装载小表b的所有数据，然后大表a的一个数据块数据比如a1去跟b全量数据做连接，就省去了reduce做汇总的过程。</p><p>所以相对来说，在内存允许的条件下使用map join比直接使用MapReduce效率还高些，当然这只限于做join查询的时候。</p><p>所有的工作都在Map端进行计算。首先小表的Map阶段它会将自己转化成MapReduce Local Task，然后从HDFS取小表的所有数据，将自己转化成Hashtable file并压缩打包放入DistributedCache里面。</p><p><code>hive.auto.convert.join=true ;</code>  设置Map Join优化自动开启 </p><p><code>hive.smalltable.filesize=25000000L;</code>  参数控制（默认25M），当小表超过这个大小，hive会默认转化成common join。另一种是手动判断<code>/*+mapjoin(map_table)*/</code>。</p><p>认为开启Map Join：</p><p><code>select /* +mapjoin(b) */ a.id aid,name,age from a join b on a.id = b.id;</code></p><p>因为加了<code>/* +mapjoin(b) */</code>这一段代码，执行的时候就会将b表读入内存中，但是要求b表必须是小表，数据量不能太大。</p><h4 id="SMB-Map-Join"><a href="#SMB-Map-Join" class="headerlink" title="SMB Map Join"></a>SMB Map Join</h4><p>两个表关联键为imei，需要按imei分桶并且排序，小表（lxw_test）分桶数是大表（lxw_test1）的倍数。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> lxw_test(imei <span class="keyword">string</span>,sndaid <span class="keyword">string</span>,data_time <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">CLUSTERED <span class="keyword">BY</span>(imei) SORTED <span class="keyword">BY</span>(imei) <span class="keyword">INTO</span> <span class="number">10</span> BUCKETS;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> lxw_test1(imei <span class="keyword">string</span>,sndaid <span class="keyword">string</span>,data_time <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">CLUSTERED <span class="keyword">BY</span>(imei) SORTED <span class="keyword">BY</span>(imei) <span class="keyword">INTO</span> <span class="number">5</span> BUCKETS;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--插入数据前需要打开该选项</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--join时需要打开的参数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(b) */</span> <span class="keyword">count</span>(<span class="number">1</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxw_test1 a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> lxw_test b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> a.imei = b.imei</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--包括insert数据，差不多10分钟左右；</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--如果这两个表做普通的join, 耗时1个多小时，没跑完，kill掉了。</span></span></pre></td></tr></table></figure><h4 id="嵌套Map-Join"><a href="#嵌套Map-Join" class="headerlink" title="嵌套Map Join"></a>嵌套Map Join</h4><p>小表不大不小，怎么用map join解决倾斜问题。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+mapjoin(x)*/</span>* <span class="keyword">from</span> <span class="keyword">log</span> a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span>  <span class="comment">/*+mapjoin(c)*/</span>d.*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">from</span> ( <span class="keyword">select</span> <span class="keyword">distinct</span> user_id <span class="keyword">from</span> <span class="keyword">log</span> ) c</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">join</span> <span class="keyword">users</span> d</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">on</span> c.user_id = d.user_id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    ) x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">on</span> a.user_id = b.user_id;</span></pre></td></tr></table></figure><h4 id="where优化"><a href="#where优化" class="headerlink" title="where优化"></a>where优化</h4><p>WHERE条件放置在ON条件中，以达到两表做join的时候，数据量相对变小的效果。</p><h4 id="union-all优化"><a href="#union-all优化" class="headerlink" title="union all优化"></a>union all优化</h4><p>union all优化还不完善，嵌套查询时会生成过多的job。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--Before</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> *</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> t1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">union</span> <span class="keyword">all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">select</span> * <span class="keyword">from</span> t4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">union</span> <span class="keyword">all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">select</span> * <span class="keyword">from</span> t2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">join</span> t3</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">on</span> t2.id = t3.id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">     ) x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> c1,c2;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--After</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--先join生成临时表，后union all。原来4个jobs，改进后变成2个jobs。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t5</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">join</span> t3</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">on</span> t2.id = t3.id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (t1 <span class="keyword">union</span> <span class="keyword">all</span> t4 <span class="keyword">union</span> <span class="keyword">all</span> t5);</span></pre></td></tr></table></figure><h4 id="count-distinct优化"><a href="#count-distinct优化" class="headerlink" title="count distinct优化"></a>count distinct优化</h4><p>在Hive中应小心使用count distinct，因为很可能出现性能问题。</p><p>因为要去重，hive会把map阶段的输出全部分配到一个reduce task上，此时很容易发生性能问题，我们可以先group by，再count，减少distinct的使用。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/*改写前*/</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">count</span>(<span class="keyword">distinct</span> course) <span class="keyword">as</span> course_count</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/*改写后*/</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id,<span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">as</span> course_count</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span> <span class="keyword">distinct</span> <span class="keyword">id</span>,course </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">from</span> student</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">) a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 或者</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id,<span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">as</span> course_count </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> student </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>,course</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">) a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.id;</span></pre></td></tr></table></figure><h4 id="空值特殊处理"><a href="#空值特殊处理" class="headerlink" title="空值特殊处理"></a>空值特殊处理</h4><p>在日志中，常会有字段值丢失的问题，比如日志中的user_id，如果取其中的user_id和用户表中的user_id相关联，就会碰到数据倾斜的问题。</p><p>方法1，user_id为空的不参与关联：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> <span class="keyword">user</span> b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> a.user_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> a.user_id = b.user_id </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> <span class="keyword">all</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> c </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> c.user_id <span class="keyword">is</span> <span class="literal">null</span>;</span></pre></td></tr></table></figure><p>方法2，把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">user</span> b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> a.user_id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">'null_'</span>,<span class="keyword">rand</span>()) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> a.user_id <span class="keyword">end</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">= b.user_id;</span></pre></td></tr></table></figure><p>方法2比方法1效率更好，不但IO少了，而且作业数也少了。</p><p>方法1中，log表读了两次，job数肯定是2，而方法2的job数是1.</p><p>方法2使本身为null的所有记录不会拥挤在同一个reduce task中，加上随机字符串值，会分散到过个reduce task中，由于null值关联不上，处理后并不影响最终结果。</p><h4 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h4><p>场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的join操作时，默认的hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reduce中。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">users</span> a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">logs</span> b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">on</span> a.usr_id = <span class="keyword">cast</span>(b.user_id <span class="keyword">as</span> <span class="keyword">string</span>)</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据倾斜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的数据分桶及使用场景</title>
      <link href="/2018/05/14/hive/hive-bucket/"/>
      <url>/2018/05/14/hive/hive-bucket/</url>
      
        <content type="html"><![CDATA[<h3 id="数据分桶引入"><a href="#数据分桶引入" class="headerlink" title="数据分桶引入"></a>数据分桶引入</h3><p>分区提供了一个隔离数据和优化查询的便利方式，不过并非所有的数据都可以形成合理的分区，尤其是需要确定合适大小的分区划分时（不合理的分区划分方式可能导致有的分区数据过多，而某些分区没有多少数据的情况）。</p><p>分桶是将数据集分解为若干部分的一种技术。</p><h3 id="分桶的原理"><a href="#分桶的原理" class="headerlink" title="分桶的原理"></a>分桶的原理</h3><p>跟MR中的HashPartitioner的原理一样。</p><p>MR中，按照key的hash值去模除以reduceTask的个数。</p><p>Hive中，按照分桶字段的hash值去模除以分桶的个数。</p><p>Hive也是针对某一列进行桶的组织，Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p><h3 id="分桶的作用"><a href="#分桶的作用" class="headerlink" title="分桶的作用"></a>分桶的作用</h3><ul><li><p>方便抽样</p><p>使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，能够在数据集的一小部分数据上试运行查询。</p></li><li><p>提高join查询效率</p><p>获得更高的查询处理效率。桶为表加上了额外的结构，Hive在处理有些查询时能利用这个结构。具体而言，连接两个在相同列（包含连接列的）上划分了桶的表，可以使用Map端连接（Map-side join）高效地实现。比如join操作，对于join操作两个表有一个相同的列，如果对这两个表都进行了桶操作，那么将保存相同列值的桶进行join操作就可以，可以大大减小join的数据量。</p></li></ul><h3 id="分桶表的创建"><a href="#分桶表的创建" class="headerlink" title="分桶表的创建"></a>分桶表的创建</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> clickcube;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> <span class="string">`clickcube_mid`</span>(             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`logtype`</span> <span class="built_in">bigint</span>,                                </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`date`</span> <span class="keyword">string</span>,                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`hour`</span> <span class="built_in">bigint</span>,                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`projectid`</span> <span class="built_in">bigint</span>,                              </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`campaignid`</span> <span class="built_in">bigint</span>,                             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`templateid`</span> <span class="built_in">bigint</span>,                             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`mediaid`</span> <span class="built_in">bigint</span>,                                </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`slotid`</span> <span class="built_in">bigint</span>,                                 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`channeltype`</span> <span class="built_in">bigint</span>,                            </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`regioncode`</span> <span class="keyword">string</span>,                             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`campclick`</span> <span class="built_in">bigint</span>,                              </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`campimp`</span> <span class="built_in">bigint</span>,                                </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`mediaclick`</span> <span class="built_in">bigint</span>,                             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`mediaimp`</span> <span class="built_in">bigint</span>,                               </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`templateimp`</span> <span class="built_in">bigint</span>,                            </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`templatecampimp`</span> <span class="built_in">bigint</span>,                        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`mediaclickcost`</span> <span class="keyword">double</span>,                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`campclickcost`</span> <span class="keyword">double</span>)                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  PARTITIONED <span class="keyword">BY</span> (                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`day`</span> <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">  CLUSTERED <span class="keyword">BY</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="string">`campaignid`</span>,  <span class="string">`mediaid`</span> ) <span class="keyword">INTO</span> <span class="number">100</span> BUCKETS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">STORED</span> <span class="keyword">AS</span> INPUTFORMAT</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">  OUTPUTFORMAT</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">  TBLPROPERTIES (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'last_modified_by'</span>=<span class="string">'cloudera-scm'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'last_modified_time'</span>=<span class="string">'1530676367'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'transient_lastDdlTime'</span>=<span class="string">'1530676367'</span>)</span></pre></td></tr></table></figure><h3 id="将数据插入分桶表"><a href="#将数据插入分桶表" class="headerlink" title="将数据插入分桶表"></a>将数据插入分桶表</h3><p>步骤：</p><ol><li>从HDFS或本地磁盘中load数据，导入中间表</li><li>通过从中间表查询的方式完成数据导入</li></ol><p>分桶的本质就是对分桶的字段做了hash，然后存放到对应的文件中，所以说如果原有数据没有按key hash，需要在插入分桶的时候hash，也就是说向分桶表中插入数据的时候必然要执行一次MapReduce，分桶表的数据基本只能通过从结果集查询插入的方式进行导入。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> clickcube;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> clickcube_mid_bucket </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">PARTITION</span>( <span class="keyword">day</span> = <span class="string">'2018-07-03'</span> )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.logtype,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.<span class="string">`date`</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.<span class="string">`hour`</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.projectid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campaignid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templateid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.slotid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.channeltype,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.regioncode,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campclick,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaclick,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templateimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templatecampimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaclickcost,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campclickcost </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> clickcube_mid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> <span class="keyword">day</span> = <span class="string">'2018-07-03'</span></span></pre></td></tr></table></figure><p>我们需要确保reduce的数量与表中的bucket数量一致，为此有两种做法：</p><ol><li><p>让hive强制分桶，自动按照分桶表的bucket进行分桶（推荐）</p><p><code>set hive.enforce.bucketing=true;</code></p></li><li><p>手动指定reduce数量</p><p><code>set mapreduce.job.reduces=num;</code></p><p><code>set mapreduce.reduce.tasks=num;</code></p><p>并在SELECT后增加CLUSTER BY语句</p></li></ol><p>整体的数据导入脚本：</p><p>insert_into_bucket.hql   数据导入HQL</p><p>insert_into_bucket.init   设置初始环境</p><p>insert_into_bucket.sh     主体执行脚本</p><p>insert_into_bucket.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> -o errexit</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">ROOT_PATH=$(dirname $(readlink -f <span class="variable">$0</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$ROOT_PATH</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">date_pattern_old=<span class="string">'^[0-9]&#123;4&#125;-[0-9]&#123;1,2&#125;-[0-9]&#123;1,2&#125;$'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">date_pattern=<span class="string">'^[0-9]&#123;4&#125;-((0([1-9]&#123;1&#125;))|(1[1|2]))-(([0-2]([0-9]&#123;1&#125;))|(3[0|1]))$'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#参数数量</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">argsnum=<span class="variable">$#</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#一些默认值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">curDate=`date +%Y%m%d`</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">partitionDate=`date -d <span class="string">'-1 day'</span> +%Y-%m-%d`</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">fileLocDate=`date -d <span class="string">'-1 day'</span> +%Y-%m-%d`</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#日志存放位置</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">logdir=insert_bucket_logs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">tips</span></span>() &#123; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Usage : insert_into_bucket.sh [date]"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Args :"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"date"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"date use this format yyyy-MM-dd , ex : 2018-06-02"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">echo</span> <span class="string">"============================================================"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Example :"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"example1 : sh insert_into_bucket.sh"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"example2 : sh insert_into_bucket.sh 2018-06-02"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="variable">$argsnum</span> -eq 0 ] ; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"No argument, use default value"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> [ <span class="variable">$argsnum</span> -eq 1 ] ; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"One argument, check date pattern"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">arg1=<span class="variable">$1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ! [[ <span class="string">"<span class="variable">$arg1</span>"</span> =~ <span class="variable">$date_pattern</span> ]] ; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">       <span class="built_in">echo</span> -e <span class="string">"\033[31m Please specify valid date in format like 2018-06-02"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">       <span class="built_in">echo</span> -e <span class="string">"\033[0m"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">       tips</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">        <span class="built_in">exit</span> 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fi</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">dateArr=($(<span class="built_in">echo</span> <span class="variable">$arg1</span> |tr <span class="string">"-"</span> <span class="string">" "</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"dateArr length is "</span><span class="variable">$&#123;#dateArr[@]&#125;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">partitionDate=<span class="variable">$&#123;dateArr[0]&#125;</span>-<span class="variable">$&#123;dateArr[1]&#125;</span>-<span class="variable">$&#123;dateArr[2]&#125;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">"\033[31m Not valid num of arguments"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">"\033[0m"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">tips</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span> 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fi</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ ! -d <span class="string">"<span class="variable">$logdir</span>"</span> ]; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">    mkdir -p <span class="variable">$logdir</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fi</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$ROOT_PATH</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#nohup hive -hivevar p_date=$&#123;partitionDate&#125; -hivevar f_date=$&#123;fileLocDate&#125; -f  hdfs_add_partition_dmp_clearlog.hql  &gt;&gt; $logdir/load_$&#123;curDate&#125;.log</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">nohup beeline -u jdbc:hive2://master:10000 -n root --color=<span class="literal">true</span> --silent=<span class="literal">false</span>  --hivevar p_date=<span class="variable">$&#123;partitionDate&#125;</span> -i insert_into_bucket.init -f insert_into_bucket.hql  &gt;&gt; <span class="variable">$logdir</span>/insert_bucket_<span class="variable">$&#123;curDate&#125;</span>.<span class="built_in">log</span></span></pre></td></tr></table></figure><p>insert_into_bucket.init：</p><p><code>set hive.enforce.bucketing = true;</code></p><p>insert_into_bucket.hql：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> clickcube;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> clickcube_mid_bucket </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">PARTITION</span>( <span class="keyword">day</span> = <span class="string">'$&#123;hivevar:p_date&#125;'</span> )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.logtype,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.<span class="string">`date`</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.<span class="string">`hour`</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.projectid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campaignid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templateid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.slotid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.channeltype,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.regioncode,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campclick,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaclick,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templateimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templatecampimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaclickcost,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campclickcost  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> clickcube_mid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> <span class="keyword">day</span> = <span class="string">'$&#123;hivevar:p_date&#125;'</span></span></pre></td></tr></table></figure><h3 id="针对于分桶表的数据抽样"><a href="#针对于分桶表的数据抽样" class="headerlink" title="针对于分桶表的数据抽样"></a>针对于分桶表的数据抽样</h3><p>分桶的一个主要优势就是数据抽样，主要有两种方式：</p><ol><li>基于桶抽样</li><li>基于百分比抽样</li></ol><h4 id="基于桶抽样"><a href="#基于桶抽样" class="headerlink" title="基于桶抽样"></a>基于桶抽样</h4><p><code>select * from bucketed_users tablesample(bucket 1 out of 4 on id);</code></p><p>桶的个数从1开始计数，因此，前面的查询从4个桶的第一个中获取所有的用户。对于一个大规模的、均匀分布的数据集，这会返回表中约四分之一的数据行。我们也可以用其他比例对若干个桶进行取样（因为取样并不是一个精确的操作，因此这个比例不一定要是桶数的整数倍）。</p><p><strong>说法一：</strong></p><p><code>注：</code>tablesample是抽样语句，语法：<code>tablesample(bucket x out of y)</code></p><p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了64份，y=32时，抽取（64/32=）2个bucket的数据，当y=128时，抽取（64/128=）1/2个bucket的数据。</p><p>x表示从哪个bucket开始抽取。例如，table总bucket数为32，<code>tablesample(bucket 3 out of 16)</code>，表示总共抽（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</p><p><strong>说法二：</strong></p><p>分桶语句中的分母表示的是数据将会被散列的桶的个数，分子表示将会选择的桶的个数。</p><p><strong>实例：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> clickcube_mid_bucket </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">10</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">100</span> <span class="keyword">on</span> <span class="keyword">rand</span>()) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> <span class="keyword">day</span> = <span class="string">'2018-07-03'</span>;</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/hive/hive-bucket-sample.png" alt="基于桶的抽样"></p><h4 id="基于百分比抽样"><a href="#基于百分比抽样" class="headerlink" title="基于百分比抽样"></a>基于百分比抽样</h4><p>hive另外一种按照抽样百分比进行抽样的方式，该种方式基于行数，按照输入路径下的数据块的百分比进行抽样。这种抽样的最小单元是一个HDFS数据块，如果表的数据大小小于普通块大小128M，将返回所有行。</p><p>基于百分比的抽样方式提供了一个变量，用于控制基于数据块的调优种子信息：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.sample.seednumber<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>A number userd for percentage sampling. By changing this number, user will change the subsets of data sampled.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><h3 id="数据分桶的缺点"><a href="#数据分桶的缺点" class="headerlink" title="数据分桶的缺点"></a>数据分桶的缺点</h3><p>如果通过数据文件load到分桶表中，会存在额外的MR负担。</p><p>实际生产中分桶策略使用频率较低，更常见的还是使用数据分区。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive分桶 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么kafka这么快，又能保证消息不丢失？</title>
      <link href="/2018/05/13/kafka/kafka-3/"/>
      <url>/2018/05/13/kafka/kafka-3/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么Kafka这么快？"><a href="#为什么Kafka这么快？" class="headerlink" title="为什么Kafka这么快？"></a>为什么Kafka这么快？</h3><h4 id="顺序写磁盘"><a href="#顺序写磁盘" class="headerlink" title="顺序写磁盘"></a>顺序写磁盘</h4><p>顺序写磁盘的性能是随机写入的性能的6000倍的提升，媲美内存随机访问的性能，磁盘不再是瓶颈点。</p><h4 id="Page-Cache"><a href="#Page-Cache" class="headerlink" title="Page Cache"></a>Page Cache</h4><p>为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。</p><h4 id="零拷贝技术"><a href="#零拷贝技术" class="headerlink" title="零拷贝技术"></a>零拷贝技术</h4><p>零拷贝技术，可以有效的减少上下文切换和拷贝次数。</p><h3 id="如何做到消息不丢失"><a href="#如何做到消息不丢失" class="headerlink" title="如何做到消息不丢失"></a>如何做到消息不丢失</h3><h4 id="ACK机制"><a href="#ACK机制" class="headerlink" title="ACK机制"></a>ACK机制</h4><p>通过 ACK 机制保证消息送达。Kafka 采用的是至少一次（At least once），消息不会丢，但是可能会重复传输。</p><h4 id="发送消息"><a href="#发送消息" class="headerlink" title="发送消息"></a>发送消息</h4><p>为了得到更好的性能，Kafka 支持在生产者一侧进行本地buffer，也就是累积到一定的条数才发送，如果这里设置不当是会丢消息的。</p><p>生产者端设置 producer.type=async, sync，默认是 sync。</p><p>当设置为 async，会大幅提升性能，因为生产者会在本地缓冲消息，并适时批量发送。</p><p>如果对可靠性要求高，那么这里可以设置为 sync 同步发送。</p><h4 id="消费消息"><a href="#消费消息" class="headerlink" title="消费消息"></a>消费消息</h4><p>如果更注重可靠性，则需要显示提交 Offset，也就是当所有业务都处理完成的时候，再提交 Offset。这样会导致重复消费，需要提供幂等性接口。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka工作流程</title>
      <link href="/2018/05/12/kafka/kafka-2/"/>
      <url>/2018/05/12/kafka/kafka-2/</url>
      
        <content type="html"><![CDATA[<h3 id="Kafka生产过程"><a href="#Kafka生产过程" class="headerlink" title="Kafka生产过程"></a>Kafka生产过程</h3><h4 id="写入方式"><a href="#写入方式" class="headerlink" title="写入方式"></a>写入方式</h4><p>producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。</p><h4 id="分区（Partition）"><a href="#分区（Partition）" class="headerlink" title="分区（Partition）"></a>分区（Partition）</h4><p>Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。</p><p>Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。</p><p>消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：</p><p><img src="https://vinxikk.github.io/img/kafka/anatomy-of-a-topic.png" alt="Topic的结构"></p><p>上图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响。</p><p><img src="https://vinxikk.github.io/img/kafka/partition-producer-consumer.png" alt="Partition的消息是有序的"></p><p>我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。</p><p>发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。</p><ol><li><p>分区的原因</p><ol><li><p>方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p></li><li><p>可以提高并发，因为可以以Partition为单位读写了。</p><p>传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。</p><p>Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。</p></li></ol></li><li><p>分区的原则</p><ol><li><p>指定了partition，则直接使用；</p></li><li><p>未指定partition但指定key，通过对key的value进行hash出一个partition；</p></li><li><p>partition和key都未指定，使用轮询选出一个partition。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DefaultPartitioner类</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">            &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">            <span class="comment">// hash the keyBytes to choose a partition</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr></table></figure></li></ol></li></ol><h4 id="副本（Replication）"><a href="#副本（Replication）" class="headerlink" title="副本（Replication）"></a>副本（Replication）</h4><p>同一个partition可能会有多个replication（对应 server.properties 配置中的 <code>default.replication.factor=N</code>）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。</p><h4 id="写入流程"><a href="#写入流程" class="headerlink" title="写入流程"></a>写入流程</h4><p>producer写入消息流程如下：</p><ol><li>producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader</li><li>producer将消息发送给该leader</li><li>leader将消息写入本地log</li><li>followers从leader pull消息，写入本地log后向leader发送ACK</li><li>leader收到所有ISR（In Sync Replicas）中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK</li></ol><h3 id="Broker保存信息"><a href="#Broker保存信息" class="headerlink" title="Broker保存信息"></a>Broker保存信息</h3><h4 id="存储方式"><a href="#存储方式" class="headerlink" title="存储方式"></a>存储方式</h4><p>物理上把topic分成一个或多个patition（对应 server.properties 中的<code>num.partitions=3</code>配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 logs]$ ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">drwxrwxr-x. 2 vinx vinx  4096 8月   6 14:37 first-0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">drwxrwxr-x. 2 vinx vinx  4096 8月   6 14:35 first-1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">drwxrwxr-x. 2 vinx vinx  4096 8月   6 14:37 first-2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 logs]$ <span class="built_in">cd</span> first-0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 first-0]$ ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 vinx vinx 10485760 8月   6 14:33 00000000000000000000.index</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 vinx vinx      219 8月   6 15:07 00000000000000000000.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 vinx vinx 10485756 8月   6 14:33 00000000000000000000.timeindex</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 vinx vinx        8 8月   6 14:37 leader-epoch-checkpoint</span></pre></td></tr></table></figure><h4 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h4><p>无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：</p><ol><li>基于时间：<code>log.retention.hours=168</code></li><li>基于大小：<code>log.retention.bytes=1073741824</code></li></ol><p>需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。</p><h3 id="Kafka消费过程"><a href="#Kafka消费过程" class="headerlink" title="Kafka消费过程"></a>Kafka消费过程</h3><p>kafka提供了两套consumer API：高级Consumer API和低级API。</p><h4 id="消费模型"><a href="#消费模型" class="headerlink" title="消费模型"></a>消费模型</h4><p>消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）。</p><p>基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的。</p><p>Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。</p><p><img src="https://vinxikk.github.io/img/kafka/consumer-pull.png" alt="consumer的pull拉取模式"></p><p>在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉。</p><h4 id="高级API"><a href="#高级API" class="headerlink" title="高级API"></a>高级API</h4><p>高级API优点：</p><ol><li>写起来简单</li><li>不需要自行去管理offset，系统通过zookeeper自行管理</li><li>不需要管理分区、副本等情况，系统自动管理</li><li>消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset）</li><li>可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）</li></ol><p>高级API缺点：</p><ol><li>不能自行控制offset（对于某些特殊需求来说）</li><li>不能细化控制如分区、副本、zk等</li></ol><h4 id="低级API"><a href="#低级API" class="headerlink" title="低级API"></a>低级API</h4><p>低级API优点：</p><ol><li>能够让开发者自己控制offset，想从哪里读取就从哪里读取</li><li>自行控制连接分区，对分区自定义进行负载均衡</li><li>对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）</li></ol><p>低级API缺点：</p><ol><li>太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等</li></ol><h4 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h4><p>消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。</p><p>在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。</p><h4 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h4><p>consumer采用pull（拉）模式从broker中读取数据。</p><p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p><p>对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p><p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka Exactly Once语义</title>
      <link href="/2018/05/11/kafka/kafka-exactly-once/"/>
      <url>/2018/05/11/kafka/kafka-exactly-once/</url>
      
        <content type="html"><![CDATA[<h3 id="kafka是什么"><a href="#kafka是什么" class="headerlink" title="kafka是什么"></a>kafka是什么</h3><p>问题引入：Kafka会出现多次消费的情况，Kafka怎么才能确保一次性消费？</p><ol><li>checkpoint（学习可以，生产不建议）</li><li>kafka自身（至少一次消费语义）</li><li>外部存储（精准一次消费语义）</li></ol><p>无论消费多少，比如就算有重复，也要考虑幂等性设计。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka基本概念</title>
      <link href="/2018/05/10/kafka/kafka-1/"/>
      <url>/2018/05/10/kafka/kafka-1/</url>
      
        <content type="html"><![CDATA[<h3 id="kafka是什么"><a href="#kafka是什么" class="headerlink" title="kafka是什么"></a>kafka是什么</h3><p>在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。</p><ol><li>Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。</li><li>Kafka最初是由LinkedIn公司开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。</li><li>Kafka是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)称为broker。</li><li>无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。</li></ol><h4 id="为什么需要消息队列"><a href="#为什么需要消息队列" class="headerlink" title="为什么需要消息队列"></a>为什么需要消息队列</h4><ol><li><p>解耦：</p><p>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p></li><li><p>冗余：</p><p>消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。</p></li><li><p>扩展性：</p><p>因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。</p></li><li><p>灵活性&amp;峰值处理能力：</p><p>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p></li><li><p>可恢复性：</p><p>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p></li><li><p>顺序保证：</p><p>在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）</p></li><li><p>缓冲：</p><p>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p></li><li><p>异步通信：</p><p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p></li></ol><h4 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h4><ol><li>Producer: 消息生产者，就是向kafka broker发消息的客户端。</li><li>Consumer: 消息消费者，向kafka broker取消息的客户端。</li><li>Topic: 可以理解为一个队列。</li><li>Consumer Group: kafka提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者或消费者实例（consumer instance），它们共享一个公共的ID，即group ID。组内的所有消费者协调在一起来消费订阅主题（subscribed topics）的所有分区（partition）。当然，每个分区只能由同一个消费者组内的一个consumer来消费。</li><li>Broker: 一台kafka服务器就是一个broker。一个集群由多个broker组成，一个broker可以容纳多个topic。</li><li>Partition: 为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。</li><li>Offset: kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka</li></ol><h4 id="分布式模型"><a href="#分布式模型" class="headerlink" title="分布式模型"></a>分布式模型</h4><p>Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本（Leader），其他节点作为备份副本（Follower，也叫作从副本）。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本出现故障时，备份副本中的一个副本会被选择为新的主副本。因为每个分区的副本中只有主副本接受读写，所以每个服务器端都会作为某些分区的主副本，以及另外一些分区的备份副本，这样Kafka集群的所有服务端整体上对客户端是负载均衡的。</p><p>Kafka的生产者和消费者相对于服务器端而言都是客户端。</p><p>Kafka生产者客户端发布消息到服务端的指定主题，会指定消息所属的分区。生产者发布消息时根据消息是否有键，采用不同的分区策略。消息没有键时，通过轮询方式进行客户端负载均衡；消息有键时，根据分区语义（例如hash）确保相同键的消息总是发送到同一分区。</p><p>Kafka的消费者通过订阅主题来消费消息，并且每个消费者都会设置一个消费组名称。因为生产者发布到主题的每一条消息都只会发送给消费者组的一个消费者。所以，如果要实现传统消息系统的“队列”模型，可以让每个消费者都拥有相同的消费组名称，这样消息就会负责均衡到所有的消费者；如果要实现“发布-订阅”模型，则每个消费者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。</p><p>分区是消费者现场模型的最小并行单位。如下图（图1）所示，生产者发布消息到一台服务器的3个分区时，只有一个消费者消费所有的3个分区。在下图（图2）中，3个分区分布在3台服务器上，同时有3个消费者分别消费不同的分区。假设每个服务器的吞吐量是300MB，在下图（图1）中分摊到每个分区只有100MB，而在下图（图2）中，集群整体的吞吐量有900MB。可以看到，增加服务器节点会提升集群的性能，增加消费者数量会提升处理性能。</p><p><img src="https://vinxikk.github.io/img/kafka/kafka-partition.png" alt="Kafka的分区"></p><p>同一个消费组下多个消费者互相协调消费工作，Kafka会将所有的分区平均地分配给所有的消费者实例，这样每个消费者都可以分配到数量均等的分区。Kafka的消费组管理协议会动态地维护消费组的成员列表，当一个新消费者加入消费者组，或者有消费者离开消费组，都会触发再平衡操作。</p><p>Kafka的消费者消费消息时，只保证在一个分区内的消息的完全有序性，并不保证同一个主题汇中多个分区的消息顺序。而且，消费者读取一个分区消息的顺序和生产者写入到这个分区的顺序是一致的。比如，生产者写入“hello”和“Kafka”两条消息到分区P1，则消费者读取到的顺序也一定是“hello”和“Kafka”。如果业务上需要保证所有消息完全一致，只能通过设置一个分区完成，但这种做法的缺点是最多只能有一个消费者进行消费。一般来说，只需要保证每个分区的有序性，再对消息键(message Key 可以是user id等)来保证相同键的所有消息落入同一分区，就可以满足绝大多数的应用。</p><h3 id="Kafka命令行"><a href="#Kafka命令行" class="headerlink" title="Kafka命令行"></a>Kafka命令行</h3><ol><li><p>查看当前服务器中的所有topic</p><p><code>kafka-topics.sh --zookeeper hadoop001:2182 --list</code></p></li><li><p>创建topic</p><p><code>kafka-topics.sh --zookeeper hadoop001:2181 --create --replication-factor 3 --partitions 1 --topic first</code></p><p>选项说明：</p><p><code>--replication-factor</code> 定义副本数</p><p><code>--partitions</code> 定义分区数</p><p><code>--topic</code> 定义topic名</p></li><li><p>删除topic</p><p><code>kafka-topics.sh --zookeeper hadoop001:2181 --delete --topic first</code></p><p>需要server.properties中设置<code>delete.topic.enable=true</code>，否则只是标记删除或者直接重启。</p></li><li><p>发送消息</p><p><code>kafka-console-producer.sh --broker-list hadoop001:9092 --topic first</code></p></li><li><p>消费消息</p><p><code>kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --from-beginning --topic first</code></p><p><code>--from-beginning:</code> 会把first主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。</p></li><li><p>查看某个topic的详情</p><p>kafka-topics.sh –zookeeper hadoop001:2181 –describe –topic first</p></li></ol><h3 id="Kafka配置信息"><a href="#Kafka配置信息" class="headerlink" title="Kafka配置信息"></a>Kafka配置信息</h3><h4 id="Broker配置信息"><a href="#Broker配置信息" class="headerlink" title="Broker配置信息"></a>Broker配置信息</h4><table><thead><tr><th>属性</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>broker.id</td><td></td><td>broker的唯一标识</td></tr><tr><td>log.dirs</td><td>/tmp/kafka-logs</td><td>kafka数据存放的目录。可以指定多个目录，中间用逗号分隔，当新partition被创建时会被存放到当前存放partition最少的目录</td></tr><tr><td>port</td><td>9092</td><td>BrokerServer接受客户端连接的端口号</td></tr><tr><td>zookeeper.connect</td><td>null</td><td>Zookeeper的连接串，格式为：hostname1:port1,hostname2:port2,hostname3:port3。可以填一个或多个，为了提高可靠性，建议都填上。注意，此配置允许我们指定一个zookeeper路径来存放此kafka集群的所有数据，为了与其他应用集群区分开，建议在此配置中指定本集群存放目录，格式为：hostname1:port1,hostname2:port2,hostname3:port3/chroot/path 。需要注意的是，消费者的参数要和此参数一致。</td></tr><tr><td>delete.topic.enable</td><td>false</td><td>启用delete topic参数，建议设置为true</td></tr></tbody></table><h4 id="Producer配置信息"><a href="#Producer配置信息" class="headerlink" title="Producer配置信息"></a>Producer配置信息</h4><table><thead><tr><th>属性</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>metadata.broker.list</td><td></td><td>启动时producer查询brokers的列表，可以是集群中所有brokers的一个子集。注意，这个参数只是用来获取topic的元信息用，producer会从元信息中挑选合适的broker并与之建立socket连接。格式是：host1:port1,host2:port2。</td></tr><tr><td>request.required.acks</td><td>0</td><td></td></tr><tr><td>request.timeout.ms</td><td>10000</td><td>Broker等待ack的超时时间，若等待时间超过此值，会返回客户端错误信息。</td></tr><tr><td>producer.type</td><td>sync</td><td>同步异步模式。async表示异步，sync表示同步。如果设置成异步模式，可以允许生产者以batch的形式push数据，这样会极大的提高broker性能，推荐设置为异步。</td></tr></tbody></table><h4 id="Consumer配置信息"><a href="#Consumer配置信息" class="headerlink" title="Consumer配置信息"></a>Consumer配置信息</h4><table><thead><tr><th>属性</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>group.id</td><td></td><td>Consumer的组ID，相同goup.id的consumer属于同一个组。</td></tr><tr><td>zookeeper.connect</td><td></td><td>Consumer的zookeeper连接串，要和broker的配置一致。</td></tr><tr><td>consumer.id</td><td>null</td><td>如果不设置会自动生成。</td></tr><tr><td>socket.timeout.ms</td><td>30*1000</td><td>网络请求的socket超时时间。实际超时时间由max.fetch.wait + socket.timeout.ms 确定。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop导入数据ERROR: NoClassDefFoundError: org/json/JSONObject</title>
      <link href="/2018/05/09/sqoop/sqoop-import-error/"/>
      <url>/2018/05/09/sqoop/sqoop-import-error/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL数据导入HDFS-ERROR"><a href="#MySQL数据导入HDFS-ERROR" class="headerlink" title="MySQL数据导入HDFS ERROR"></a>MySQL数据导入HDFS ERROR</h3><p>执行如下sqoop命令出错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password ruozedata \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--table staff \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span></span></pre></td></tr></table></figure><p>抛错截图：</p><p><img src="https://vinxikk.github.io/img/sqoop/sqoop-mysql-to-hdfs-error.png" alt="MySQL导入HDFS抛错"></p><p>错误原因：</p><p>缺少<code>java-json.jar</code></p><p>解决办法：</p><p>添加相应jar包<code>cp java-json.jar /home/vinx/app/sqoop/lib/</code></p>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop-ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中delete时抛错：...update or delete... not support these operations.</title>
      <link href="/2018/05/08/hive/hive-delete-error/"/>
      <url>/2018/05/08/hive/hive-delete-error/</url>
      
        <content type="html"><![CDATA[<h3 id="delete-error"><a href="#delete-error" class="headerlink" title="delete error"></a>delete error</h3><p>在Hive中使用<code>delete</code>删除数据时抛以下错误：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10086&gt; delete from downstream where id in (5,6);</span></pre></td></tr></table></figure><p><em>Error: Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations. (state=42000,code=10294)</em></p><p>错误原因：</p><p><code>update</code>和<code>delete</code>都属于事务操作，Hive的行级修改，需要开启事务。</p><p>解决办法：</p><p>在hive-site.xml中，增加如下配置：</p><p><code>hive.support.concurrency – true</code></p><p><code>hive.enforce.bucketing – true</code></p><p><code>hive.exec.dynamic.partition.mode – nonstrict</code></p><p><code>hive.txn.manager –org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</code></p><p><code>hive.compactor.initiator.on – true</code></p><p><code>hive.compactor.worker.threads – 1</code></p><p><code>hive.in.test - true</code></p><p>保存后，重启Hive服务。</p><hr><p>参考：</p><p><a href="https://stackoverflow.com/questions/34198339/attempt-to-do-update-or-delete-using-transaction-manager-that-does-not-support-t" target="_blank" rel="noopener">https://stackoverflow.com/questions/34198339/attempt-to-do-update-or-delete-using-transaction-manager-that-does-not-support-t</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据重刷机制</title>
      <link href="/2018/05/07/dw/dw-data-rebrush/"/>
      <url>/2018/05/07/dw/dw-data-rebrush/</url>
      
        <content type="html"><![CDATA[<h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h3><p>使用Sqoop抽取数据，有时会出现数据的丢失，上下游数据会出现不一致的情况，但在整个过程中并没有抛ERROR。那么针对这个问题，该如何解决呢？</p><p>数据丢失或者不准确，归根结底就是：要么上下游数据量不同，要么数据内容不同。基于此，数据重刷要解决的问题就是验证上下游数据量是否相同，不相同则重刷；验证数据内容是否相同，不相同则重刷。</p><p>基于上述问题，为了确认使用Sqoop做数据迁移时，是否存在数据丢失，需要做数据监控。</p><h3 id="数据重刷"><a href="#数据重刷" class="headerlink" title="数据重刷"></a>数据重刷</h3><p>背景：已经count验证，现在发现上下游的数据不准确</p><p>重刷机制：通过对上下游的两个表full outer join来对比字段的NULL值</p><p>创建表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> upstream(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">in</span> <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> downstream <span class="keyword">like</span> upstream;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/upstream.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> upstream;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/downstream.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> downstream;</span></pre></td></tr></table></figure><p>对上游表和下游表做<code>full outer join</code>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">* </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> upstream up</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> downstream down</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> up.id = down.id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------+----------+------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| up.id  |  up.name  | down.id  | down.name  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------+----------+------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 1      | zhangsan  | 1        | zhangsan   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 2      | lisi      | NULL     | NULL       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 3      | wangwu    | 3        | wangwu     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| NULL   | NULL      | 5        | alice      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| NULL   | NULL      | 6        | tom        |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| 7      | jack      | NULL     | NULL       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------+----------+------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">6 rows selected (72.696 seconds)</span></pre></td></tr></table></figure><p>将全连接的结果保存：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> udstream <span class="keyword">as</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">u.id <span class="keyword">as</span> uid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">u.name <span class="keyword">as</span> uname,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">d.id <span class="keyword">as</span> did,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">d.name <span class="keyword">as</span> dname</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> upstream u</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> downstream d</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> u.id = d.id;</span></pre></td></tr></table></figure><p>以上游表upstream为标准，对全连接后的大表udstream做筛选：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> udstream <span class="keyword">where</span> uid <span class="keyword">is</span> <span class="literal">NULL</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| udstream.uid  | udstream.uname  | udstream.did  | udstream.dname  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| NULL          | NULL            | 5             | alice           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| NULL          | NULL            | 6             | tom             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows selected (0.129 seconds)</span></pre></td></tr></table></figure><p>发现did为5和6的行，uid为NULL，说明下游表downstream数据多了，需要删除did为5和6的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> downstream <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">in</span> (<span class="number">5</span>,<span class="number">6</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 也可以根据上面数据直接删除</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> downstream </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">in</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span> did <span class="keyword">from</span> udstream <span class="keyword">where</span> uid <span class="keyword">is</span> <span class="literal">NULL</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure><p><strong>注意：</strong>Hive中的<code>update</code>和<code>delete</code>都属于事务操作，需要开启事务。</p><p>发现uid为2和7的行，did为NULL，说明下游表downstream数据少了，需要追加uid为2和7的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> udstream <span class="keyword">where</span> did <span class="keyword">is</span> <span class="literal">NULL</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| udstream.uid  | udstream.uname  | udstream.did  | udstream.dname  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2             | lisi            | NULL          | NULL            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 7             | jack            | NULL          | NULL            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows selected (0.229 seconds)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> downstream <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">'lisi'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> downstream <span class="keyword">values</span>(<span class="number">7</span>,<span class="string">'jack'</span>);</span></pre></td></tr></table></figure><p>经过重新构建，也就是重刷后的数据是：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> downstream;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------+------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| downstream.id  | downstream.name  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------+------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 1              | zhangsan         |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 2              | lisi             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 3              | wangwu           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 7              | jack             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------+------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4 rows selected (0.133 seconds)</span></pre></td></tr></table></figure><p>数据重刷完成，下游表的数据得到了校正。</p><p>总体流程就是：使用count校验数据量是否相同（也可以进一步校验数据内容是否相同），如果不同，则校验数据内容，并重刷下游表的数据。</p><p><code>full outer join</code>其实就是<code>left join</code> + <code>right join</code>的结果，为NULL的刚好是缺失的或者多出的，而交集是上下游都有的，需要做的就是对下游表做<code>delete</code>或<code>insert</code>。</p>]]></content>
      
      
      <categories>
          
          <category> DataWarehouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据重刷 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban入门</title>
      <link href="/2018/05/06/azkaban/azkaban-basic/"/>
      <url>/2018/05/06/azkaban/azkaban-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="Azkaban基础"><a href="#Azkaban基础" class="headerlink" title="Azkaban基础"></a>Azkaban基础</h3><hr><p>工作流的概念</p>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MongoDB同步数据到Hive</title>
      <link href="/2018/05/02/sqoop/sqoop-mongodb-to-hive/"/>
      <url>/2018/05/02/sqoop/sqoop-mongodb-to-hive/</url>
      
        <content type="html"><![CDATA[<h3 id="MongoDB同步数据到Hive"><a href="#MongoDB同步数据到Hive" class="headerlink" title="MongoDB同步数据到Hive"></a>MongoDB同步数据到Hive</h3><p>Sqoop作为常用的数据同步工具，常用于RDBMS和HDFS的数据迁移，但是不支持NoSql，比如说MongoDB。</p><p>假如业务数据库是MongoDB，现在的需求是把MongoDB的数据同步到Hive中。</p><p><img src="https://vinxikk.github.io/img/sqoop/sqoop-mongodb-to-hive.png" alt="MongoDB to Hive"></p><ol><li><p>MongoDB可以导出数据为csv格式或者json格式的文件，csv是以逗号分隔的，可以直接把这个文件put到HDFS上，然后load到Hive。但是如果数据本身就自带一个或者多个逗号，那么这样做就会造成字段错位的问题。基于此，我们选择生成json格式的文件。</p></li><li><p>MongoDB可以条件导出，这样我们可以每次只导出增量数据。</p></li><li><p>如果使用json格式的文件，那么数据相当于在Hive中只有一列，我们可以每天或者每月导出增量数据，每次同步数据的时候都把这次的数据放到以一个分区中（第一次同步之前需要建立好分区表）。可以以时间为分区字段，然后scp将文件发送到Hive的客户端上，通过调度工具每天或每月定时执行任务，将文件直接load到Hive分区表中。</p></li><li><p>导入到Hive表中后，这个Hive表只有1个主要字段及分区字段，可以通过hive_json_object(name,’$.xxx’)解析，导入一个新表里面，新表的字段就是json中的key。</p><p>比如json格式如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="attr">"name"</span>: <span class="string">"zhangsan"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="attr">"city"</span>: <span class="string">"shenzhen"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="attr">"children"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"girl"</span>: <span class="string">"alice"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>Hive中解析如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">get_json_object(<span class="keyword">name</span>, <span class="string">'$.name'</span>) <span class="keyword">as</span> <span class="keyword">name</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">get_json_object(<span class="keyword">name</span>, <span class="string">'$.city'</span>) <span class="keyword">as</span> city,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">get_json_object(get_json_object(<span class="keyword">name</span>, <span class="string">'$.girl'</span>), <span class="string">'$.children'</span>) <span class="keyword">as</span> girl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">tb_name;</span></pre></td></tr></table></figure></li><li><p>判断MongoDB的数据是否已经导入文件并发送过来</p><p>在调度器上面配置一个检查任务，当到了定时执行同步任务的时候让同步任务依赖一个检查任务，检查任务就是去不断检查scp的文件传输过来没，如果没有过来隔一段时间再去检查，直到检查到数据来了，检查任务就结束，后面的同步任务就开始执行。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的UDF自定义函数</title>
      <link href="/2018/04/28/hive/hive-udf/"/>
      <url>/2018/04/28/hive/hive-udf/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive的内置函数"><a href="#Hive的内置函数" class="headerlink" title="Hive的内置函数"></a>Hive的内置函数</h3><p>Hive为我们提供了一些内置函数，比如截取字段串、大小写转换等。</p><p>测试substr()函数：</p><ol><li><p>建立一个伪表dual：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual(<span class="keyword">id</span> <span class="keyword">string</span>);</span></pre></td></tr></table></figure></li><li><p>准备数据</p><p>在本地创建一个dual.txt文件，内容为一个空格或者空行</p></li><li><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/dual.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dual;</span></pre></td></tr></table></figure></li><li><p>进行测试：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">substr</span>(<span class="string">'spark'</span>,<span class="number">1</span>,<span class="number">3</span>) <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| spa  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.116 seconds)</span></pre></td></tr></table></figure></li><li><p>也可以直接使用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">substr</span>(<span class="string">'spark'</span>,<span class="number">1</span>,<span class="number">3</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| spa  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.104 seconds)</span></pre></td></tr></table></figure></li></ol><h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><p>添加maven依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-metastore --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-metastore<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-common --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-service --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-service<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr></table></figure><h4 id="大写转小写"><a href="#大写转小写" class="headerlink" title="大写转小写"></a>大写转小写</h4><ol><li><p>创建<code>UpperToLowerCase</code>类，继承<code>UDF</code>，重写<code>evaluate</code>方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UpperToLowerCase</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">/*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">     * 重载evaluate</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">     * 访问限制必须是public</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">     */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String word)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        String lowerWord = word.toLowerCase();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> lowerWord;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li><li><p>打包上传到hadoop集群上</p></li><li><p>将jar包放到hive的classpath下</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add jar /home/vinx/jars/hive.jar;</code></p></li><li><p>创建临时函数，指定完整类名（包名+类名）</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>create temporary function tolower as &#39;com.ivinx.hive.UpperToLowerCase&#39;;</code></p></li><li><p>使用临时函数</p><p><code>select tolower(&#39;HELLO SPARK&#39;);</code></p></li></ol><h4 id="根据电话号码显示归属地信息"><a href="#根据电话号码显示归属地信息" class="headerlink" title="根据电话号码显示归属地信息"></a>根据电话号码显示归属地信息</h4><ol><li><p>创建工具类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoneNumParse</span> <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">static</span> HashMap&lt;String, String&gt; phoneMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">static</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        phoneMap.put(<span class="string">"136"</span>, <span class="string">"beijing"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        phoneMap.put(<span class="string">"137"</span>, <span class="string">"shanghai"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        phoneMap.put(<span class="string">"138"</span>, <span class="string">"shenzhen"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">evaluate</span><span class="params">(<span class="keyword">int</span> phoneNum)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        String num = String.valueOf(phoneNum);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        String province = phoneMap.get(num.substring(<span class="number">0</span>, <span class="number">3</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> province==<span class="keyword">null</span>?<span class="string">"foreign"</span>:province;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//测试</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        String string = evaluate(<span class="number">136666</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        System.out.println(string);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li><li><p>重新打jar包，然后上传到hadoop集群上</p></li><li><p>将jar包放到hive的classpath下</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add jar /home/vinx/jars/hive.jar;</code></p></li><li><p>创建临时函数，指定完整类名</p><p>create temporary function getprovince as ‘com.ivinx.hive.PhoneNumParse’;</p></li><li><p>创建本地数据</p><p>创建flow.txt，并追加以下数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ vi flow.txt</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">1367788,1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">1367788,10</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">1377788,80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">1377788,97</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">1387788,98</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1387788,99</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">1387788,100</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">1555118,99</span></pre></td></tr></table></figure></li><li><p>创建表，然后加载数据</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>create table flow(phonenum int,flow int) row format delimited fields terminated by &#39;,&#39;;</code></p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>load data local inpath &#39;/home/vinx/data/flow.txt&#39; into table flow;</code></p></li><li><p>查询结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10086&gt; select phonenum,getprovince(phonenum),flow from flow;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------+-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| phonenum  |    _c1    | flow  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------+-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 1367788   | beijing   | 1     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 1367788   | beijing   | 10    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 1377788   | shanghai  | 80    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 1377788   | shanghai  | 97    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 1387788   | shenzhen  | 98    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 1387788   | shenzhen  | 99    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 1387788   | shenzhen  | 100   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 1555118   | foreign   | 99    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------+-------+--+</span></span></pre></td></tr></table></figure></li></ol><h4 id="Json数据解析"><a href="#Json数据解析" class="headerlink" title="Json数据解析"></a>Json数据解析</h4><ol><li><p>创建数据源文件</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"1193"</span>,<span class="attr">"rate"</span>:<span class="string">"5"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978300760"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"661"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978302109"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"914"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978301968"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"3408"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978300275"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"2355"</span>,<span class="attr">"rate"</span>:<span class="string">"5"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978824291"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"1197"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978302268"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr></table></figure></li><li><p>创建表，然后上传数据</p><p><code>create table json(line string);</code></p><p><code>load data local inpath ‘/home/vinx/data/json.txt’ into table json;</code></p></li><li><p>与json数据对应的javabean</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MovieRateBean</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String movie;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String rate;<span class="comment">//评分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String timeStamp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String uid;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span>  <span class="keyword">this</span>.movie+<span class="string">"\t"</span>+<span class="keyword">this</span>.rate+<span class="string">"\t"</span>+<span class="keyword">this</span>.timeStamp+<span class="string">"\t"</span>+<span class="keyword">this</span>.uid;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//  get、set方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li><li><p>java工具类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ivinx.hive;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.TypeReference;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonParse</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String jsonStr)</span></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        MovieRateBean movieRateBean=JSON.parseObject(jsonStr,<span class="keyword">new</span> TypeReference&lt;MovieRateBean&gt;()&#123;&#125;);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        returnmovieRateBean.toString();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li><li><p>打jar包然后上传到hadoop集群</p></li><li><p>将jar包添加到hive下的classpath</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add jar /home/vinx/jars/hive.jar;</code></p></li><li><p>将fastjson的jar包添加到hive下的classpath</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add jar /home/vinx/jars/fastjson-1.1.41.jar;</code></p></li><li><p>创建临时函数</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>create temporary function parsejson as &#39;com.ivinx.hive.JsonParse&#39;;</code></p></li><li><p>执行查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10086&gt; select parsejson(line) from json limit 10;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|         _c0         |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 1193  5       978300760       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 661   3       978302109       1   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 914   3       978301968       1   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 3408  4       978300275       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 2355  5       978824291       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 1197  3       978302268       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 1287  5       978302039       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 2804  5       978300719       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| 594   4       978302268       1   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| 919   4       978301368       1   |</span></pre></td></tr></table></figure></li><li><p>显示字段名</p><p>从上面的结果可以看出来，数据虽然分开了，但是没有字段名，现在我们通过建表来实现显示字段名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie <span class="keyword">as</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">split</span>(parsejson(line), <span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">as</span> movieid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">split</span>(parsejson(line), <span class="string">'\t'</span>)[<span class="number">1</span>] <span class="keyword">as</span> rate,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">split</span>(parsejson(line), <span class="string">'\t'</span>)[<span class="number">2</span>] <span class="keyword">as</span> timestring,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">split</span>(parsejson(line), <span class="string">'\t'</span>)[<span class="number">3</span>] <span class="keyword">as</span> uid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> <span class="keyword">json</span>;</span></pre></td></tr></table></figure><p>再次执行查询，查看结果：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10086&gt; select * from movie;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+----------------+----------------------+---------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| t_rating.movieid  | t_rating.rate  | t_rating.timestring  | t_rating.uid  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+----------------+----------------------+---------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 919               | 4              | 978301368            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 594               | 4              | 978302268            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 2804              | 5              | 978300719            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 1287              | 5              | 978302039            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 1197              | 3              | 978302268            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 2355              | 5              | 978824291            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 3408              | 4              | 978300275            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 914               | 3              | 978301968            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| 661               | 3              | 978302109            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| 1193              | 5              | 978300760            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+----------------+----------------------+---------------+--+</span></span></pre></td></tr></table></figure></li></ol><h3 id="transform关键字的使用"><a href="#transform关键字的使用" class="headerlink" title="transform关键字的使用"></a>transform关键字的使用</h3><p>将某一个字段时间戳改为输出周几，可以不用实现UDF。</p><p>现在直接使用上面创建好的表，将第三个字段改为时间。</p><h4 id="编写python脚本"><a href="#编写python脚本" class="headerlink" title="编写python脚本"></a>编写python脚本</h4><p>在本地创建一个python脚本trans.py：</p><p><code>vi trans.py</code></p><p>脚本代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  line = line.strip()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  movieid, rating, unixtime,userid = line.split(<span class="string">'\t'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">print</span> <span class="string">'\t'</span>.join([movieid, rating, str(weekday),userid])</span></pre></td></tr></table></figure><h4 id="使用该脚本"><a href="#使用该脚本" class="headerlink" title="使用该脚本"></a>使用该脚本</h4><ol><li><p>添加脚本到hive</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add FILE /home/vinx/scripts/trans.py;</code></p></li><li><p>使用该脚本</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>select transform(movieid,rate,timestring,uid) using &#39;python trans.py&#39; as (mov,rat,tim,uid) from movie;</code></p><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">INFO  : Number of reduce tasks is set to 0 since there&#39;s no reduce operator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">INFO  : number of splits:1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">INFO  : Submitting tokens for job: job_1546821616463_0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">INFO  : The url to track the job: http:&#x2F;&#x2F;hadoop001:8088&#x2F;proxy&#x2F;application_1546821616463_0002&#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">INFO  : Starting Job &#x3D; job_1546821616463_0002, Tracking URL &#x3D; http:&#x2F;&#x2F;hadoop001:8088&#x2F;proxy&#x2F;application_1546821616463_0002&#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">INFO  : Kill Command &#x3D; &#x2F;home&#x2F;vinx&#x2F;app&#x2F;hadoop&#x2F;bin&#x2F;hadoop job  -kill job_1546821616463_0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">INFO  : 2018-01-05 00:35:37,847 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">INFO  : 2018-01-05 00:35:56,316 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 1.55 sec</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">INFO  : MapReduce Total cumulative CPU time: 1 seconds 550 msec</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">INFO  : Ended Job &#x3D; job_1546821616463_0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+-------+------+------+------+--+</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|  mov  | rat  | tim  | uid  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+-------+------+------+------+--+</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">| 1197  | 3    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">| 2355  | 5    | 7    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">| 3408  | 4    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">| 914   | 3    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">| 661   | 3    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">| 1193  | 5    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">+-------+------+------+------+--+</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">6 rows selected (36.201 seconds)</span></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的dual伪表</title>
      <link href="/2018/04/27/hive/hive-dual/"/>
      <url>/2018/04/27/hive/hive-dual/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive的伪表"><a href="#Hive的伪表" class="headerlink" title="Hive的伪表"></a>Hive的伪表</h3><p>Hive中的伪表，也叫虚表，和Oracle中的dual相似，dual是Oracle提供的最小的工作表，只有一行一列，具有某些特殊的功能。</p><p>下面我们在Hive中构建一个类似于Oracle的dual虚表。</p><p>创建dual.txt，将X重定向到文件中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 data]$ touch dual.txt</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 data]$ <span class="built_in">echo</span> <span class="string">'X'</span> &gt; dual.txt</span></pre></td></tr></table></figure><p>在hive中创建dual表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual (dummy <span class="keyword">string</span>);</span></pre></td></tr></table></figure><p>加载dual.txt数据到dual表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/dual.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dual;</span></pre></td></tr></table></figure><p>查看dual虚表中的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| dual.dummy  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| X           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.142 seconds)</span></pre></td></tr></table></figure><p>测试dual虚表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"> <span class="keyword">select</span> <span class="number">1</span>+<span class="number">1</span> <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> +<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.142 seconds)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="string">'hello hive'</span> <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| hello hive  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.159 seconds)</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-dual </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的调优（二）</title>
      <link href="/2018/04/26/hive/hive-optimize-2/"/>
      <url>/2018/04/26/hive/hive-optimize-2/</url>
      
        <content type="html"><![CDATA[<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><h4 id="合理设置Map数"><a href="#合理设置Map数" class="headerlink" title="合理设置Map数"></a>合理设置Map数</h4><ol><li><p>通常情况下，job会通过input的目录产生一个或者多个map任务。</p><p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p></li><li><p>是不是map数越多越好？</p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128M），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p></li><li><p>是不是保证每个map处理接近128M的文件块，就高枕无忧了？</p><p>答案是不一定。比如有一个127M的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却又几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p></li></ol><p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数。</p><h4 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h4><p>在map执行前合并小文件，减少map。CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式），HiveInputFormat没有对小文件的合并功能。</p><p><code>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</code></p><h4 id="复杂文件增加Map数"><a href="#复杂文件增加Map数" class="headerlink" title="复杂文件增加Map数"></a>复杂文件增加Map数</h4><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减小，从而提高任务的执行效率。</p><p>增加map的方法：</p><p>根据<code>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))</code>=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><p>案例剖析：</p><ol><li><p>执行查询</p><p>hive (default)&gt; <code>select count(*) from emp;</code></p><p>Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</p></li><li><p>设置最大切片值为100个字节</p><p>hive (default)&gt; <code>set mapreduce.input.fileinputformat.split.maxsize=100;</code></p><p>hive (default)&gt; <code>select count(*) from emp;</code></p><p>Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1</p></li></ol><h4 id="合理设置Reduce数"><a href="#合理设置Reduce数" class="headerlink" title="合理设置Reduce数"></a>合理设置Reduce数</h4><h5 id="调整reduce个数方法一"><a href="#调整reduce个数方法一" class="headerlink" title="调整reduce个数方法一"></a>调整reduce个数方法一</h5><ol><li><p>每个Reduce处理的数据量默认是256MB</p><p><code>set hive.exec.reducers.bytes.per.reducer=256000000</code></p></li><li><p>每个任务最大的reduce数，默认为1009</p><p><code>set hive.exec.reducers.max=1009</code></p></li><li><p>计算reducer数的公式</p><p>N=min(参数2=1009，总输入数据量/参数1=？)</p></li></ol><h5 id="调整reduce个数方法二"><a href="#调整reduce个数方法二" class="headerlink" title="调整reduce个数方法二"></a>调整reduce个数方法二</h5><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数：</p><p><code>set mapreduce.job.reduces=5;</code></p><h5 id="reduce个数并不是越多越好"><a href="#reduce个数并不是越多越好" class="headerlink" title="reduce个数并不是越多越好"></a>reduce个数并不是越多越好</h5><ol><li>过多的启动和初始化reduce也会消耗时间和资源；</li><li>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li></ol><p>在设置reduce个数的时候也需要考虑这两个原则：</p><p>处理大数据量利用合适的reduce数；单个reduce任务处理数据量大小要合适。</p><h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数<code>hive.exec.parallel</code>值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><p>打开任务并行执行（默认false）：</p><p><code>set hive.exec.parallel=true;</code></p><p>同一个sql允许最大并行度（默认为8）：</p><p><code>set hive.exec.parallel.thread.number=16;</code>  </p><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p><h3 id="严格模式"><a href="#严格模式" class="headerlink" title="严格模式"></a>严格模式</h3><p>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。</p><p>开启严格模式需要修改<code>hive.mapred.mode</code>值为strict，开启严格模式可以禁止3种类型的查询：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    The mode in which the Hive operations are being performed. </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    In strict mode, some risky queries are not allowed to run. They include:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      Cartesian Product.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      No partition being picked up for a query.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      Comparing bigints and strings.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      Comparing bigints and doubles.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">      Orderby without limit.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><ol><li>对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</li><li>对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</li><li>限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</li></ol><h3 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h3><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  no limit. </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h3 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h3><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p><p>设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">               may be executed in parallel.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">               may be executed in parallel.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>不过hive本身也提供了配置项来控制reduce-side的推测执行：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.reduce.tasks.speculative.execution<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether speculative execution for reducers should be turned on. </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p><h3 id="执行计划（Explain）"><a href="#执行计划（Explain）" class="headerlink" title="执行计划（Explain）"></a>执行计划（Explain）</h3><p>基本语法：</p><p><code>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</code></p><p>案例：</p><ol><li><p>查看下面这条语句的执行计划</p><p>hive (default)&gt; <code>explain select * from emp;</code></p><p>hive (default)&gt; <code>explain select deptno, avg(sal) avg_sal from emp group by deptno;</code></p></li><li><p>查看详细执行计划</p><p>hive (default)&gt; <code>explain extended select * from emp;</code></p><p>hive (default)&gt; <code>explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</code></p></li></ol><p>以下是在MySQL中的显示：</p><p><img src="https://vinxikk.github.io/img/dw/hive-explain-mysql.png" alt="三种模式的关系"></p><p>EXPLAIN字段：</p><ol><li><p>table: 显示这一行的数据是关于哪张表的</p></li><li><p>possible_keys: 显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句。</p></li><li><p>key: 实际使用的索引。如果为NULL，则没有使用索引。MYSQL很少会选择优化不足的索引，此时可以在SELECT语句中使用USE INDEX（index）来强制使用一个索引或者用IGNORE INDEX（index）来强制忽略索引。</p></li><li><p>key_len: 使用的索引的长度。在不损失精确性的情况下，长度越短越好。</p></li><li><p>ref: 显示索引的哪一列被使用了，如果可能的话，是一个常数</p></li><li><p>rows: MySQL认为必须检索的用来返回请求数据的行数</p></li><li><p>type: 这是最重要的字段之一，显示查询使用了何种类型。从最好到最差的连接类型为system、const、eq_reg、ref、range、index和ALL</p></li><li><p>system/const: 可以将查询的变量转为常量.  如id=1; id为 主键或唯一键</p></li><li><p>eq_ref: 访问索引,返回某单一行的数据.(通常在联接时出现，查询使用的索引为主键或惟一键)</p></li><li><p>range: 这个连接类型使用索引返回一个范围中的行，比如使用&gt;或&lt;查找东西，并且该字段上建有索引时发生的情况(注:不一定好于index)</p></li><li><p>index: 以索引的顺序进行全表扫描，优点是不用排序,缺点是还要全表扫描</p></li><li><p>ALL: 全表扫描，应该尽量避免</p></li><li><p>Extra: 关于MYSQL如何解析查询的额外信息，主要有以下几种</p><ol><li>using index：只用到索引,可以避免访问表. </li><li>using where：使用到where来过虑数据. 不是所有的where clause都要显示using where. 如以=方式访问索引.</li><li>using tmporary：用到临时表</li><li>using filesort：用到额外的排序. (当使用order by v1,而没用到索引时,就会使用额外的排序)</li></ol></li><li><p>range checked for eache record(index map:N): 没有好的索引</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库</title>
      <link href="/2018/04/25/dw/dw-1/"/>
      <url>/2018/04/25/dw/dw-1/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h3><p>数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。</p><p>数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。</p><p>它出于分析性报告和决策支持目的而创建，为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。</p><h3 id="数据库与数据仓库的区别"><a href="#数据库与数据仓库的区别" class="headerlink" title="数据库与数据仓库的区别"></a>数据库与数据仓库的区别</h3><p><strong>数据库：</strong></p><p>是一种逻辑概念，用来存放数据的仓库，通过数据库软件来实现。数据库由很多表组成，表是二维的，一张表里可以有很多字段，对应的数据就一行一行写入表中。数据库的表，在于能够用二维表现多维关系。目前市面上流行的数据库都是二维数据库，如：Oracle、DB2、MySQL、Sybase、MS SQL Server等。</p><p><strong>数据仓库：</strong></p><p>是数据库概念的升级。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现的存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大得多。数据仓库主要用于数据挖掘和数据分析，辅助业务决策。数据仓库的表结构是依照分析需求，分析维度，分析指标进行设计的。</p><p>数据库与数据仓库的区别实际讲的是OLTP与OLAP的区别。</p><p><strong>操作型处理</strong>，叫联机事务处理OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。</p><p><strong>分析型处理</strong>，叫联机分析处理OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。</p><table><thead><tr><th>操作型处理</th><th>分析型处理</th></tr></thead><tbody><tr><td>细节的</td><td>综合的或提炼的</td></tr><tr><td>实体-关系（E-R）模型</td><td>星型模型或雪花模型</td></tr><tr><td>存取瞬间数据</td><td>存储历史数据，不包含最近的数据</td></tr><tr><td>可更新的</td><td>只读、只追加</td></tr><tr><td>一次操作一个单元</td><td>一次操作一个集合</td></tr><tr><td>性能要求高，响应时间短</td><td>性能要求宽松</td></tr><tr><td>面向事务</td><td>面向分析</td></tr><tr><td>一次操作数据量小</td><td>一次操作数据量大</td></tr><tr><td>支持日常操作</td><td>支持决策需求</td></tr><tr><td>数据量小</td><td>数据量大</td></tr><tr><td>客户订单、库存水平和银行账户查询等</td><td>客户收益分析、市场细分等</td></tr></tbody></table><h3 id="数据仓库架构分层"><a href="#数据仓库架构分层" class="headerlink" title="数据仓库架构分层"></a>数据仓库架构分层</h3><h4 id="数据仓库架构"><a href="#数据仓库架构" class="headerlink" title="数据仓库架构"></a>数据仓库架构</h4><p>数据仓库标准上可以分为四层：</p><p>ODS(operation data store)：原始数据层，存放原始数据，不做任何的处理。</p><p>DWD/DWI(data warehouse detail)：结构和粒度与ODS层是一致的，只做对ODS层数据进行清洗(脏数据/空值)。拉链表  脱敏 手机号 银行卡 密码(mysql加密)。</p><p>DWS(data warehouse service)：以DWD层为基础，进行轻度汇总，宽表(join)。</p><p>ADS/APP/domain域表(application data store)：按主题提供统计数据(group计算，少量可能做join计算取决于宽表没有你想要的字段)。</p><ol><li><p>ODS层：</p><p>为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。一般来说ODS层的数据和源系统的数据是同构的，主要目的是简化后续数据加工处理的工作。从数据粒度上来说ODS层的数据粒度是最细的。ODS层的表通常包括两类，一个用于存储当前需要加载的数据，一个用于存储处理完后的历史数据。历史数据一般保存3-6个月后需要清除，以节省空间。但不同的项目要区别对待，如果源系统的数据量不大，可以保留更长的时间，甚至全量保存。</p></li><li><p>DWD层：</p><p>为数据仓库明细层，DWD层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了脏数据/空值等）后的数据。这一层的数据一般是遵循数据库第三范式的，其数据粒度通常和ODS的粒度相同。在DWD层会保存BI系统中所有的历史数据，例如保存10年的数据。</p></li><li><p>DWS层：</p><p>为数据仓库服务层，这层数据是面向主题来组织数据的，通常是星形或雪花结构的数据。从数据粒度来说，这层的数据是轻度汇总级的数据，已经不存在明细数据了。从数据的时间跨度来说，通常是DWD层的一部分，主要的目的是为了满足用户分析的需求，而从分析的角度来说，用户通常只需要分析近几年（如近三年的数据）的即可。从数据的广度来说，任然覆盖了所有的业务数据。</p></li><li><p>ADS层：</p><p>为应用数据存储层，这层数据是完全为了满足具体的分析需求而构建的数据，也是星形或雪花结构的数据。从数据粒度来说是高度汇总的数据。从数据的广度来说，则并不一定会覆盖所有业务数据，而是DWS层数据的一个真子集，从某种意义上来说是DWS层数的一个重复。从极端情况来说，可以为每一张报表在ADS层构建一个模型来支持，达到以空间换时间的目的。</p><p>数据仓库的标准分层只是一个建议性质的标准，实际实施时需要根据实际情况确定数据仓库的分层，不同类型的数据也可能采取不同的分层方法。</p></li></ol><h4 id="为什么要对数据仓库分层"><a href="#为什么要对数据仓库分层" class="headerlink" title="为什么要对数据仓库分层"></a>为什么要对数据仓库分层</h4><ol><li>用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据。</li><li>如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗的过程，工作量巨大。</li><li>通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</li></ol><p>数据仓库的分层不是越多越好，合理的层次设计，以及计算成本和人力成本的平衡，是一个好的数仓架构的表现。</p><h4 id="库表命名原则"><a href="#库表命名原则" class="headerlink" title="库表命名原则"></a>库表命名原则</h4><p>数仓建设的第一步就是要有一个良好的命名规则。</p><p>比如：<code>dwd_whct_xmxx_m</code></p><ol><li>数仓分层：可能取值为ods, dwd, dws, ads等。</li><li>业务领域：可能为whct(文化传统)，whcp(文化产品)等。</li><li>自定义标签：比如项目信息为xmxx，用户可以自定义业务、项目和产品标签。</li><li>时间标签：比如d为天，m为月，y为年，di为增量表，df为全量表。</li></ol><p>ods: 源头数据原封不动的抽取一遍，准备层。</p><p>dw: dwd(数据仓库明细层), dws(数据仓库汇总层)，ods层数据经过etl清洗、转换、加载生成。</p><p>ads: 应用层，各个业务方或部门基于dwd和dws建立的数据集市（DM），原则上ads层数据是基于dw层的，不能直接访问ods层，该层只包含部门或业务方自己关心的dwd或dws。</p><h3 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h3><h4 id="元数据的定义"><a href="#元数据的定义" class="headerlink" title="元数据的定义"></a>元数据的定义</h4><p>数据仓库的元数据是关于数据仓库中数据的数据。它的作用类似于数据库管理系统的数据字典，保存了逻辑数据结构、文件、地址和索引等信息。广义上讲，在数据仓库中，元数据描述了数据仓库内数据的结构和建立方法的数据。</p><p>元数据是数据仓库管理系统的重要组成部分，元数据管理器是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p><ol><li>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</li><li>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</li><li>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</li></ol><p>元数据可分为技术元数据和业务元数据。</p><p>技术元数据为开发和管理数据仓库的IT人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。</p><p>而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。</p><p>由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体，如图所示：</p><p><img src="https://vinxikk.github.io/img/dw/dw-metadata.png" alt="元数据与各个组件的关系"></p><h4 id="元数据的存储方式"><a href="#元数据的存储方式" class="headerlink" title="元数据的存储方式"></a>元数据的存储方式</h4><p>元数据有两种常见存储方式：</p><p>一种是以数据集为基础，每一个数据集有对应的元数据文件，每一个元数据文件包含对应数据集的元数据内容；另一种存储方式是以数据库为基础，即元数据库。其中元数据文件由若干项组成，每一项表示元数据的一个要素，每条记录为数据集的元数据内容。</p><p>上述存储方式各有优缺点，第一种存储方式的优点是调用数据时相应的元数据也作为一个独立的文件被传输，相对数据库有较强的独立性，在对元数据进行检索时可以利用数据库的功能实现，也可以把元数据文件调到其他数据库系统中操作；不足是如果每一数据集都对应一个元数据文档，在规模巨大的数据库中则会有大量的元数据文件，管理不方便。第二种存储方式下，元数据库中只有一个元数据文件，管理比较方便，添加或删除数据集，只要在该文件中添加或删除相应的记录项即可。在获取某数据集的元数据时，因为实际得到的只是关系表格数据的一条记录，所以要求用户系统可以接受这种特定形式的数据。</p><p>因此推荐使用元数据库的方式。</p><p>元数据库用于存储元数据，因此元数据库最好选用主流的关系数据库管理系统。元数据库还包含用于操作和查询元数据的机制。建立元数据库的主要好处是提供统一的数据结构和业务规则，易于把企业内部的多个数据集市有机地集成起来。目前，一些企业倾向建立多个数据集市，而不是一个集中的数据仓库，这时可以考虑在建立数据仓库（或数据集市）之前，先建立一个用于描述数据、服务应用集成的元数据库，做好数据仓库实施的初期支持工作，对后续开发和维护有很大的帮助。元数据库保证了数据仓库数据的一致性和准确性，为企业进行数据质量管理提供基础。</p><h4 id="元数据的作用"><a href="#元数据的作用" class="headerlink" title="元数据的作用"></a>元数据的作用</h4><p>在数据仓库中，元数据的主要作用如下：</p><ol><li>描述哪些数据在数据仓库中，帮助决策分析者对数据仓库的内容定位。</li><li>定义数据进入数据仓库的方式，作为数据汇总、映射和清洗的指南。</li><li>记录业务事件发生而随之进行的数据抽取工作时间安排。</li><li>记录并检测系统数据一致性的要求和执行情况。</li><li>评估数据质量。</li></ol>]]></content>
      
      
      <categories>
          
          <category> DataWarehouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中lateral view与explode函数的使用</title>
      <link href="/2018/04/24/hive/hive-lateral-view-and-explode/"/>
      <url>/2018/04/24/hive/hive-lateral-view-and-explode/</url>
      
        <content type="html"><![CDATA[<h3 id="explode函数"><a href="#explode函数" class="headerlink" title="explode函数"></a>explode函数</h3><p>查看explode的函数说明：</p><p>hive (default)&gt; <code>desc function explode;</code></p><p>官方解释：</p><p><code>explode(a)</code> - separates the elements of array a into multiple rows, or the elements of a map into multiple rows and columns.</p><p><code>explode</code>函数可以将一个array或者map展开。</p><p>其中<code>explode(array)</code>将array中的元素分割为多行，也就是将array列表中的每一个元素作为一行。</p><p><code>explode(map)</code>将map中的元素分割为多个行和列，具体就是将map里的每一对元素作为一行，key为一列，value为一列。</p><h4 id="explode的使用"><a href="#explode的使用" class="headerlink" title="explode的使用"></a>explode的使用</h4><p>数据如下：</p><p>001,allen,Java|Scala|Python,90|80|82<br>002,kobe,Java|SQL|Python,82|95|91</p><p>建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> message(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    courses <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    scores <span class="built_in">array</span>&lt;<span class="built_in">int</span>&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'|'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/message.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> message;</span></pre></td></tr></table></figure><p>查询全表数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> message;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+---------------+----------------------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| message.id  | message.name  |      message.courses       | message.scores  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+---------------+----------------------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 001         | allen         | ["Java","Scala","Python"]  | [90,80,82]      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 002         | kobe          | ["Java","SQL","Python"]    | [82,95,91]      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+---------------+----------------------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows selected (0.127 seconds)</span></pre></td></tr></table></figure><p>查询指定下标的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,courses[<span class="number">1</span>] <span class="keyword">from</span> message;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--------+--------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|  id  |  name  |  _c2   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--------+--------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 001  | allen  | Scala  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 002  | kobe   | SQL    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--------+--------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows selected (0.115 seconds)</span></pre></td></tr></table></figure><p>使用explode查询array中的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">explode</span>(courses) <span class="keyword">from</span> message;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|   col   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| Java    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| Scala   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| Python  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| Java    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| SQL     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| Python  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">6 rows selected (0.118 seconds)</span></pre></td></tr></table></figure><p>UDTF不能出现在SELECT子句的外面，下面的SQL语句会出错：</p><p><code>select name,explode(courses) from message;</code></p><p>Error: Error while compiling statement: FAILED: SemanticException [Error 10081]: UDTF’s are not supported outside the SELECT clause, nor nested in expressions (state=42000,code=10081)</p><h4 id="lateral-view"><a href="#lateral-view" class="headerlink" title="lateral view"></a>lateral view</h4><p>lateral view: 侧视图，可以配合UDTF函数来使用，把某一行数据拆分为多行数据。</p><p>不加lateral view的UDTF只能提取单个字段拆分,并不能塞会原来数据表中，加上lateral view就可以将拆分的单个字段数据与原始表数据关联上。</p><p>使用lateral view的时候需要指定视图别名和生成的新列别名：</p><p><code>tableA lateral view UDTF(xxx) 视图别名 as a,b,c</code></p><p>lateral view explode相当于一个拆分courses字段的虚表，然后与原表进行关联。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- subview为视图别名，course为指定新列别名</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> subview.* <span class="keyword">from</span> message <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(courses) subview <span class="keyword">as</span> course;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| subview.course  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| Java            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| Scala           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| Python          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| Java            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| SQL             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| Python          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">6 rows selected (0.098 seconds)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,subview.* <span class="keyword">from</span> message <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(courses) subview <span class="keyword">as</span> course;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|  name  | subview.course  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">| allen  | Java            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">| allen  | Scala           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">| allen  | Python          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">| kobe   | Java            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">| kobe   | SQL             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">| kobe   | Python          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">6 rows selected (0.123 seconds)</span></pre></td></tr></table></figure><h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>表tmp两个字段：user, profile</p><p>abc    key1:value1,key2:value2<br>def    key1:value1,key2:value2,key3:value3,key4:value4<br>xyz    key1:value1</p><p>需要转换成下面的表结构：</p><p><img src="https://vinxikk.github.io/img/hive/hive-tmp-result.png" alt="转换后的表"></p><p>总体思路：</p><p>先按照“,”拆分，再按照”:”拆分</p><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>拆分”,”，并保存视图：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> tmp_profile_single <span class="keyword">as</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,profile_single</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tmp a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(a.profile,<span class="string">','</span>)) b <span class="keyword">as</span> profile_single;</span></pre></td></tr></table></figure><p>拆分”:”：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.user <span class="keyword">as</span> <span class="keyword">user</span>,t.arr[<span class="number">0</span>] <span class="keyword">as</span> profile_key,t.arr[<span class="number">1</span>] <span class="keyword">as</span> profile_value</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">split</span>(profile_single, <span class="string">':'</span>) <span class="keyword">as</span> arr <span class="keyword">from</span> tmp_profile_single</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> t;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> tmp_profile_single;</span></pre></td></tr></table></figure><h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">t1.user <span class="keyword">as</span> <span class="keyword">user</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">t1.arr[<span class="number">0</span>] <span class="keyword">as</span> profile_key,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">t1.arr[<span class="number">1</span>] <span class="keyword">as</span> profile_value</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">split</span>(profile_item, <span class="string">':'</span>) <span class="keyword">as</span> arr <span class="keyword">from</span> tmp <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">outer</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(profile, <span class="string">','</span>)) t <span class="keyword">as</span> profile_item</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> t1;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lateral view </tag>
            
            <tag> explode </tag>
            
            <tag> 列转行 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的复杂数据类型&amp;json_tuple的使用</title>
      <link href="/2018/04/24/hive/hive-type-json/"/>
      <url>/2018/04/24/hive/hive-type-json/</url>
      
        <content type="html"><![CDATA[<h3 id="复杂数据类型"><a href="#复杂数据类型" class="headerlink" title="复杂数据类型"></a>复杂数据类型</h3><p>Hive有三种复杂数据类型ARRAY、MAP和STRUCT，复杂数据类型允许任意层次的嵌套。</p><p>表info中有如下一行，我们用JSON格式来表示其数据结构：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"name"</span>:<span class="string">"zhangsan"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"friends"</span>:[<span class="string">"lisi"</span>,<span class="string">"wangwu"</span>], <span class="comment">//Array</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"children"</span>:&#123;                 <span class="comment">//Map</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"alice"</span>:<span class="number">18</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"tom"</span>:<span class="number">19</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"address"</span>:&#123;                 <span class="comment">//Struct</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"street"</span>:<span class="string">"W Jefferson Blvd"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"city"</span>:<span class="string">"Los Angeles"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>基于上述的数据结构，我们在Hive中创建对应的表，并导入数据。</p><p>数据文件info.txt</p><p>zhangsan,lisi_wangwu,alice:18_tom:19,Jefferson Boulevard_Los Angeles<br>jason,curry_kobe,daniel:20_sally:21,McClintock Ave_Los Angeles</p><p>创建表info: </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> info(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="built_in">int</span>&gt;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;</span></pre></td></tr></table></figure><p>字段解释：</p><ul><li><code>row format delimited fields terminated by &#39;,&#39;</code> - 字段分隔符</li><li><code>collection items terminated by &#39;_&#39;</code> - Array/Map/Struct的分隔符（数据分割符号）</li><li><code>map keys terminated by &#39;:&#39;</code> - Map中的key与value的分隔符</li><li><code>lines terminated by &#39;\n&#39;</code> - 行分隔符</li></ul><p>导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/info.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> info;</span></pre></td></tr></table></figure><p>访问三种集合列里的数据（分别是Array, Map, Struct的访问方式）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> friends[<span class="number">1</span>],children[<span class="string">'sally'</span>],address.city <span class="keyword">from</span> info <span class="keyword">where</span> <span class="keyword">name</span>=<span class="string">'jason'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">|  _c0  | _c1  |     city     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| kobe  | 21   | Los Angeles  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.43 seconds)</span></pre></td></tr></table></figure><h3 id="json-tuple的使用"><a href="#json-tuple的使用" class="headerlink" title="json_tuple的使用"></a>json_tuple的使用</h3><p><code>json_tuple()</code>函数也是UDTF函数，因为一个json字符串对应解析出n个字段，与原表数据关联的时候需要使用<code>lateral view</code>。</p><p>语法格式：</p><p><code>select id from table lateral view json_tuple(property, &#39;tag_id&#39;, &#39;tag_type&#39;);</code></p><p>表sku_info_tbl，表中有两个字段id, sku_info，表中有一条数据如下：</p><p>id: 101</p><p>sku_info: </p><p>[{“skuId”:”1017570”,”num”:”2”,”price”:5.8,”jd_price”:23.9,”sale_price”:5.8},{“skuId”:”1329431”,”num”:”1”,”price”:38.38,”jd_price”:59,”sale_price”:36.84},{“skuId”:”1381473”,”num”:”1”,”price”:8.5,”jd_price”:39.8,”sale_price”:8.5}]</p><p>建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> sku_info_tbl(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sku_info <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sku_info_tbl <span class="keyword">values</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="string">'101'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="string">'[&#123;"skuId":"1017570","num":"2","price":5.8,"jd_price":23.9,"sale_price":5.8&#125;,&#123;"skuId":"1329431","num":"1","price":38.38,"jd_price":59,"sale_price":36.84&#125;,&#123;"skuId":"1381473","num":"1","price":8.5,"jd_price":39.8,"sale_price":8.5&#125;]'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure><p>首先将sku_info_tbl表中的list列表拆成三行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span>,sku_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sku_info_tbl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="keyword">substr</span>(sku_infos,<span class="number">2</span>,<span class="keyword">length</span>(sku_infos)<span class="number">-2</span>),<span class="string">'&#125;,'</span>)) subview <span class="keyword">as</span> sku_info;</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/hive/hive-json-1.png" alt="拆分sku_infos"></p><p>补全每行记录：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">t.id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">substr</span>(t.sku_info,<span class="keyword">length</span>(t.sku_info),<span class="number">1</span>) = <span class="string">'&#125;'</span> <span class="keyword">then</span> t.sku_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">concat</span>(t.sku_info,<span class="string">'&#125;'</span>) <span class="keyword">end</span> <span class="keyword">as</span> sku_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,sku_info <span class="keyword">from</span> sku_info_tbl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="keyword">substr</span>(sku_infos,<span class="number">2</span>,<span class="keyword">length</span>(sku_infos)<span class="number">-2</span>),<span class="string">'&#125;,'</span>)) subview <span class="keyword">as</span> sku_info</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">) t;</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/hive/hive-json-2.png" alt="补全&quot;}&quot;号"></p><p>将每一行的json数据拆分成对应字段：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">m.id,skuid,<span class="keyword">num</span>,price,jd_price,sale_price</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">t.id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">substr</span>(t.sku_info,<span class="keyword">length</span>(t.sku_info),<span class="number">1</span>) = <span class="string">'&#125;'</span> <span class="keyword">then</span> t.sku_info</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">concat</span>(t.sku_info,<span class="string">'&#125;'</span>) <span class="keyword">end</span> <span class="keyword">as</span> sku_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,sku_info <span class="keyword">from</span> sku_info_tbl </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="keyword">substr</span>(sku_infos,<span class="number">2</span>,<span class="keyword">length</span>(sku_infos)<span class="number">-2</span>),<span class="string">'&#125;,'</span>)) subview <span class="keyword">as</span> sku_info</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">) t</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">) m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> json_tuple(m.sku_info,<span class="string">'skuId'</span>,<span class="string">'num'</span>,<span class="string">'price'</span>,<span class="string">'jd_price'</span>,<span class="string">'sale_price'</span>) subview </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">as</span> skuid,<span class="keyword">num</span>,price,jd_price,sale_price;</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/hive/hive-json-3.png" alt="拆解完成"></p><p>当然也可以将每一步生成的结果表注册成视图，简化和拆解SQL语句的书写。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Array/Map/Struct </tag>
            
            <tag> json_tuple </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive静态分区与动态分区的业务场景应用</title>
      <link href="/2018/04/23/hive/hive-partition/"/>
      <url>/2018/04/23/hive/hive-partition/</url>
      
        <content type="html"><![CDATA[<h3 id="静态分区的应用"><a href="#静态分区的应用" class="headerlink" title="静态分区的应用"></a>静态分区的应用</h3><p>业务场景：</p><p>统计每天的销售额，日期是确定的，需要把每天统计好的销售额数据插入到指定的日期分区中。</p><p>hive脚本：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive -e "</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hive job任务队列</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.queue.name=pms;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建销售额数据汇总表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> pms.rpt_rcmd_gmv(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">page_name <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">section_name <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">order_count <span class="built_in">bigint</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">order_amount <span class="keyword">double</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span>(ds <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;"</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">date=`date +"%Y-%m-%d"`</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">hive -e "</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hive job任务队列</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.queue.name=pms;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 将数据插入到指定的日期分区中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> pms.rpt_rcmd_gmv <span class="keyword">partition</span>(ds=<span class="string">'$date'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">page_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">section_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">order_count,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">order_amount</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xxx;"</span></pre></td></tr></table></figure><h3 id="动态分区的应用"><a href="#动态分区的应用" class="headerlink" title="动态分区的应用"></a>动态分区的应用</h3><p>业务场景：</p><p>一个二级类目对应着多个产品，现在想要按二级类目进行分区，问题在于输入数据中包含不止一个二级类目，所以分区的值是非确定的，需要根据输入数据来确定。</p><p>参数设置：</p><ol><li><code>set hive.exec.dynamic.partition=true;</code>（默认true，表示开启动态分区）</li><li><code>set hive.exec.dynamic.partition.mode=nonstrict;</code>（strict表示至少需要指定一个静态分区，nonstrict表示不需要指定静态分区）</li><li>动态分区的字段，需要写在<code>select</code>语句中所有字段的最后</li></ol><p>hive脚本：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建立hive分区表，以二级类目id作为分区，字段之间以\t分隔，行之间以\n分隔</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> exist pms.dynamic_groupon;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> pms.dynamic_groupon(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    product_id <span class="built_in">bigint</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    area_id <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">type</span> <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    original_price <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    price <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    start_time <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    end_time <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    groupon_num <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    ds <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (category_lvl2_id <span class="built_in">bigint</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 根据查询的结果，动态将数据插入到对应的二级类目分区中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max=<span class="number">32</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">32</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.name=[HQL]dynamic_groupon;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> pms.dynamic_groupon <span class="keyword">partition</span>(category_lvl2_id)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.productid <span class="keyword">as</span> <span class="built_in">bigint</span>) <span class="keyword">as</span> product_id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">  a.areaid <span class="keyword">as</span> area_id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.type <span class="keyword">as</span> <span class="built_in">int</span>) <span class="keyword">as</span> <span class="keyword">type</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.originalprice <span class="keyword">as</span> <span class="keyword">double</span>) <span class="keyword">as</span> original_price,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.price <span class="keyword">as</span> <span class="keyword">double</span>) <span class="keyword">as</span> price,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">  a.start_time,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">  a.end_time,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.grouponnum <span class="keyword">as</span> <span class="built_in">int</span>) <span class="keyword">as</span> groupon_num,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">  a.ds,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">  c.categ_lvl2_id <span class="keyword">as</span> category_lvl2_id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pms.pms_all_groupon_items a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> product b <span class="keyword">on</span> (<span class="keyword">cast</span>(a.productid <span class="keyword">as</span> <span class="built_in">bigint</span>) = b.id)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">select</span> <span class="keyword">distinct</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">    categ_lvl_id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">    categ_lvl2_id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">from</span> dw.hier_categ_by_orig</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">) c <span class="keyword">on</span> (b.category_id = c.categ_lvl_id)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> a.ds=<span class="string">'2018-04-06'</span>;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分区表 </tag>
            
            <tag> 业务场景 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop的常规使用</title>
      <link href="/2018/04/22/dw/sqoop-1/"/>
      <url>/2018/04/22/dw/sqoop-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Sqoop概述"><a href="#Sqoop概述" class="headerlink" title="Sqoop概述"></a>Sqoop概述</h3><p>Apache Sqoop(TM)是一种旨在有效地在Apache Hadoop和诸如关系数据库等结构化数据存储之间传输大量数据的工具。</p><p>Sqoop于2012年3月孵化出来，现在是一个顶级的Apache项目。</p><p>请注意，1.99.7与1.4.6不兼容，且没有特征不完整，它并不打算用于生产部署。</p><p>Sqoop原理：</p><p>将导入或导出命令翻译成MapReduce程序来实现。</p><p>在翻译出的MapReduce中主要是对inputformat和outputformat进行定制。</p><h3 id="Sqoop部署"><a href="#Sqoop部署" class="headerlink" title="Sqoop部署"></a>Sqoop部署</h3><p>下载（这里选择CDH5.16.2）：</p><p><code>wget http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz</code></p><p>解压：</p><p><code>tar -zxvf sqoop-1.4.6-cdh5.16.2.tar.gz -C ~/app/</code></p><p>设置软连接：</p><p><code>ln -s sqoop-1.4.6-cdh5.16.2.tar.gz sqoop</code></p><p>配置环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加以下内容</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SQOOP_HOME=/home/hadoop/app/sqoop-1.4.6-cdh5.16.2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$SQOOP_HOME</span>/bin:<span class="variable">$PATH</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存退出后source，使环境变量生效</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看环境变量是否生效</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">which</span> sqoop</span></pre></td></tr></table></figure><p>修改配置文件：</p><p><code>cp sqoop-env-template.sh sqoop-env.sh</code></p><p><code>vi sqoop-env.sh</code></p><p>修改以下内容：</p><p><code>export HADOOP_COMMON_HOME=/home/hadoop/app/hadoop</code></p><p><code>export HADOOP_MAPRED_HOME=/home/hadoop/app/hadoop</code></p><p><code>export HIVE_HOME=/home/hadoop/app/hive</code></p><p>添加驱动包：</p><p><code>cp mysql-connector-java-5.1.27-bin.jar $SQOOP_HOME/lib/</code></p><p>至此，Sqoop部署完成，可以使用<code>sqoop help</code>来验证配置是否正确。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ sqoop <span class="built_in">help</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Warning信息...</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Available commands:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  codegen            Generate code to interact with database records</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  create-hive-table  Import a table definition into Hive</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="built_in">eval</span>               Evaluate a SQL statement and display the results</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  <span class="built_in">export</span>             Export an HDFS directory to a database table</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="built_in">help</span>               List available commands</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  import             Import a table from a database to HDFS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  import-all-tables  Import tables from a database to HDFS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  job                Work with saved <span class="built_in">jobs</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  list-databases     List available databases on a server</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">  list-tables        List available tables <span class="keyword">in</span> a database</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">  merge              Merge results of incremental imports</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  metastore          Run a standalone Sqoop metastore</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">  version            Display version information</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">See <span class="string">'sqoop help COMMAND'</span> <span class="keyword">for</span> information on a specific <span class="built_in">command</span>.</span></pre></td></tr></table></figure><h3 id="查看数据库和表"><a href="#查看数据库和表" class="headerlink" title="查看数据库和表"></a>查看数据库和表</h3><p>查看数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop list-databases \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 出现如下数据库信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">information_schema</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">metadata_hive</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">performance_schema</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">test</span></span></pre></td></tr></table></figure><p>查看test数据库中的所有表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop list-tables \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/<span class="built_in">test</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password ruozedata</span></pre></td></tr></table></figure><h3 id="Sqoop导入数据"><a href="#Sqoop导入数据" class="headerlink" title="Sqoop导入数据"></a>Sqoop导入数据</h3><p>在Sqoop中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做导入，即使用import关键字。</p><h4 id="RDBMS到HDFS"><a href="#RDBMS到HDFS" class="headerlink" title="RDBMS到HDFS"></a>RDBMS到HDFS</h4><p>在MySQL中新建一张表并插入一些数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> company;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> company.staff(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) primary <span class="keyword">key</span> <span class="keyword">not</span> <span class="literal">null</span> auto_increment, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Thomas'</span>, <span class="string">'Male'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Catalina'</span>, <span class="string">'FeMale'</span>);</span></pre></td></tr></table></figure><p>全部导入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--table staff \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span></span></pre></td></tr></table></figure><p>查看HDFS上的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -ls /user/company/staff1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Found 2 items</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--   1 vinx supergroup          0 2017-12-27 17:06 /user/company/staff1/_SUCCESS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--   1 vinx supergroup         32 2017-12-27 17:06 /user/company/staff1/part-m-00000</span></pre></td></tr></table></figure><p>查询导入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff2 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--query <span class="string">'select name,sex from staff where id=2 and $CONDITIONS;'</span></span></pre></td></tr></table></figure><p>导入指定列：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff2 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--columns id,name \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">--table staff</span></pre></td></tr></table></figure><p>使用sqoop关键字筛选查询导入数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff2 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--table staff \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">--<span class="built_in">where</span> <span class="string">"id=2"</span></span></pre></td></tr></table></figure><h4 id="RDBMS到Hive"><a href="#RDBMS到Hive" class="headerlink" title="RDBMS到Hive"></a>RDBMS到Hive</h4><p>过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库。</p><p>从MySQL到Hive，本质是从MySQL =&gt; HDFS =&gt; load to Hive</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/<span class="built_in">test</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--table student \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--hive-import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">--hive-overwrite \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">--hive-table student_hive</span></pre></td></tr></table></figure><h3 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h3><p>在Sqoop中，导出是指：从大数据集群（HDFS/Hive/HBase）向非大数据集群（RDBMS）中传输数据，即使用export关键字。</p><h4 id="HIVE-HDFS到RDBMS"><a href="#HIVE-HDFS到RDBMS" class="headerlink" title="HIVE/HDFS到RDBMS"></a>HIVE/HDFS到RDBMS</h4><p>MySQL中创建表：</p><p><code>create table people(id int,name varchar(5));</code></p><p>导出数据到MySQL：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop <span class="built_in">export</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://bigdata111:3306/<span class="built_in">test</span> characterEncoding=utf-8\</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--<span class="built_in">export</span>-dir /user/hive/warehouse/student_hive \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--table student1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--input-fields-terminated-by <span class="string">"\t"</span></span></pre></td></tr></table></figure><p>MySQL中如果表不存在，不会自动创建，需要手动先创建相应的表。</p><p>重复往MySQL的同一个表导出数据，MySQL的表不能设置主键和自增。</p><p>数据以追加的方式导出到表中。</p><h3 id="脚本打包"><a href="#脚本打包" class="headerlink" title="脚本打包"></a>脚本打包</h3><p>使用opt格式的文件打包sqoop命令，然后执行。</p><ol><li><p>创建一个.opt文件</p><p><code>touch job_HDFS2MySQL.opt</code></p></li><li><p>编写sqoop脚本</p><p><code>vi ./job_HDFS2MySQL.opt</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从student_hive中追加导入到mysql的student表中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--connect</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">jdbc:mysql://hadoop:3306/<span class="built_in">test</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--username</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">root</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--password</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">123456</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--table</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">student</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">--num-mappers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">--<span class="built_in">export</span>-dir</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse111/student_hive</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">--input-fields-terminated-by</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="string">"\t"</span></span></pre></td></tr></table></figure></li><li><p>执行该脚本</p><p><code>sqoop --options-file job_HDFS2MySQL.opt</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HiveServer2和Beeline&amp;JOIN&amp;GROUP BY&amp;排序</title>
      <link href="/2018/04/21/dw/hive-3/"/>
      <url>/2018/04/21/dw/hive-3/</url>
      
        <content type="html"><![CDATA[<h3 id="HiveServer2和Beeline"><a href="#HiveServer2和Beeline" class="headerlink" title="HiveServer2和Beeline"></a>HiveServer2和Beeline</h3><p>当我们启动使用<code>hive</code>命令启动Hive时，会出现下面的信息：</p><p>WARNING: Hive CLI is deprecated and migration to Beeline is recommended.</p><p>警告中说：Hive CLI已经被标记为是过时的，并且推荐使用Beeline。</p><p>HS2和beeline的角色：</p><ul><li><p>HiveServer2: Server 默认端口是10000，可以指定端口</p></li><li><p>Beeline: Client</p></li></ul><h4 id="默认端口启动"><a href="#默认端口启动" class="headerlink" title="默认端口启动"></a>默认端口启动</h4><p>提示：需要先配置HIVE_HOME环境变量。</p><p>先启动HS2: </p><p><code>hiveserver2</code></p><p>然后启动Beeline: </p><p><code>beeline -u jdbc:hive2://hadoop001:10000/ -n vinx</code></p><p>退出Beeline: </p><p><code>!q</code></p><h4 id="指定端口启动"><a href="#指定端口启动" class="headerlink" title="指定端口启动"></a>指定端口启动</h4><p>启动HS2: </p><p><code>hiveserver2 --hiveconf hive.server2.thrift.port=10086</code></p><p>启动beeline: </p><p><code>beeline -u jdbc:hive2://hadoop001:10086/ -n vinx</code></p><h3 id="JOIN语句"><a href="#JOIN语句" class="headerlink" title="JOIN语句"></a>JOIN语句</h3><h4 id="常用JOIN"><a href="#常用JOIN" class="headerlink" title="常用JOIN"></a>常用JOIN</h4><p>内连接：只要进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename,d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dept = d.deptno;</span></pre></td></tr></table></figure><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span></pre></td></tr></table></figure><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span></pre></td></tr></table></figure><p>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字符没有符合条件的值的话，那么就使用NULL值替代。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span></pre></td></tr></table></figure><h4 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h4><p>连接n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p><p>创建表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> location(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    loc <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    loc_name <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/location.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> location;</span></pre></td></tr></table></figure><p>多表连接查询：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,d.deptno,l.loc_name</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> dept d <span class="keyword">on</span> d.deptno=e.deptno</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> location l <span class="keyword">on</span> d.loc=l.loc;</span></pre></td></tr></table></figure><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。Hive总是按照从左到右的顺序执行连接操作的。</p><h4 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h4><p>笛卡尔积会在下面的条件下产生：</p><ol><li>省略了连接条件</li><li>连接条件无效</li><li>所有表中的所有行互相连接</li></ol><p>例如：</p><p>hive (default)&gt; <code>select empno, deptno from emp, dept;</code></p><p>FAILED: SemanticException Column deptno Found in more than One Tables/Subqueries</p><h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><h4 id="GROUP-BY语句"><a href="#GROUP-BY语句" class="headerlink" title="GROUP BY语句"></a>GROUP BY语句</h4><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个队列结果进行分组，然后对每个组执行聚合操作。</p><p>计算emp表每个部门的平均工资：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.deptno,<span class="keyword">avg</span>(e.sal) avg_sal <span class="keyword">from</span> emp e <span class="keyword">group</span> <span class="keyword">by</span> e.deptno;</span></pre></td></tr></table></figure><p>计算emp每个部门中每个岗位的最高薪水：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.deptno,e.job,<span class="keyword">max</span>(e.sal) max_sal <span class="keyword">from</span> emp e <span class="keyword">group</span> <span class="keyword">by</span> e.deptno,e.job;</span></pre></td></tr></table></figure><h4 id="HAVING语句"><a href="#HAVING语句" class="headerlink" title="HAVING语句"></a>HAVING语句</h4><p>having与where的不同点：</p><ol><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据</li><li>where后面不能写分组函数，而having后面可以使用分组函数</li><li>having只用于group by分组统计语句</li></ol><p>求每个部门的平均工资大于2000的部门：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno,<span class="keyword">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal &gt; <span class="number">2000</span>;</span></pre></td></tr></table></figure><h4 id="TopN实现"><a href="#TopN实现" class="headerlink" title="TopN实现"></a>TopN实现</h4><p>需求：求每个部门每个职业的薪水和最高的2个职位。</p><p>创建视图sal，保存每个部门每个职业的薪水和：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> sal <span class="keyword">as</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno,job,<span class="keyword">sum</span>(sal) sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno,job;</span></pre></td></tr></table></figure><p>求出每个职业的top2：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  a.*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sal a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> sal b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">where</span> a.deptno=b.deptno <span class="keyword">and</span> a.sal &lt; b.sal</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">) = <span class="number">1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 求top1可以将1改为0</span></span></pre></td></tr></table></figure><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><h4 id="ORDER-BY全局排序"><a href="#ORDER-BY全局排序" class="headerlink" title="ORDER BY全局排序"></a>ORDER BY全局排序</h4><p>ORDER BY: 全局排序，一个MapReduce，用在SELECT语句的结尾。</p><p>查询员工信息，并按工资升序排序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal;</span></pre></td></tr></table></figure><p>查询员工信息，并按工资降序排序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><p>按照部门和工资升序排序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ename,deptno,sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno,sal;</span></pre></td></tr></table></figure><h4 id="SORT-BY内部排序"><a href="#SORT-BY内部排序" class="headerlink" title="SORT BY内部排序"></a>SORT BY内部排序</h4><p>SORT BY: 每个MapReduce内部进行排序，分区规则按照key的hash来运算，（区内排序）对全局结果集来说不是排序。</p><p>设置reduce个数：</p><p><code>set mapreduce.job.reduces=3;</code></p><p>查看设置reduce个数：</p><p><code>set mapreduce.job.reduces;</code> </p><p>查看员工信息，并根据部门编号降序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><p>将查询结果导入到文件中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/vinx/data/output/emp.txt'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><h4 id="DISTRIBUTE-BY分区排序"><a href="#DISTRIBUTE-BY分区排序" class="headerlink" title="DISTRIBUTE BY分区排序"></a>DISTRIBUTE BY分区排序</h4><p>DISTRIBUTE BY: 类似MR中partition，进行分区，结合sort by使用。DISTRIBUTE BY语句要写在SORT BY语句之前。</p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>设置reduce个数：</p><p><code>set mapreduce.job.reduces=3;</code></p><p>先按照部门编号分区，再按照员工编号降序排序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/vinx/data/distribute-result'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><h4 id="CLUSTER-BY"><a href="#CLUSTER-BY" class="headerlink" title="CLUSTER BY"></a>CLUSTER BY</h4><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。</p><p>以下两种写法等价：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/vinx/data/cluster-result'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span></pre></td></tr></table></figure><p>按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HiveServer2/Beeline </tag>
            
            <tag> JOIN </tag>
            
            <tag> GROUP BY </tag>
            
            <tag> 排序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的UDF/UDAF/UDTF</title>
      <link href="/2018/04/20/hive/hive-function/"/>
      <url>/2018/04/20/hive/hive-function/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive的内置函数"><a href="#Hive的内置函数" class="headerlink" title="Hive的内置函数"></a>Hive的内置函数</h3><p>内置函数：</p><ul><li><code>show functions;</code> - 查看内置函数</li><li><code>desc function upper;</code> - 查看upper的用法</li><li><code>desc function extended upper;</code> - 查看upper的详细用法（带示例）</li></ul><p>常用内置函数：</p><ul><li><p>日期函数</p><p>date_add, datediff, to_date, from_unixtime, unix_timestamp</p></li><li><p>字符串函数</p><p>substr, concat, concat_ws, split, regexp_replace, get_json_object, trim, length</p></li><li><p>聚合函数</p><p>abs, ceil, floor ,round, rand, pow</p></li><li><p>数学函数</p><p>count, max, min, avg, count distinct, sum, group_concat</p></li><li><p>窗口函数</p><p>row_number, lead, lag, rank</p></li><li><p>其他函数</p><p>coalesce, cast, decode explode</p></li></ul><h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><p>用户自定义函数分为以下3种：</p><ol><li><p>UDF(User-Defined-Function)</p><p>一进一出</p></li><li><p>UDAF(User-Defined Aggregation Function)</p><p>聚合函数，多进一出</p><p>类似于：count/max/min</p></li><li><p>UDTF(User-Defined Table-Generating Funcion)</p><p>一进多出</p><p>如：lateral view explode()</p></li></ol><p>编程步骤：</p><ol><li><p>继承<code>org.apache.hadoop.hive.sql.UDF</code></p></li><li><p>实现<code>evaluate</code>函数，<code>evaluate</code>函数支持重载</p></li><li><p>在hive的命令行窗口创建函数</p><ol><li><p>添加jar</p><p><code>add jar linux_jar_path</code></p></li><li><p>创建function</p><p><code>create [temporary] function [dbname.]function_name AS class_name;</code></p></li></ol></li><li><p>在hive的命令行窗口删除函数</p><p><code>drop [temporary] function [if exists] [dbname.]function_name;</code></p></li></ol><p>UDF必须要有返回类型，可以返回null，但是返回类型不能为null。</p><h4 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h4><p>User-Defined-Function用户自定义函数，一进一出。</p><p>in:out=1:1，输入一条记录数据，同时返回一条处理结果。</p><p>类似于cos, sin, substring, indexof…</p><p>实现步骤（Java创建自定义UDF类）：</p><ul><li>自定义一个java类</li><li>继承UDF类</li><li>重写evaluate方法</li><li>打包类所在项目成一个all-in-one的jar包，并上传到hive所在机器</li><li>在hive中执行add jar操作，将jar加载到classpath中</li><li>在hive中创建模板函数，使得后边可以使用该函数名称调用实际的UDF函数</li><li>hive sql中像调用系统函数一样使用UDF函数</li></ul><p>代码实现：</p><ul><li>功能要求：实现当输入字符串超过2个字符的时候，多余的字符以”…”来表示。</li><li>如”12”则返回”12”，如”123”返回”12…”</li><li>自定义类、继承UDF、重写evaluate方法已在代码中体现</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 功能：实现当输入字符串超过2个字符的时候，多余的字符以"..."来表示。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 输入/输出：* 如“12”则返回“12”，如“123”返回“12..."</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ValueMaskUDF</span> <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">       <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String input,<span class="keyword">int</span> maxSaveStringLength,String replaceSign)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">             <span class="keyword">if</span>(input.length()&lt;=maxSaveStringLength)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">                    <span class="keyword">return</span> input;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">             &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">             <span class="keyword">return</span> input.substring(<span class="number">0</span>,maxSaveStringLength)+replaceSign;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">       &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">             System.out.println(<span class="keyword">new</span> ValueMaskUDF().evaluate(<span class="string">"河北省"</span>,<span class="number">2</span>,<span class="string">"..."</span>));;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">       &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h4 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h4><p>in:out=n:1，即接受N条记录数据，同时返回一条处理结果</p><p>类似于：count, sum, avg, max</p><p>实现步骤：</p><ul><li>自定义一个java类</li><li>继承UDAF类</li><li>内部定义一个静态类，实现UDAFEvaluator接口</li><li>实现方法init, iterate, terminatePartial, merge, terminate共5个方法</li><li>在hive中执行add jar操作，将jar加载到classpath中</li><li>在hive中创建模板函数，使得后边可以使用该函数名称调用实际的UDF函数</li><li>hive sql中像调用系统函数一样使用UDAF函数</li></ul><p><img src="https://vinxikk.github.io/img/hive/hive-udaf.png" alt="UDAF的实现步骤"></p><p>建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_score(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    course <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    score <span class="built_in">int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/student_score.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> student_score;</span></pre></td></tr></table></figure><p>查询数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; select * from student_score;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">student_score.idstudent_score.namestudent_score.coursestudent_score.score</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">001zhangsanJava90</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">001zhangsanPython80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">001zhangsanScala85</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">002lisiJava95</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">002lisiPython90</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">002lisiScala93</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">003wangwuJava90</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">003wangwuPython80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">003wangwuScala70</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">003wangwuEnglish80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">004aliceScala80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">Time taken: 2.482 seconds, Fetched: 11 row(s)</span></pre></td></tr></table></figure><p>UDAF代码实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Map;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Set;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDAF;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDAFEvaluator;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">* 实现多条数据合并成一条数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">*/</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 主类继承UDAF</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudentScoreAggUDAF</span> <span class="keyword">extends</span> <span class="title">UDAF</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// 日志对象初始化</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Logger logger = Logger.getLogger(StudentScoreAggUDAF<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// 静态类实现UDAFEvaluator</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Evaluator</span> <span class="keyword">implements</span> <span class="title">UDAFEvaluator</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">// 设置成员变量，存储每个统计范围内的总记录数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">private</span> Map&lt;String, String&gt; courseScoreMap;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//初始化函数,map和reduce均会执行该函数,起到初始化所需要的变量的作用</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Evaluator</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">            init();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">// 初始化函数间传递的中间变量</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">            courseScoreMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">         <span class="comment">//map阶段，返回值为boolean类型，当为true则程序继续执行，当为false则程序退出  </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">iterate</span><span class="params">(String course, String score)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> (course == <span class="keyword">null</span> || score == <span class="keyword">null</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">            courseScoreMap.put(course, score);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">         <span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"><span class="comment">         * 类似于combiner,在map范围内做部分聚合，将结果传给merge函数中的形参mapOutput  </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"><span class="comment">         * 如果需要聚合，则对iterator返回的结果处理，否则直接返回iterator的结果即可</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="comment">         */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> Map&lt;String, String&gt; <span class="title">terminatePartial</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> courseScoreMap;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">         <span class="comment">// reduce 阶段，用于逐个迭代处理map当中每个不同key对应的 terminatePartial的结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">merge</span><span class="params">(Map&lt;String, String&gt; mapOutput)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">this</span>.courseScoreMap.putAll(mapOutput);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">// 处理merge计算完成后的结果，即对merge完成后的结果做最后的业务处理</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">terminate</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> courseScoreMap.toString();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>添加jar包到Hive的classpath：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">add jar /home/vinx/jars/hiveudaf.jar;</span></pre></td></tr></table></figure><p>注册临时函数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> score_agg <span class="keyword">as</span> <span class="string">'com.ivinx.hiveudaf.StudentScoreAggUDAF'</span>;</span></pre></td></tr></table></figure><p>或者永久添加函数：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 永久注册，hive-site.xml --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.aux.jars.path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/vinx/jars/hive.jar<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>测试sql语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,score_agg(course,score) <span class="keyword">from</span> student_score <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>,<span class="keyword">name</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------------------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|  id  |   name    |                     _c2                     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------------------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 001  | zhangsan  | &#123;Java=90, Scala=85, Python=80&#125;              |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 002  | lisi      | &#123;Java=95, Scala=93, Python=90&#125;              |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 003  | wangwu    | &#123;Java=90, English=80, Scala=70, Python=80&#125;  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 004  | alice     | &#123;Scala=80&#125;                                  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------------------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4 rows selected (69.974 seconds)</span></pre></td></tr></table></figure><h3 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h3><p>User-Defined Table-Generating Functions</p><p>一行输入，多行输出</p><p>用udtf解决一行输入多行输出的不多，往往被lateral view explode+udf等替代实现，比直接用udtf会更简单、直接一些</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的调优</title>
      <link href="/2018/04/18/hive/hive-optimize/"/>
      <url>/2018/04/18/hive/hive-optimize/</url>
      
        <content type="html"><![CDATA[<h3 id="Fetch抓取"><a href="#Fetch抓取" class="headerlink" title="Fetch抓取"></a>Fetch抓取</h3><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：<code>select * from emp</code>，Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果。</p><p>在hive-default.xml.template文件中<code>hive.fetch.task.conversion</code>默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p><ol><li><p>把<code>hive.fetch.task.conversion</code>设置成none，然后执行查询语句，都会执行mapreduce程序。</p><p>hive (default)&gt; <code>set hive.fetch.task.conversion=none;</code></p><p>hive (default)&gt; <code>select * from emp;</code></p><p>hive (default)&gt; <code>select ename from emp;</code></p><p>hive (default)&gt; <code>select ename from emp limit 3;</code></p></li><li><p>把<code>hive.fetch.task.conversion</code>设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p><p>hive (default)&gt; <code>set hive.fetch.task.conversion=more;</code></p><p>hive (default)&gt; <code>select * from emp;</code></p><p>hive (default)&gt; <code>select ename from emp;</code></p><p>hive (default)&gt; <code>select ename from emp limit 3;</code></p></li></ol><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>大多数分Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多得多。对于大数据这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p><p>用户可以通过设置<code>hive.exec.mode.local.auto</code>的值为true，来让Hive在适当的时候自动启动这个优化。</p><p>开启本地mr：</p><p><code>set hive.exec.mode.local.auto=true;</code></p><p>设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M：</p><p><code>set hive.exec.mode.local.auto.inputbytes.max=50000000;</code></p><p>设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4：</p><p><code>set hive.exec.mode.local.auto.input.files.max=10;</code></p><p>案例：</p><ol><li><p>开启本地模式，并执行查询语句(注意重启Hive)</p><p>hive (default)&gt; <code>set hive.exec.mode.local.auto=true;</code><br>hive (default)&gt; <code>select * from emp cluster by deptno;</code><br>Time taken: 1.328 seconds, Fetched: 14 row(s)</p></li><li><p>关闭本地模式，并执行查询语句</p><p>hive (default)&gt; <code>set hive.exec.mode.local.auto=false;</code><br>hive (default)&gt; <code>select * from emp cluster by deptno;</code><br>Time taken: 20.09 seconds, Fetched: 14 row(s)</p></li></ol><h3 id="表的优化"><a href="#表的优化" class="headerlink" title="表的优化"></a>表的优化</h3><h4 id="小表、大表JOIN"><a href="#小表、大表JOIN" class="headerlink" title="小表、大表JOIN"></a>小表、大表JOIN</h4><p>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用group变小的维度表（1000条以下的记录条数）先进内存，在map端完成reduce（预聚合）。</p><p>新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</p><h4 id="大表JOIN大表"><a href="#大表JOIN大表" class="headerlink" title="大表JOIN大表"></a>大表JOIN大表</h4><ol><li><p>空key过滤</p><p>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。很多情况下，这些key对应的数据是异常数据，需要在SQL语句中进行过滤，例如key对应的字段为空。</p><p>过滤空id：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> nullidtable <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">) n</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> oritable o <span class="keyword">on</span> n.id=o.id;</span></pre></td></tr></table></figure></li><li><p>空key转换</p><p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以给表中key为空的字段附一个随机的值，使得数据随机均匀地分布到不同的reducer上。</p><p>设置5个reduce个数：</p><p><code>set mapreduce.job.reduces=5;</code></p><p>JOIN两张表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> oritable o</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> n.id=o.id;</span></pre></td></tr></table></figure><p>可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</p><p>随机分布空null值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">full</span> <span class="keyword">join</span> oritable o</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> <span class="keyword">case</span> <span class="keyword">when</span> n.id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">'hive'</span>,<span class="keyword">rand</span>()) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> n.id <span class="keyword">end</span> = o.id;</span></pre></td></tr></table></figure><p>随机分布null值可以一定程度消除数据倾斜，负载均衡reducer的资源消耗。</p></li></ol><h4 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h4><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join，容易发生数据倾斜。可以用MapJoin把小表全部加载进内存，在map端进行join，避免reducer处理。</p><p>开启MapJoin参数设置：</p><ol><li><p>设置自动选择MapJoin（默认为true）：</p><p><code>set hive.auto.convert.join=true;</code></p></li><li><p>大表小表的阈值设置（默认25M以下认为是小表）：</p><p><code>set hive.mapjoin.smalltable.filesize=25000000;</code></p><p>小表JOIN大表：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> smalltable s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> bigtable  b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> s.id = b.id;</span></pre></td></tr></table></figure><p>大表JOIN小表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bigtable  b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> smalltable  s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> s.id = b.id;</span></pre></td></tr></table></figure><h4 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h4><p>默认情况下，Map阶段同一个key的数据分发给一个reduce，当一个key数据过大时就会发生倾斜。</p><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><p>开启Map端聚合参数设置：</p><ol><li><p>是否在Map端进行聚合（默认为true）：</p><p><code>set hive.map.aggr=true</code></p></li><li><p>在Map端进行聚合操作的数目：</p><p><code>set hive.groupby.mapaggr.checkinterval=100000</code></p></li><li><p>有数据倾斜的时候进行负载均衡（默认是false）：</p><p><code>set hive.groupby.skewindata=true</code></p></li></ol><p>当选项设定为true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><h4 id="Count-Distinct-去重统计"><a href="#Count-Distinct-去重统计" class="headerlink" title="Count(Distinct)去重统计"></a>Count(Distinct)去重统计</h4><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换。</p><p>创建一张大表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/bigtable'</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span></pre></td></tr></table></figure><p>设置5个reduce个数：</p><p><code>set mapreduce.job.reduces=5;</code></p><p>执行去重id查询（此时只有一个Stage）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> <span class="keyword">id</span>) <span class="keyword">from</span> bigtable;</span></pre></td></tr></table></figure><p>采用group by去重id（有两个Stage，其中第一个Stage会启动5个Reduce）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(a.id) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) a;</span></pre></td></tr></table></figure><h4 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h4><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p><h4 id="行列过滤"><a href="#行列过滤" class="headerlink" title="行列过滤"></a>行列过滤</h4><p>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用<code>SELECT *</code>。</p><p>行处理：在分区裁剪中，当使用外关联时，如果将副表的过滤条件写在where后面，那么就会先全表关联，之后再过滤，总而言之，就是先where还是先join的执行顺序的问题。以下两种，经过SQL优化器，执行效果大体一样：</p><ol><li><p>先关联两张表，再用where条件过滤：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> oritable o</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> o.id = b.id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> o.id &lt;=<span class="number">10</span>;</span></pre></td></tr></table></figure></li><li><p>通过子查询后，再关联表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> oritable <span class="keyword">where</span> <span class="keyword">id</span>&lt;= <span class="number">10</span>) o</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> b.id=o.id;</span></pre></td></tr></table></figure></li></ol><h4 id="动态分区调整"><a href="#动态分区调整" class="headerlink" title="动态分区调整"></a>动态分区调整</h4><p>关系型数据库中，对分区表INSERT数据时，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区（Dynamic Partition）。</p><p>开启动态分区参数设置：</p><ol><li><p>开启动态分区功能（默认true）：</p><p><code>set hive.exec.dynamic.partition=true</code></p></li><li><p>设置为非严格模式（默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区）：</p><p><code>set hive.exec.dynamic.partition.mode=nonstrict</code></p></li><li><p>在所有执行MR的节点上，最大一共可以创建多少个动态分区（默认1000）：</p><p><code>set hive.exec.max.dynamic.partitions=1000</code></p></li><li><p>在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p><p><code>set hive.exec.max.dynamic.partitions.pernode=100</code></p></li><li><p>整个MR Job中，最大可以创建多少个HDFS文件（默认值100000）：</p><p><code>set hive.exec.max.created.files=100000</code></p></li><li><p>当有空分区生成时，是否抛出异常（默认false，一般不需要设置）：</p><p><code>set hive.error.on.empty.partition=false</code></p></li></ol><p>案例需求：将ori中的数据按照时间(如：20111230000008)，插入到目标表ori_partitioned_target的相应分区中。</p><ol><li><p>创建分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partitioned(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (p_time <span class="built_in">bigint</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure></li><li><p>加载数据到分区表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/datas/ds1'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">into</span> <span class="keyword">table</span> ori_partitioned </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">partition</span>(p_time=<span class="string">'20111230000010'</span>) ;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/datas/ds2'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">into</span> <span class="keyword">table</span> ori_partitioned </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">partition</span>(p_time=<span class="string">'20111230000011'</span>) ;</span></pre></td></tr></table></figure></li><li><p>创建目标分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partitioned_target(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (p_time <span class="keyword">string</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure></li><li><p>设置动态分区</p><p><code>set hive.exec.dynamic.partition = true;</code><br><code>set hive.exec.dynamic.partition.mode = nonstrict;</code><br><code>set hive.exec.max.dynamic.partitions = 1000;</code><br><code>set hive.exec.max.dynamic.partitions.pernode = 100;</code><br><code>set hive.exec.max.created.files = 100000;</code><br><code>set hive.error.on.empty.partition = false;</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> ori_partitioned_target </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">partition</span> (p_time) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span>, <span class="built_in">time</span>, uid, keyword, url_rank, click_num, click_url, p_time </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ori_partitioned;</span></pre></td></tr></table></figure></li><li><p>查看目标分区表的分区情况</p><p><code>show partitions ori_partitioned_target;</code></p></li><li><p>如果不设置非严格模式，报错如下：</p><p>FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict</p></li></ol><h4 id="sort-by代替order-by"><a href="#sort-by代替order-by" class="headerlink" title="sort by代替order by"></a>sort by代替order by</h4><p>order by就是将结果按某字段全局排序，这会导致所有map端数据都进入一个reducer中，数据量大时导致性能下降。</p><p>sort by会视情况启动多个reducer排序，并且保证每个reducer内局部有序。为了控制map端数据分配到reducer的key，往往还要配合distribute by一同使用。如果不加distribute by的话，map端数据就会随机分配到reducer。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 以uid为key，以upload_time倒序、event_type倒序 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> uid,upload_time,event_type,recore_data </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> recore_log </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> rl_date &gt;= <span class="number">20190405</span> <span class="keyword">and</span> rl_date &lt;= <span class="number">20190410</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">distribute</span> <span class="keyword">by</span> uid </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">sort</span> <span class="keyword">by</span> upload_time <span class="keyword">desc</span>,event_type <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><h4 id="group-by代替distinct"><a href="#group-by代替distinct" class="headerlink" title="group by代替distinct"></a>group by代替distinct</h4><p>当要统计某一列的去重数时，如果数据量很大，count(distinct)就会非常慢，原因与order by类似，count(distinct)只会有很少的reducer来处理。这时可以用group by来改写：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span> uid <span class="keyword">from</span> recore_log </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">where</span> rl_date &gt;= <span class="number">20180405</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> uid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">) t;</span></pre></td></tr></table></figure><p>但是这样写会启动两个MR job（单纯distinct只会启动一个），所以要确保数据量大到启动job的overhead远小于计算耗时，才考虑这种方法。当数据集很小或者key的倾斜比较明显时，group by可能比distinct还慢。</p><p>使用group by同时统计多列：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.a,<span class="keyword">sum</span>(t.b),<span class="keyword">count</span>(t.c),<span class="keyword">count</span>(t.d) <span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> <span class="keyword">select</span> a,b,<span class="literal">null</span> c,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">) t;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive窗口函数</title>
      <link href="/2018/04/17/hive/hive-window-function/"/>
      <url>/2018/04/17/hive/hive-window-function/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive窗口函数"><a href="#Hive窗口函数" class="headerlink" title="Hive窗口函数"></a>Hive窗口函数</h3><p>窗口函数可以计算累积和、移动平均值等，可以结合聚合函数sum(), avg()等使用，也可以结合first_value()和last_value()，返回窗口的第一个和最后一个值。</p><p>如果只使用partition by子句，未指定order by的话，聚合是分组内的聚合。</p><p>使用了order by子句，未使用window子句的情况下，默认从起点到当前行。</p><p>window子句：</p><ul><li>preceding: 往前</li><li>following: 往后</li><li>current row: 当前行</li><li>unbounded: 界限，UNBOUNDED PRECEDING表示往前无界，UNBOUNDED FOLLOWING表示往后无界</li></ul><h4 id="计算累积和"><a href="#计算累积和" class="headerlink" title="计算累积和"></a>计算累积和</h4><ol><li><p>计算所有月份的累积和：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法1：默认就是UNBOUNDED PRECEDING到CURRENT ROW，可以不写</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li><li><p>计算前3个月和本月共4个月的累积和：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法1：从前3条记录到本条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法2：默认下界就是CURRENT ROW，可以不写</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="number">3</span> <span class="keyword">PRECEDING</span>) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li><li><p>计算前1月后1月和本月，共3个月的累积和：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span>) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li></ol><h4 id="计算平均值"><a href="#计算平均值" class="headerlink" title="计算平均值"></a>计算平均值</h4><ol><li><p>计算前1月后1月和本月，共3个月总值的平均值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_month,<span class="keyword">avg</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span>) average_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li><li><p>计算前3个月和本月，共4个月总值的平均值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_month,<span class="keyword">avg</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) average_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li></ol><h4 id="计算首尾值"><a href="#计算首尾值" class="headerlink" title="计算首尾值"></a>计算首尾值</h4><ol><li><p>计算窗体第一条和最后一条的值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">first_value</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span>) first_amount,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">last_value</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span>) last_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive窗口函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的管理表和外部表&amp;分区表&amp;分桶表&amp;行转列和列转行</title>
      <link href="/2018/04/15/dw/hive-2/"/>
      <url>/2018/04/15/dw/hive-2/</url>
      
        <content type="html"><![CDATA[<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><p>建表语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[<span class="keyword">COMMENT</span> table_comment] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">[LOCATION hdfs_path]</span></pre></td></tr></table></figure><p>字段说明：</p><ol><li><p>CREATE TABLE创建一个指定名字的表，如果表已经存在，则抛出异常；用户可以用IF NOT EXISTS选项来忽略这个异常。</p></li><li><p>EXTERNAL关键字可以创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）。Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p></li><li><p>COMMENT: 为表和列添加注释。</p></li><li><p>PARTITIONED BY: 创建分区表</p></li><li><p>CLUSTERED BY: 创建分桶表</p></li><li><p>ROW FORMAT</p><p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p><p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe，如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。建表时用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p></li><li><p>STORED AS: 指定存储文件类型</p><p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）。</p><p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p></li><li><p>LOCATION: 指定表在HDFS上的存储位置。</p></li><li><p>LIKE允许用户复制现有的表结构，但是不复制数据。</p></li></ol><h3 id="内部表和外部表"><a href="#内部表和外部表" class="headerlink" title="内部表和外部表"></a>内部表和外部表</h3><p>内部表（管理表）：删除表时，元数据（META）删除，业务数据（HDFS）也删除。</p><p>外部表：删除表时，元数据删除，HDFS中业务数据不删除。</p><p>管理表和外部表的使用场景：</p><p>每天将收集到的网站日志定期流入HDFS文本文件，在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p><h4 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h4><p>默认创建的表都是所谓的管理表，也被称为内部表，因为这种表，Hive会（或多或少）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在<code>hive.metastore.warehouse.dir</code>(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive也会删除这个表中的数据。管理表不适合和其他工具共享数据。</p><p>创建管理表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">empno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">ename <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">job <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">mgr <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">hiredate <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">sal <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">comm <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">deptno <span class="built_in">int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 从Linux本地加载</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 从HDFS加载</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/data/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span></pre></td></tr></table></figure><p>查看表结构：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 简单查看</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">desc emp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 格式化查看详细信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">desc formatted emp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Table Type:             MANAGED_TABLE</span></pre></td></tr></table></figure><h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4><p>因为是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p><p>创建外部表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建外部表，并指定在HDFS中存储的路径</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp_external(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">empno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ename <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">job <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">mgr <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">hiredate <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">sal <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">comm <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">deptno <span class="built_in">int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">location <span class="string">'/hive_data/hive_external_table/emp/'</span>;</span></pre></td></tr></table></figure><p>使用HDFS命令加载表数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hdfs dfs -put emp.txt /hive_data/hive_external_table/emp/</span></pre></td></tr></table></figure><p>查看创建的表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show tables;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">tab_name</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">emp</span></pre></td></tr></table></figure><p>查询表数据：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span></pre></td></tr></table></figure><p>查看格式化表结构：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Table Type:             EXTERNAL_TABLE</span></pre></td></tr></table></figure><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>引入分区表（需要根据日期对日志进行管理）：</p><p><code>/user/hive/warehouse/log_partition/20180402/20180402.log</code></p><p><code>/user/hive/warehouse/log_partition/20180403/20180403.log</code></p><p><code>/user/hive/warehouse/log_partition/20180404/20180404.log</code></p><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>创建分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_partition(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">lid <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">lrecord <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ltime <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据到分区表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/logs/log.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> log_partition <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201804'</span>);</span></pre></td></tr></table></figure><p>查询分区表中的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 单分区查询</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201804'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 多分区查询</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201803'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201804'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201805'</span></span></pre></td></tr></table></figure><p>增加分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建单个分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> log_partiton <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201806'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建多个分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> log_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201806'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201807'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 增加多个分区之间用空格" "隔开，删除多个分区用","隔开</span></span></pre></td></tr></table></figure><p>删除分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除单个分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> log_partition <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201804'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除多个分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> log_partition <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201803'</span>),<span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201805'</span>);</span></pre></td></tr></table></figure><p>查看分区表有多少分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> log_partition;</span></pre></td></tr></table></figure><p>查看分区表结构：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">desc formatted log_partition;</span></pre></td></tr></table></figure><h4 id="多级分区表"><a href="#多级分区表" class="headerlink" title="多级分区表"></a>多级分区表</h4><p>创建二级分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_partition2(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">lid <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">lrecord <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ltime <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据到二级分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/logs/log.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> log_partition2 <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201804'</span>, <span class="keyword">day</span>=<span class="string">'13'</span>);</span></pre></td></tr></table></figure><p>查询分区数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition2 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201804'</span> <span class="keyword">and</span> <span class="keyword">day</span>=<span class="string">'13'</span>;</span></pre></td></tr></table></figure><h4 id="数据关联和修复"><a href="#数据关联和修复" class="headerlink" title="数据关联和修复"></a>数据关联和修复</h4><p>把数据上传到分区目录上，让分区表和数据产生关联的三种方式：</p><ol><li><p>上传数据后修复</p><p>上传数据：</p><p>hive (hive_2)&gt; <code>dfs -mkdir -p /user/hive/warehouse/log_partition2/month=201804/day=13;</code></p><p>hive (hive_2)&gt; <code>dfs -put /home/vinx/logs/log.txt /user/hive/warehouse/log_partition2/month=201804/day=13;</code></p><p>查询数据（查询不到刚才上传的数据）：</p><p>hive (hive_2)&gt; <code>select * from log_partition2 where month=&#39;201804&#39; and day=&#39;13&#39;;</code></p><p>执行修复命令：</p><p>hive (hive_2)&gt; <code>msck repair table log_partition2;</code></p><p>再次查询数据：</p><p>hive (hive_2)&gt; <code>select * from log_partition2 where month=&#39;201804&#39; and day=&#39;13&#39;;</code></p></li><li><p>上传数据后添加分区</p><p>上传数据：</p><p>hive (hive_2)&gt; <code>dfs -mkdir -p /user/hive/warehouse/log_partition2/month=201804/day=14;</code></p><p>hive (hive_2)&gt; <code>dfs -put /home/vinx/logs/log.txt /user/hive/warehouse/log_partition2/month=201804/day=14;</code></p><p>添加分区：</p><p>hive (hive_2)&gt; <code>alter table log_partition2 add partition(month=&#39;201804&#39;, day=&#39;14&#39;);</code></p><p>查询数据：</p><p>hive (hive_2)&gt; <code>select * from log_partition2 where month=&#39;201804&#39; and day=&#39;14&#39;;</code></p></li><li><p>上传数据后load数据到分区：</p><p>创建目录：</p><p>hive (hive_2)&gt; <code>dfs -mkdir -p /user/hive/warehouse/log_partition2/month=201804/day=15;</code></p><p>上传数据：</p><p>hive (hive_2)&gt; <code>load data local inpath &#39;/home/vinx/logs/log.txt&#39; into table log_partition2 partition(month=&#39;201804&#39;,day=&#39;15&#39;);</code></p><p>查询数据：</p><p>hive (hive_2)&gt; <code>select * from log_partition2 where month=&#39;201804&#39; and day=&#39;15&#39;;</code></p></li></ol><h3 id="静态分区和动态分区"><a href="#静态分区和动态分区" class="headerlink" title="静态分区和动态分区"></a>静态分区和动态分区</h3><p>分区是Hive存放数据的一种形式，查询数据时使用分区列进行过滤，只需根据列值直接扫描对应目录下的数据。</p><p>Hive的一个分区名对应一个目录名，子分区名就是子目录名，并不是一个实际的字段。</p><p>插入数据的时候指定分区，其实就是新建一个目录或者子目录，或者在原有的目录上添加数据文件。</p><h4 id="静态分区"><a href="#静态分区" class="headerlink" title="静态分区"></a>静态分区</h4><p>若分区的值是确定的，那么称为静态分区。新增分区或者是加载分区数据时，已经指定分区名。</p><p>创建静态分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> order_partition(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    order_number <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    event_time <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (event_month <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据到分区表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法1：通过load方式加载</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/order.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> order_partition (event_month=<span class="string">'2017-09'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法2：查询并导入</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> order_partition <span class="keyword">partition</span>(event_month=<span class="string">'2017-09'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> order_number,event_time <span class="keyword">from</span> order_partition <span class="keyword">where</span> event_month=<span class="string">'2017-08'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法3：手动创建HDFS文件夹，并上传文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 静态分区表如果手动创建对应的HDFS目录，并上传文件，分区表中无法查到该分区信息，则需要刷新修复</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> order_partition <span class="keyword">where</span> event_month=<span class="string">'2017-09'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 此时是查询不到分区数据的，需要修复表信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">msck <span class="keyword">repair</span> <span class="keyword">table</span> order_partiton;</span></pre></td></tr></table></figure><p>查询指定分区数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> order_partiton <span class="keyword">where</span> event_month=<span class="string">'2017-09'</span>;</span></pre></td></tr></table></figure><h4 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h4><p>动态分区指不需要为不同的分区添加不同的插入语句，分区不确定，需要从数据中获取。</p><p>参数设置：</p><ol><li><p>开启动态分区功能（默认true，开启）</p><p><code>set hive.exec.dynamic.partition=true</code></p></li><li><p>设置为非严格模式（默认strict，表示必须指定至少一个分区为静态分区，nonstrict表示允许所有的分区字段都可以使用动态分区）</p><p><code>set hive.exec.dynamic.partiton.mode=nonstrict</code></p></li><li><p>在所有执行MR的节点上，最大一共可以创建多少个动态分区（默认1000）</p><p><code>set hive.exec.max.dynamic.partitions=1000</code></p></li><li><p>在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错</p><p><code>set hive.exec.max.dynamic.partitions.pernode=100</code></p></li><li><p>整个MR job中，最大可以创建多少个HDFS文件（默认值100000）</p><p><code>set hive.exec.max.created.files=100000</code></p></li><li><p>当有空分区生成时，是否抛出异常（默认false，一般不需要设置）</p><p><code>set hive.error.on.empty.partition=false</code></p></li></ol><p>案例需求：</p><p>将ori中的数据按照时间(如：20111230000008)，插入到目标表ori_partition_target的相应分区中。</p><p>创建分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partition(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (p_time <span class="built_in">bigint</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据到分区表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/datas/ds1'</span> <span class="keyword">into</span> <span class="keyword">table</span> ori_partition <span class="keyword">partition</span>(p_time=<span class="string">'20111230000010'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/datas/ds2'</span> <span class="keyword">into</span> <span class="keyword">table</span> ori_partition <span class="keyword">partition</span>(p_time=<span class="string">'20111230000011'</span>);</span></pre></td></tr></table></figure><p>创建目标分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partition_target(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (p_time <span class="keyword">string</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>设置动态分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition = <span class="literal">true</span>;<span class="comment">-- （默认true）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode = nonstrict;<span class="comment">-- (默认strict)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions = <span class="number">1000</span>;<span class="comment">-- (默认1000)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode = <span class="number">100</span>;<span class="comment">-- （默认100）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.max.created.files = <span class="number">100000</span>;<span class="comment">-- (默认值100000)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.error.on.empty.partition = <span class="literal">false</span>;<span class="comment">-- (默认值false)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> ori_partition_target partitoin(p_time) <span class="keyword">select</span> <span class="keyword">id</span>,<span class="built_in">time</span>,uid,keyword,url_rank,click_num,click_url,p_time <span class="keyword">from</span> ori_partition;</span></pre></td></tr></table></figure><p>查看目标分区表的分区情况：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> ori_partition_target;</span></pre></td></tr></table></figure><p>如果不设置为非严格模式，报错如下：</p><p>FAILED: SemanticException [Error<br>10096]: Dynamic partition strict mode requires at least one static partition<br>column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict.</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>尽量不要使用动态分区，因为动态分区的时候，将会为每一个分区分配reducer，当分区数量多的时候，reducer数量将会相应增加。</p><p>静态分区不管有没有数据都会创建该分区，动态分区是有结果集就创建，否则不创建。</p><p>hive.mapred.mode=strict 为了阻止用户不小心提交恶意HQL</p><p>如果设置为strict，将会阻止以下3种查询：</p><ol><li>对分区表查询，where中过滤字段不是分区字段</li><li>笛卡尔积join查询，不带on或where的join查询语句</li><li>order by查询不带limit条件</li></ol><h3 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h3><p>分区针对的是数据的存储路径，分桶针对的是数据文件。</p><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。分桶是将数据集分解成更容易管理的若干部分的一个技术。</p><p>Hive分桶就是将表（或者分区，即HDFS存储在该目录下的真正数据）中文件分成几个文件去存储。在处理大规模数据集时，在开发阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。</p><p>桶中的数据可以根据一个或多个列另外进行排序，这样对每个桶的连接变成了高效的归并排序（merge-sort），可以进一步提升Map端连接的效率。</p><p>声明一个表使用排序桶：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bucketed_user(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sorted <span class="keyword">by</span>(<span class="keyword">id</span> <span class="keyword">asc</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span></pre></td></tr></table></figure><h4 id="创建分桶表，并导入本地数据"><a href="#创建分桶表，并导入本地数据" class="headerlink" title="创建分桶表，并导入本地数据"></a>创建分桶表，并导入本地数据</h4><p>创建分桶表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_bucket(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) <span class="keyword">into</span> <span class="number">4</span> buckets</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>查看表结构：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">desc formatted stu_bucket;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Num Buckets:            4</span></pre></td></tr></table></figure><p>导入数据到分桶表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_bucket;</span></pre></td></tr></table></figure><p>此时会发现并没有分成4个桶</p><h4 id="创建分桶表，并通过子查询的方式导入数据"><a href="#创建分桶表，并通过子查询的方式导入数据" class="headerlink" title="创建分桶表，并通过子查询的方式导入数据"></a>创建分桶表，并通过子查询的方式导入数据</h4><p>创建一个普通的stu表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>向stu表中导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu;</span></pre></td></tr></table></figure><p>清空stu_bucket表中数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> stu_bucket;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_bucket;</span></pre></td></tr></table></figure><p>导入数据到分桶表，通过子查询的方式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span></pre></td></tr></table></figure><p>设置属性：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">-1</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_bucket <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span></pre></td></tr></table></figure><p>查询分桶的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_bucket;</span></pre></td></tr></table></figure><h4 id="分桶抽样查询"><a href="#分桶抽样查询" class="headerlink" title="分桶抽样查询"></a>分桶抽样查询</h4><p>对于非常大的数据集，有时需要使用的是一个具有代表性的查询结果而不是全部结果，Hive可以通过对表进行抽样来满足这个需求。</p><p>查询stu_bucket中的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_bucket <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- tablesample是抽样语句，语法：tablesample(bucket x out of y)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 不是桶数的倍数或者因子也可以，但是不推荐</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_bucket <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">3</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span></pre></td></tr></table></figure><p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取（4/2=）2个bucket的数据，当y=8时，抽取（4/8=）1/2个bucket的数据。</p><p>x表示从哪个bucket开始抽取。例如，table总bucket数为4，tablesample(bucket 4 out of 4)，表示总共抽取（4/4=）1个bucket的数据，抽取第4个bucket的数据。</p><p>注意：x的值必须小于等于y的值，否则：</p><p>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_bucket.</p><h4 id="数据块抽样"><a href="#数据块抽样" class="headerlink" title="数据块抽样"></a>数据块抽样</h4><p>Hive提供了另外一种按照百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行的抽样。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu <span class="keyword">tablesample</span>(<span class="number">0.1</span> <span class="keyword">percent</span>);</span></pre></td></tr></table></figure><p>提示：这种抽样方式不一定适用于所有的文件格式。另外，这种抽样的最小抽样单元是一个HDFS数据块。因此，如果表的数据大小小于普通的块的大小128M的话，那么将会返回所有行。</p><h3 id="行转列和列转行"><a href="#行转列和列转行" class="headerlink" title="行转列和列转行"></a>行转列和列转行</h3><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>创建学生表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_info(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">course <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 导入本地数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/student_info.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> student_info;</span></pre></td></tr></table></figure><p>查看表中数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student_info;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------------+--------------------+----------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| student_info.id  | student_info.name  | student_info.course  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------------+--------------------+----------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 103              | zhangsan           | Java                 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 103              | zhangsan           | SQL                  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 105              | lisi               | Python               |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 109              | wangwu             | Scala                |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 109              | wangwu             | Spark                |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------------+--------------------+----------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">5 rows selected (0.742 seconds)</span></pre></td></tr></table></figure><p>将多行转为1行（行转列）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">max</span>(<span class="keyword">id</span>),<span class="keyword">name</span>,<span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_set(course)) <span class="keyword">as</span> courses</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 没有id列</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_set(course)) <span class="keyword">as</span> courses</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">INFO  : OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| _c0  |   name    |   courses    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| 105  | lisi      | Python       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">| 109  | wangwu    | Scala,Spark  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">| 103  | zhangsan  | Java,SQL     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">3 rows selected (35.758 seconds)</span></pre></td></tr></table></figure><h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_info2(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">courses <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 加载数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/student_info2.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> student_info2;</span></pre></td></tr></table></figure><p>查看表数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student_info2;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+---------------------+------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| student_info2.id  | student_info2.name  | student_info2.courses  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+---------------------+------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 103               | zhangsan            | Java,SQL               |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 105               | lisi                | Python                 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 109               | wangwu              | Scala,Spark            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+---------------------+------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">3 rows selected (0.128 seconds)</span></pre></td></tr></table></figure><p>将1行转为多行（列转行）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,course </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student_info2 a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(a.courses, <span class="string">','</span>)) b <span class="keyword">as</span> course;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">|  id  |   name    | course  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 103  | zhangsan  | Java    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 103  | zhangsan  | SQL     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 105  | lisi      | Python  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 109  | wangwu    | Scala   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 109  | wangwu    | Spark   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">5 rows selected (0.202 seconds)</span></pre></td></tr></table></figure><h4 id="关键函数释义"><a href="#关键函数释义" class="headerlink" title="关键函数释义"></a>关键函数释义</h4><p>使用<code>desc function extended split</code>可以查看split()函数的用法，行列互转的关键函数的官方释义如下：</p><ul><li><p><code>collect_set(x)</code> - returns a set of objects with duplicate elements eliminated.</p><p>返回一个不重合的set集合</p></li><li><p><code>concat_ws(separator, [string | array(string)]+)</code> - returns the concatenation of the strings separated by the separator.</p><p>返回以指定分隔符隔离的拼接起来的字符串</p></li><li><p><code>split(str, regex)</code> - splits str around occurances that match regex.</p><p>以指定分隔符切割字符串，并返回切割后的字符串数组</p></li><li><p><code>explode(a)</code> - separates the elements of array a into multiple rows, or the elements of a map into multiple rows and columns.</p><p>分割数组元素为多行，或分割map的元素为多行和多列</p></li><li><p><code>lateral view</code> - 侧视图配合<code>explode</code>，把单行数据拆解成多行，不加<code>lateral view</code>的UDTF(User-Defined Table-Generating Functions)只能提取单个字段拆分，并不能塞回原来的数据表中，加上<code>lateral view</code>就可以将拆分的单个字段数据与原始表数据关联上</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理表和外部表 </tag>
            
            <tag> 分区表 </tag>
            
            <tag> 分桶表 </tag>
            
            <tag> 行列互转 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基本概念&amp;Hive的部署&amp;Hive的基本命令</title>
      <link href="/2018/04/14/dw/hive-1/"/>
      <url>/2018/04/14/dw/hive-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive基本概念"><a href="#Hive基本概念" class="headerlink" title="Hive基本概念"></a>Hive基本概念</h3><p>Hive的官网：</p><p><a href="http://hive.apache.org/" target="_blank" rel="noopener">http://hive.apache.org/</a></p><p>The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p><h4 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h4><p>Hive: 由Facebook开源，用于解决海量结构化日志的数据统计问题。</p><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文化映射为一张表，并提供类SQL查询功能。</p><p>本质：将HQL/SQL转化成MapReduce程序</p><ol><li>Hive处理的数据存储在HDFS</li><li>Hive底层执行引擎：MapReduce/Tez/Spark，只需要通过一个参数就能够切换底层的执行引擎</li><li>Hive作业提交到YARN上运行</li></ol><h4 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h4><p><strong>优点：</strong></p><ol><li>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）</li><li>避免了MapReduce编程，减少学习成本</li><li>Hive的执行延迟比较高，因此Hive常用于离线分析，对实时性要求不高的场合</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li></ol><p><strong>缺点：</strong></p><ol><li>Hive的HQL表达能力有限<ol><li>迭代式算法无法表达</li><li>数据挖掘方面不擅长</li></ol></li><li>Hive的效率比较低<ol><li>Hive自动生成的MapReduce作业，通常不够智能化</li><li>Hive调优比较困难，粒度较粗</li></ol></li></ol><h4 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h4><p><img src="https://vinxikk.github.io/img/dw/hive-architecture.png" alt="Hive的架构"></p><p>如上图所示，Hive通过给用户提供的一系列交互接口，接收到用户的指令（SQL），使用自己的Driver，结合元数据（MetaStore），将这些指令翻译成MapReduce，提交到Hadoop中执行，最后将执行返回的结果输出到用户交互接口。</p><ol><li><p>用户接口：Client</p><p>CLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive)</p></li><li><p>元数据：Meta store</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p><p>默认存储在自带的derby数据库中，推荐使用MySQL存储Meta store。</p></li><li><p>Hadoop</p><p>使用HDFS进行存储，使用MapReduce进行计算</p></li><li><p>驱动器：Driver</p><ol><li>解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</li><li>编译器（Physical Plan）：将AST编译生成逻辑执行计划。</li><li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li><li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划，对于Hive来说，就是MR/Spark。</li></ol></li></ol><h3 id="Hive的部署"><a href="#Hive的部署" class="headerlink" title="Hive的部署"></a>Hive的部署</h3><p>Hive的使用文档：</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p><p>环境要求：</p><ul><li>JDK8</li><li>Hadoop2.x</li><li>Linux</li><li>MySQL</li></ul><p>Hive版本选择：<code>wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2.tar.gz</code></p><h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><p>解压：<code>tar -zxvf hive-1.1.0-cdh5.16.2.tar.gz -C ~/app/</code></p><p>设置软连接：<code>ln -s hive-1.1.0-cdh5.16.2 hive</code></p><p>配置环境变量，建议普通用户的<code>.bashrc</code>文件，追加以下内容：</p><p><code>export HIVE_HOME=/home/hadoop/app/hive</code><br><code>export PATH=$HIVE_HOME/bin:$PATH</code></p><p>生效环境变量文件：<code>source .bashrc</code></p><p>检查是否生效：<code>which hive</code></p><p><code>cd $HIVE_HOME/conf/</code>进入配置文件目录，<code>vi hive-site.xml</code>，编译以下内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop001:3306/metadata_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr></table></figure><p>保存并退出。</p><p>然后，拷贝MySQL驱动包到$HIVE_HOME/lib/下。</p><p>最后，启动Hive（需要先启动HDFS和YARN）：<code>hive</code></p><h3 id="Hive的使用"><a href="#Hive的使用" class="headerlink" title="Hive的使用"></a>Hive的使用</h3><h4 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h4><ul><li><code>show databases;</code>  查看数据库</li><li><code>use default;</code>  打开默认数据库</li><li><code>create database company location &#39;/user/company&#39;;</code>  创建库并指定hdfs路径</li><li><code>alter database company set dbproperties(&#39;creator&#39; = &#39;vinx&#39;);</code>  为数据库添加额外的描述信息</li><li><code>drop database if exists company cascade;</code>  删除不为空的数据库</li><li><code>show tables;</code>  查看所在数据库的所有表</li><li><code>create table student(id int, name string);</code>  创建一张表</li><li><code>desc student;</code>  查看表结构</li><li><code>desc extended student;</code>  查看表结构详细信息</li><li><code>desc formatted student;</code>  查看表结构的格式化信息</li><li><code>insert into student values(1001, &quot;zhangsan&quot;);</code>  向表中插入一条数据（慎用，会跑MR程序）</li><li><code>select * from student;</code>  查询表中数据</li><li><code>!clear;</code>  清空屏幕</li><li><code>quit;</code>  退出hive</li><li><code>bin/hive -e &quot;select * from student;&quot;</code>  不登录hive客户端直接操作hive</li><li><code>bin/hive -f /home/vinx/shell/select_stu.sql</code>  执行HQL文件</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive基本概念 </tag>
            
            <tag> Hive SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch ERROR汇总</title>
      <link href="/2018/04/13/es/es-boot-error/"/>
      <url>/2018/04/13/es/es-boot-error/</url>
      
        <content type="html"><![CDATA[<h4 id="权限错误"><a href="#权限错误" class="headerlink" title="权限错误"></a>权限错误</h4><p>Q: 启动es时，如出现异常：can not run elasticsearch as root.</p><p>问题原因：elasticsearch权限控制严格，不能用root用户启动。</p><p>解决方法：创建一个新账户，并修改es文件夹用户权限。</p><h4 id="启动错误"><a href="#启动错误" class="headerlink" title="启动错误"></a>启动错误</h4><p>Centos6.5启动elasticsearch时出现以下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">ERROR: [4] bootstrap checks failed</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[2]: max number of threads [1024] for user [vinx] is too low, increase to at least [2048]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk</span></pre></td></tr></table></figure><p>解决方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[1]: max file descriptors [4096] <span class="keyword">for</span> elasticsearch process is too low, increase to at least [65536]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">* soft nofile 65536</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">* hard nofile 65536</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">重新登录用户，才会生效</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">[2]: max number of threads [1024] <span class="keyword">for</span> user [vinx] is too low, increase to at least [2048]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.d/90-nproc.conf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">定位到：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">* soft nproc 1024</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">修改为：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">* soft nproc 4096</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误3</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">vi /ect/sysctl.conf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">vm.max_map_count=655360</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">保存后，执行：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">sysctl -p</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误4</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">[4]: system call filters failed to install; check the logs and fix your configuration or <span class="built_in">disable</span> system call filters at your own risk</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误原因</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">因为Centos6不支持SecComp，而ES5.2.1默认bootstrap.system_call_filter为<span class="literal">true</span>进行检测，所以导致检测失败，失败后直接导致ES不能启动。</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">参考：https://github.com/elastic/elasticsearch/issues/22899</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">修改config/elasticsearch.yml，修改或追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">bootstrap.memory_lock: <span class="literal">false</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">bootstrap.system_call_filter: <span class="literal">false</span></span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> ES </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ES错误 </tag>
            
            <tag> ERROR信息 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch入门</title>
      <link href="/2018/04/12/es/es-basic/"/>
      <url>/2018/04/12/es/es-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="ElasticSearch简介"><a href="#ElasticSearch简介" class="headerlink" title="ElasticSearch简介"></a>ElasticSearch简介</h3><hr><h4 id="什么是ElasticSearch"><a href="#什么是ElasticSearch" class="headerlink" title="什么是ElasticSearch"></a>什么是ElasticSearch</h4><ul><li>基于Apache Lucene构建的<strong>开源搜索引擎</strong></li><li>采用Java编写，提供简单易用的RESTFul API</li><li>轻松的横向扩展，可支持PB级的结构化或非结构化数据处理</li></ul><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><ul><li>海量数据分析引擎</li><li>站内搜索引擎</li><li>数据仓库</li></ul><h3 id="ES安装"><a href="#ES安装" class="headerlink" title="ES安装"></a>ES安装</h3><h4 id="ES官网"><a href="#ES官网" class="headerlink" title="ES官网"></a>ES官网</h4><p><a href="https://www.elastic.co" target="_blank" rel="noopener">https://www.elastic.co</a></p><h4 id="单实例安装"><a href="#单实例安装" class="headerlink" title="单实例安装"></a>单实例安装</h4><p>环境要求：</p><ul><li>JDK1.8</li><li>NodeJs(6.0以上)</li></ul><h5 id="Node安装"><a href="#Node安装" class="headerlink" title="Node安装"></a>Node安装</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压安装好后，查看版本(此时node和npm命令不是全局的，需要设置软连接)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@eshost001 node]$ ./bin/node -v</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">v8.2.1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置软连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">ln -s node-v8.2.1-linux-x64 node</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">ln -s /home/vinx/app/node/bin/node /usr/<span class="built_in">local</span>/bin/node</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">ln -s /home/vinx/app/node/bin/npm /usr/<span class="built_in">local</span>/bin/npm</span></pre></td></tr></table></figure><h5 id="es安装："><a href="#es安装：" class="headerlink" title="es安装："></a>es安装：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gz</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">tar -vxf elasticsearch-5.5.2.tar.gz</span></pre></td></tr></table></figure><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> elasticsearch-5.5.2/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sh ./bin/elasticsearch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[master] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;127.0.0.1:9200&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[master] started</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[master] recovered [0] indices into cluster_state</span></pre></td></tr></table></figure><h4 id="head插件安装"><a href="#head插件安装" class="headerlink" title="head插件安装"></a>head插件安装</h4><h5 id="下载解压"><a href="#下载解压" class="headerlink" title="下载解压"></a>下载解压</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">wget https://github.com/mobz/elasticsearch-head/archive/master.zip</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">unzip master.zip</span></pre></td></tr></table></figure><h5 id="安装运行"><a href="#安装运行" class="headerlink" title="安装运行"></a>安装运行</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> elasticsearch-head-master/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">npm install</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">num run start</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&gt; elasticsearch-head@0.0.0 start /home/vinx/app/elasticsearch-head-master</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&gt; grunt server</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">Running <span class="string">"connect:server"</span> (connect) task</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">Waiting forever...</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Started connect web server on http://localhost:9100</span></pre></td></tr></table></figure><h5 id="修改配置文件："><a href="#修改配置文件：" class="headerlink" title="修改配置文件："></a>修改配置文件：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改yml配置文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">vi config/elasticsearch.yml</span></pre></td></tr></table></figure><p>在最下面添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">http.cors.enabled: <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">http.cors.allow-origin: <span class="string">"*"</span></span></pre></td></tr></table></figure><h5 id="重新启动"><a href="#重新启动" class="headerlink" title="重新启动"></a>重新启动</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动elasticsearch 后台-d</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">./bin/elasticsearch -d</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动head插件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">npm run start</span></pre></td></tr></table></figure><h5 id="启动界面"><a href="#启动界面" class="headerlink" title="启动界面"></a>启动界面</h5><p><img src="https://vinxikk.github.io/img/es/es-single-green.png" alt="es单实例"></p><h4 id="分布式安装"><a href="#分布式安装" class="headerlink" title="分布式安装"></a>分布式安装</h4><p>内存建议8G。</p><h5 id="master配置文件"><a href="#master配置文件" class="headerlink" title="master配置文件"></a>master配置文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi config/elasticsearch.yml</span></pre></td></tr></table></figure><p>增加如下信息：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="attr">http.cors.enabled:</span> <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">http.cors.allow-origin:</span> <span class="string">"*"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="attr">cluster.name:</span> <span class="string">vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">node.name:</span> <span class="string">master</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="attr">node.master:</span> <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="attr">network.host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span></pre></td></tr></table></figure><h5 id="slave配置文件"><a href="#slave配置文件" class="headerlink" title="slave配置文件"></a>slave配置文件</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># slave1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">cluster.name:</span> <span class="string">vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="attr">node.name:</span> <span class="string">slave1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">network.host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="attr">http.port:</span> <span class="number">8200</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="attr">discovery.zen.ping.unicast.hosts:</span> <span class="string">["127.0.0.1"]</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">--------------------------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># slave2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="attr">cluster.name:</span> <span class="string">vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="attr">node.name:</span> <span class="string">slave2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="attr">network.host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="attr">http.port:</span> <span class="number">8000</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="attr">discovery.zen.ping.unicast.hosts:</span> <span class="string">["127.0.0.1"]</span></span></pre></td></tr></table></figure><h5 id="分布式ES启动"><a href="#分布式ES启动" class="headerlink" title="分布式ES启动"></a>分布式ES启动</h5><p><img src="https://vinxikk.github.io/img/es/es-full-distr.png" alt="es分布式集群"></p><h3 id="ES的用法"><a href="#ES的用法" class="headerlink" title="ES的用法"></a>ES的用法</h3><h4 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h4><ul><li>Cluster-Node集群和节点：集群 = 节点1 + 节点2 + … + 节点N</li></ul><ul><li>Index索引：含有相同属性的文档集合 - Database</li><li>Type类型：索引可以定义一个或多个类型，文档必须属于一个类型 - Table</li><li>Document文档：文档是可以被索引的基本数据单位 - Row</li><li>Field字段：Column</li><li>分片：每个索引都有多个分片，每个分片是一个Lucene索引</li><li>备份：拷贝一份分片就完成了分片的备份</li></ul><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><h5 id="索引创建"><a href="#索引创建" class="headerlink" title="索引创建"></a>索引创建</h5><p><strong>RESTFul API</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">API基本格式  http:&#x2F;&#x2F;&lt;ip&gt;:&lt;port&gt;&#x2F;&lt;索引&gt;&#x2F;&lt;类型&gt;&#x2F;&lt;文档id&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">常用HTTP动词 GET&#x2F;PUT&#x2F;POST&#x2F;DELETE</span></pre></td></tr></table></figure><h5 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h5><ul><li>指定文档id插入</li><li>自动产生文档id插入</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 指定id为1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">PUT eshost001:9200/people/man/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"name"</span>: <span class="string">" vinx"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"country"</span>: <span class="string">"CN"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"age"</span>: <span class="number">30</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"date"</span>: <span class="string">"1987-03-07"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"># 自动产生id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/people/man/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"name"</span>: <span class="string">" super vinx"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"country"</span>: <span class="string">"CN"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"age"</span>: <span class="number">40</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"date"</span>: <span class="string">"1977-03-07"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h5 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h5><ul><li>直接修改文档</li><li>脚本修改文档</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 直接修改</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/people/man/1/_update</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"doc"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"name"</span>: <span class="string">"who's vinx"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"># 脚本修改</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/people/man/1/_update</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"script"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"lang"</span>: <span class="string">"painless"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"inline"</span>: <span class="string">"ctx._source.age += 1"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"script"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"lang"</span>: <span class="string">"painless"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"inline"</span>: <span class="string">"ctx._source.age = params.age"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"params"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"age"</span>: <span class="number">100</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h5 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h5><ul><li>删除文档</li><li>删除索引</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 删除id为1文档</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">DELETE eshost001:9200/people/man/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"># 删除索引people</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">DELETE eshost001:9200/people</span></pre></td></tr></table></figure><h5 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h5><ul><li>简单查询</li><li>条件查询</li><li>聚合查询</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 预先创建book索引</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"># 增加mapping</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">book/novel/_mappings</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="attr">"novel"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="attr">"properties"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      <span class="attr">"word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"type"</span>: <span class="string">"integer"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      &#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      <span class="attr">"author"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"type"</span>: <span class="string">"keyword"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">      &#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">      <span class="attr">"title"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"type"</span>: <span class="string">"text"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      &#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      <span class="attr">"publish_date"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss || yyyy-MM-dd || epoch_millis"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"type"</span>: <span class="string">"date"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"># 简单查询：查询id为1的文档</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">GET eshost001:9200/book/novel/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"># 条件查询</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"># 查询所有数据</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/book/_search</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"query"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"match_all"</span>: &#123;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"># 限制查询条数</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"query"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"match_all"</span>: &#123;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"from"</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"size"</span>: <span class="number">1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line"># 查询title中含有es的记录</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"query"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"match"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"title"</span>: <span class="string">"es"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line"># 指定排序条件</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"query"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"match"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"title"</span>: <span class="string">"es"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"sort"</span>: [</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"publish_date"</span>: &#123;<span class="attr">"order"</span>: <span class="string">"desc"</span>&#125;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"># 聚合查询</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line"># 单个分组聚合，根据word_count字段聚合</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/book/_search</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"aggs"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"group_by_word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"terms"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"word_count"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line"># 多个分组聚合</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"aggs"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"group_by_word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"terms"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"word_count"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line">&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"group_by_publish_date"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"terms"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"publish_date"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line"># 统计函数</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"aggs"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"grades_word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"stats"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"word_count"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">104</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">105</span></pre></td><td class="code"><pre><span class="line"># 最小值</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">106</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">107</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"aggs"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">108</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"grades_word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">109</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"min"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">110</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"word_count"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">111</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">112</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">113</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">114</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h4 id="高级查询"><a href="#高级查询" class="headerlink" title="高级查询"></a>高级查询</h4><ul><li>子条件查询 - 特定字段查询所指特定值</li><li>复合条件查询 - 以一定的逻辑组合子条件查询</li></ul><p><strong>子条件查询：</strong></p><ul><li><p>Query context</p><p>在查询过程中，除了判断文档是否满足查询条件外，ES还会计算一个<strong>_score</strong>来标识匹配的程度，旨在判断目标文档和查询条件匹配的<strong>有多好</strong>。</p></li><li><p>Filter context</p><p>在查询过程中，只判断该文档是否满足条件，只有Yes或者No。</p></li></ul><p><strong>Query Context常用查询：</strong></p><ul><li>全文本查询 - 针对文本类型数据</li><li>字段级别查询 - 针对结构化数据，如数字、日期等</li></ul><p><strong>复合条件查询：</strong></p><ul><li>固定分数查询</li><li>布尔查询</li></ul>]]></content>
      
      
      <categories>
          
          <category> ES </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ES入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN参数调优&amp;资源调度器</title>
      <link href="/2018/04/11/dw/hadoop-6/"/>
      <url>/2018/04/11/dw/hadoop-6/</url>
      
        <content type="html"><![CDATA[<h3 id="YARN参数调优"><a href="#YARN参数调优" class="headerlink" title="YARN参数调优"></a>YARN参数调优</h3><p>container: </p><p>运行task任务的容器，虚拟化的，维度memory+vcore</p><h4 id="系统和组件预留"><a href="#系统和组件预留" class="headerlink" title="系统和组件预留"></a>系统和组件预留</h4><p>假设128G，16物理core</p><ol><li><p>装完CentOS，消耗内存1G</p></li><li><p>系统预留15-20%内存（包括1），以防全部使用导致系统夯住和OOM-kill机制，或者给将来部署其他组件预留空间。</p><p>128x20%=25.6，取26G</p></li><li><p>假设只有DN、NM节点，DN=2G，NM=4G</p></li><li><p>至此剩下的内存为：128-26-2-4=96G</p></li></ol><h4 id="container内存"><a href="#container内存" class="headerlink" title="container内存"></a>container内存</h4><p>官网配置文件：</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></p><p>相关参数和默认配置如下表：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.memory-mb</td><td>-1</td><td>Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB.</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td><td>The minimum allocation for every container request at the RM in MBs. Memory requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have less memory than this value will be shut down by the resource manager.</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td><td>The maximum allocation for every container request at the RM in MBs. Memory requests higher than this will throw an InvalidResourceRequestException.</td></tr></tbody></table><p>按照现存的96G内存，一般设置如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.memory-mb</td><td>96G</td><td></td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1G</td><td>极限情况下，只有96个container，内存1G</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>96G</td><td>极限情况下，只有1个container，内存96G</td></tr></tbody></table><p>container的内存会自动增加，默认1G递增。</p><p>container：1-96个</p><h4 id="container虚拟核"><a href="#container虚拟核" class="headerlink" title="container虚拟核"></a>container虚拟核</h4><p>官网默认参数如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.pcores-vcores-multiplier</td><td>1.0</td><td>Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The number of vcores will be calculated as number of CPUs * multiplier.</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>-1</td><td>Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default.</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td><td>The minimum allocation for every container request at the RM in terms of virtual CPU cores. Requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have fewer virtual cores than this value will be shut down by the resource manager.</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>4</td><td>The maximum allocation for every container request at the RM in terms of virtual CPU cores. Requests higher than this will throw an InvalidResourceRequestException.</td></tr></tbody></table><p>一般设置如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.pcores-vcores-multiplier</td><td>2</td><td>物理核：虚拟核</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>32</td><td>虚拟核总数</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td><td>极限情况下，只有32个container</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>32</td><td>极限情况下，只有1个container</td></tr></tbody></table><p>container：1-32个</p><p>cloudera公司推荐，一个container的vcore最好不要超过5，我们这里设置4，即：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>4</td><td>极限情况下，只有8个container</td></tr></tbody></table><h4 id="综合memory-vcore"><a href="#综合memory-vcore" class="headerlink" title="综合memory + vcore"></a>综合memory + vcore</h4><p>以vcore为主的话，确定vcore=4，那么container=8，最终memory的参数如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>12G</td><td>极限情况，container有8个</td></tr></tbody></table><p>这是理想情况，当Spark计算时内存不够大，这个参数肯定要调大，那么这种理想化的以vcore为主的设置必然要打破，以memory为主。</p><p>假如该节点还有其他组件，比如：hbase regionserver进程，需要先分配给其他组件，然后再分配container的vcore和memory。</p><p>一般，hbase regionserver=30G</p><h4 id="vcore解读"><a href="#vcore解读" class="headerlink" title="vcore解读"></a>vcore解读</h4><p>YARN自己引入的，设计的初衷是考虑不同节点的CPU的性能和计算能力不一样，比如某个物理CPU是另外一个物理CPU的2倍，这时通过设置第一个物理CPU的虚拟core来弥补这种差异。</p><p>第一台机器 强悍 pcore:vcore=1:2</p><p>第二台机器 不强悍 pcore:vcore=1:1</p><h3 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h3><p>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。</p><p>Hadoop默认的资源调度器是Capacity Scheduler，CDH默认是Fair Scheduler。</p><p>apache官网默认参数（yarn-site.xml）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>三种调度器如下：</p><p><img src="https://vinxikk.github.io/img/dw/scheduler.png" alt="YARN的3种资源调度器"></p><h4 id="FIFO（先进先出）"><a href="#FIFO（先进先出）" class="headerlink" title="FIFO（先进先出）"></a>FIFO（先进先出）</h4><p><img src="https://vinxikk.github.io/img/dw/fifo-scheduler.png" alt="FIFO-先进先出调度器"></p><p>优点：调度算法简单，JobTracker工作负担轻。</p><p>缺点：忽略了不同作业的需求差异。例如如果类似对海量数据进行统计分析的作业长期占据计算资源，那么在其后提交的交互型作业有可能迟迟得不到处理，从而影响到用户的体验。</p><h4 id="Capacity-Scheduler（容量调度器）"><a href="#Capacity-Scheduler（容量调度器）" class="headerlink" title="Capacity Scheduler（容量调度器）"></a>Capacity Scheduler（容量调度器）</h4><p><img src="https://vinxikk.github.io/img/dw/capacity-scheduler.png" alt="Capacity Scheduler-容量调度器"></p><p>Capacity Scheduler：</p><ol><li>多队列支持，每个队列采用FIFO</li><li>为了防止同一个用户的作业独占队列中的资源，该调度器会对同一个用户提交多的作业所占资源量进行限定</li><li>首先，计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列</li><li>其次，根据作业的优先级和提交时间顺序，同时考虑用户资源量限制和内存限制对队列内任务排序</li><li>三个队列同时按照任务的先后顺序依次执行，比如：job1, job21和job31分别排在队列最前面，是最先运行，也是同时运行。</li></ol><p>容量调度器默认情况下不支持优先级，但是可以在配置文件中开启此选项，如果支持优先级，调度算法就是带有优先级的FIFO。</p><p>不支持优先级抢占，一旦一个作业开始执行，在执行完之前它的资源不会被高优先级作业所抢占。</p><p>对队列中同一用户提交的作业能够获得的资源百分比进行了限制以使同属于一个用户的作业不能出现独占资源的情况。</p><h4 id="Fair-Scheduler（公平调度器）"><a href="#Fair-Scheduler（公平调度器）" class="headerlink" title="Fair Scheduler（公平调度器）"></a>Fair Scheduler（公平调度器）</h4><p><img src="https://vinxikk.github.io/img/dw/fair-scheduler.png" alt="Fair Scheduler-容量调度器"></p><p>Fair Scheduler：</p><ol><li>支持多队列多用户，每个队列中的资源量可以配置，同一个队列中的作业公平共享队列中所有资源</li><li>比如有三个队列A, B, C。每个队列中的job按照优先级分配资源，优先级越高分配的资源越多，但是每个job都分配到资源以确保公平。在资源有限的情况下，每个job理想情况下，获得的计算资源与实际获得计算资源存在一种差距，这个差距叫缺额。同一个队列，job的资源缺额越大，越先获得的资源优先执行，作业是按照缺额的高低来先后执行的，而且可以看到上图有多个作业同时运行。</li></ol><h3 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h3><p>推测执行（Speculative Execution）是指在集群环境下运行MapReduce，可能是程序bug、负载不均或其他一些问题，导致在一个job的多个task速度不一致，比如有的任务已经完成，但是有些任务可能只跑了10%，根据木桶原理，这些任务将成为整个job的短板。如果集群启动了推测执行，这时为了最大限度的提高短板，Hadoop会为该task启动备份任务，让speculative task与原始task同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果，并且在运行完成后kill掉另外一个任务。</p><ol><li><p>作业完成时间取决于最慢的任务完成时间</p><p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件bug等，某些任务可能运行非常慢。</p><p>典型案例：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p></li><li><p>推测执行机制</p><p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时执行。谁先运行完，则采用谁的结果。</p></li><li><p>执行推测任务的前提条件</p><ol><li><p>每个task只能有一个备份任务</p></li><li><p>当前job已完成的task不小于0.05（5%）</p></li><li><p>开启推测执行参数设置，mapred-site.xml中默认是打开的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks                may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">               may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure></li><li><p>不能启动推测执行机制的情况：</p><ol><li>任务间存在严重的负载倾斜</li><li>特殊任务，比如任务向数据库中写数据</li></ol></li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YARN调优 </tag>
            
            <tag> 调度器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN架构&amp;InputSplit和MapTask的关系&amp;Shuffle机制&amp;压缩格式</title>
      <link href="/2018/04/08/dw/hadoop-5/"/>
      <url>/2018/04/08/dw/hadoop-5/</url>
      
        <content type="html"><![CDATA[<h3 id="MR-on-YARN流程"><a href="#MR-on-YARN流程" class="headerlink" title="MR on YARN流程"></a>MR on YARN流程</h3><p>YARN是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p><h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><p>官网原文：</p><p>The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system.</p><p>The ResourceManager has two main components: Scheduler and ApplicationsManager.</p><p>RM组成：</p><ul><li>Application Manager - 应用程序管理器</li><li>Resource Scheduler - memory+cpu资源调度器</li></ul><h4 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h4><p>官网原文：</p><p>The NodeManager is responsible for launching and managing containers on a node. Containers execute tasks as specified by the AppMaster.</p><p>NM组成：</p><ul><li>container - 虚拟概念，执行MR、Spark计算任务的最小单元</li></ul><h4 id="MR-on-YARN"><a href="#MR-on-YARN" class="headerlink" title="MR on YARN"></a>MR on YARN</h4><p><img src="https://vinxikk.github.io/img/dw/mr-on-yarn.png" alt="MR on YARN流程"></p><p>描述：</p><ol><li>用户向YARN提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令等。</li><li>RM为该job分配第一个container，运行job的ApplicationMaster。</li><li>App Master向Application Manager注册，这样就可以在RM WEB界面查询这个job的运行状态。</li><li>App Master采用轮询的方式通过RPC协议向RM申请和领取资源。</li><li>一旦App Master拿到资源，就对应的与NM通信，要求启动任务。</li><li>NM为任务设置好运行环境（jar包等），将任务启动命令写在一个脚本里，并通过该脚本启动任务task。</li><li>各个task通过RPC协议向App Master汇报自己的状态和进度，以此让App Master随时掌握各个task的运行状态，从而在task运行失败后重启任务。</li><li>App Master向Application Manager注销且关闭自己。</li></ol><p>总的来说就是两步：</p><ul><li>启动App Master，申请资源；</li><li>运行任务，直到任务运行完成。</li></ul><h3 id="input-split与map-task的关系"><a href="#input-split与map-task的关系" class="headerlink" title="input split与map task的关系"></a>input split与map task的关系</h3><p>首先，<strong>split数量和map task数量一一对应</strong>。</p><p>下图是wordcount的流程：</p><p><img src="https://vinxikk.github.io/img/dw/wordcount-mr.png" alt="wordcount流程"></p><p>可以看到，一个job的Map阶段map task并行度（个数），由客户端提交job时的切片个数决定。</p><p>假如有以下两个文件，blocksize=128M，则split和map task的关系如下图所示：</p><p><img src="https://vinxikk.github.io/img/dw/split-maptask.png" alt="split-maptask的关系"></p><p>也就是说，有多少个切片，就会启动相应数量的map task进行数据处理。那么，如果需要确定map task的数量，只需要确定切片的实际数量即可。</p><h4 id="切片机制"><a href="#切片机制" class="headerlink" title="切片机制"></a>切片机制</h4><p>默认使用FileInputFormat类处理数据输入，遵循如下的切片机制：</p><ol><li>按照文件的内容长度进行切片</li><li>切片大小，默认等于block大小</li><li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li></ol><p>比如待处理数据有两个文件：</p><p>file01.txt 320M</p><p>file02.txt 10M</p><p>经过FileInputFormat的切片机制运算后，形成的切片信息如下：</p><p>file01.txt.split1–  0~128</p><p>file01.txt.split2–  128~256</p><p>file01.txt.split3–  256~320</p><p>file02.txt.split1–  0~10M</p><p>那么切片的数量是否就是分块的数量+小文件的数据量呢？其实是不一定的，因为源码中还有这样一个参数：private static final double SPLIT_SLOP = 1.1，也就是还有10%的切片裕度，下面结合源码进行说明。</p><h4 id="FileInputFormat源码分析"><a href="#FileInputFormat源码分析" class="headerlink" title="FileInputFormat源码分析"></a>FileInputFormat源码分析</h4><p>版本：hadoop2.6.0-cdh5.15.1</p><p>类路径：package org.apache.hadoop.mapreduce.lib.input.FileInputFormat.java;</p><p>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> SPLIT_SLOP = <span class="number">1.1</span>;   <span class="comment">// 10% slop</span></span></pre></td></tr></table></figure><p>getSplits获取文件切片列表的核心方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * Generate the list of files and make them into FileSplits.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * <span class="doctag">@param</span> job the job context</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * <span class="doctag">@throws</span> IOException</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// generate splits</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;InputSplit&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    List&lt;FileStatus&gt; files = listStatus(job);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      Path path = file.getPath();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">long</span> length = file.getLen();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        BlockLocation[] blkLocations;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">          blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">          FileSystem fs = path.getFileSystem(job.getConfiguration());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">          blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> blockSize = file.getBlockSize();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> bytesRemaining = length;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">          <span class="comment">// while循环判断切完剩下的部分是否大于块的1.1倍，大于的话继续切分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">                        blkLocations[blkIndex].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">                        blkLocations[blkIndex].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">            bytesRemaining -= splitSize;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">          <span class="comment">//剩余部分小于等于1.1倍且不为0，就将剩下的划分为一块</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">                       blkLocations[blkIndex].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">                       blkLocations[blkIndex].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">            <span class="comment">// Log only if the file is big enough to be splitted</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> (length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">              LOG.debug(<span class="string">"File is not splittable so no parallelization "</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">                  + <span class="string">"is possible: "</span> + file.getPath());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">          splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">                      blkLocations[<span class="number">0</span>].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">      &#125; <span class="keyword">else</span> &#123; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//Create empty hosts array for zero length files</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">        splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Save the number of input files for metrics/loadgen</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">    sw.stop();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">      LOG.debug(<span class="string">"Total # of splits generated by getSplits: "</span> + splits.size()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">          + <span class="string">", TimeTaken: "</span> + sw.now(TimeUnit.MILLISECONDS));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> splits;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure><p>整体流程：</p><ol><li>找到数据存储的目录</li><li>遍历处理（规划切片）目录下的每一个文件</li><li>遍历第一个文件<ol><li>获取文件大小</li><li>计算切片大小。默认情况下，切片大小=blocksize</li><li>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分为一块切片</li><li>将切片信息写到一个切片规划文件中</li><li>整个切片的核心过程在getSplit()方法中完成</li><li>数据切片只是在逻辑上对输入数据进行分片，并不会在磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。</li><li>注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分</li></ol></li><li>提交切片规划文件到YARN上，YARN上的AppMaster就可以根据切片规划文件计算开启map task个数。</li></ol><p>总结：所以，切片的数量并不一定等于分块的数量+小文件的数据量，还要考虑大文件切分后剩余的部分是否大于块的1.1倍。</p><h3 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h3><p>MapReduce确保每个reduce的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reduce的过程）就是Shuffle。</p><p><img src="https://vinxikk.github.io/img/dw/shuffle.png" alt="Shuffle"></p><p><img src="https://vinxikk.github.io/img/dw/shuffle-detail.png" alt="Shuffle机制"></p><p><strong>Shuffle过程：</strong></p><p>shuffle一开始就是map阶段做输出操作，map在做输出时会在内存里开启一个环形缓冲区，这个缓冲区是专门用来输出的，默认大小是100MB，并且在配置文件里为这个缓冲区设定了一个阈值，默认是0.80。同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阈值的80%的时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill（溢写）。另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作互不干扰，如果缓存区被撑爆了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作。</p><p>写入磁盘前会有个排序操作，这个是在写入磁盘操作时进行，不是在写入内存时进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。每次spill操作也就是写入磁盘时就会写一个溢出文件，也就是说在做map输出时有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。</p><p>这个过程里还会有一个Partitioner操作，和map阶段的输入分片（Input Split）很像，一个Partitioner对应一个reduce作业，如果我们的MR操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片。</p><p>到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，这个复制过程和map写入磁盘过程类似，也有阈值和内存大小，阈值可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。</p><p>上面也提到，将map的输出作为reduce的输入的过程就是shuffle（洗牌）。</p><p>那么shuffle到底是属于map阶段还是reduce阶段呢？比较公认的说法是，shuffle应该是属于reduce阶段，因为map阶段的主要任务是数据映射，而shuffle的目的是主要是为reduce阶段做准备的。</p><h4 id="MapReduce计算框架的不足之处"><a href="#MapReduce计算框架的不足之处" class="headerlink" title="MapReduce计算框架的不足之处"></a>MapReduce计算框架的不足之处</h4><p>从上图可以看到，shuffle阶段中会有多次写入磁盘的操作，基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。另外，当一些查询（Hive）翻译到MapReduce任务时，往往会产生多个stage（阶段），而这些串联的stage又依赖于底层文件系统（HDFS）来存储每一个stage的输出结果，而I/O的效率往往较低，从而影响了MapReduce的运行速度。相比之下，Spark的运算结果中间不落地，而且兼容HDFS、Hive，实际应用中会以Spark替代MR作为常用的计算引擎。</p><h3 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h3><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。</p><h4 id="压缩的好处和坏处"><a href="#压缩的好处和坏处" class="headerlink" title="压缩的好处和坏处"></a>压缩的好处和坏处</h4><p>好处：</p><ul><li>减少磁盘存储空间</li><li>降低IO（网络的IO和磁盘的IO）</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p>坏处：</p><ul><li>由于使用数据时，需要先将数据解压，加重CPU负荷</li></ul><p>基本原则：</p><ol><li>运算密集型的job，少用压缩</li><li>IO密集型的job，多用压缩</li></ol><h4 id="压缩格式-1"><a href="#压缩格式-1" class="headerlink" title="压缩格式"></a>压缩格式</h4><p>Hadoop中的压缩格式：</p><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>扩展名</th><th>是否支持分割</th><th>Hadoop编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>N/A</td><td>DEFLATE</td><td>.deflate</td><td>No</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>gzip</td><td>DEFLATE</td><td>.gz</td><td>No</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>Yes</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>Lzop</td><td>LZO</td><td>.lzo</td><td>Yes(if index)</td><td>com.hadoop.compression.lzo.LzoCodec</td></tr><tr><td>LZ4</td><td>N/A</td><td>LZ4</td><td>.lz4</td><td>No</td><td>org.apache.hadoop.io.compress.Lz4Codec</td></tr><tr><td>Snappy</td><td>N/A</td><td>Snappy</td><td>.snappy</td><td>No</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>是否可分割是指，压缩后的文件是否可以再分割。可以分割的格式允许单一文件由多个Mapper程序同时读取，可以做到更好的并行化。</p><p>压缩比：</p><p><img src="https://vinxikk.github.io/img/dw/compress-codec-size.png" alt="压缩之后的size"></p><p>压缩时间：</p><p><img src="https://vinxikk.github.io/img/dw/compress-codec-time.png" alt="压缩时间-两次不同大小的文件"></p><p>可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</p><p>压缩格式的优缺点：</p><table><thead><tr><th>压缩格式</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>gzip</td><td>压缩比在四种压缩方式中较高；Hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分Linux系统都自带gzip命令，使用方便</td><td>不支持split</td></tr><tr><td>lzo</td><td>压缩/解压速度也比较快，合理的压缩率；支持split，是Hadoop中最流行的压缩格式；支持hadoop native库；需要在Linux系统下自行安装lzop命令，使用方便</td><td>压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td></tr><tr><td>snappy</td><td>压缩速度快；支持hadoop native库</td><td>不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td></tr><tr><td>bzip2</td><td>支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td>压缩/解压速度慢；不支持native</td></tr></tbody></table><h4 id="压缩的应用场景"><a href="#压缩的应用场景" class="headerlink" title="压缩的应用场景"></a>压缩的应用场景</h4><p>在Hadoop中的应用场景主要在三方面：输入，中间，输出。</p><p>整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce</p><ol><li>Use Compressd Map Input: 从HDFS中读取文件进行MR作业，如果数据很大，可以使用压缩并且选择支持分片的压缩方式（bzip2, LZO），可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式，例如：Sequence Files, RC, ORC等。</li><li>Compress Intermediate Data: Map输出作为Reduce的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少存储文件所占空间，提升数据传输速率。建议使用压缩速度快的压缩方式，例如：Snappy和LZO。</li><li>Compress Reducer Output: 进行归档处理或者链接MR的工作（该作业的输出作为下个作业的输入），压缩可以减少存储文件所占空间，提升数据传输速率，如果作为归档处理，可以采用高的压缩比（Gzip, bzip2），如果作为下个作业的输入，考虑是否要分片进行选择。</li></ol><h4 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h4><p>要在Hadoop中启动压缩，可以配置如下参数：</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs <br/>（在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress<br/>（在mapred-site.xml中配置）</td><td>False</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec<br/>（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress<br/>（在mapred-site.xml中配置）</td><td>False</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec<br/>（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type<br/>（在mapred-site.xml中配置）</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><hr><p>参考：</p><p><a href="https://www.cnblogs.com/sharpxiajun/p/3151395.html" target="_blank" rel="noopener">https://www.cnblogs.com/sharpxiajun/p/3151395.html</a></p><p><a href="https://blog.csdn.net/yljphp/article/details/89067858" target="_blank" rel="noopener">https://blog.csdn.net/yljphp/article/details/89067858</a></p><p><a href="https://blog.csdn.net/yu0_zhang0/article/details/79524842" target="_blank" rel="noopener">https://blog.csdn.net/yu0_zhang0/article/details/79524842</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YARN </tag>
            
            <tag> Split分片机制 </tag>
            
            <tag> Shuffle机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS副本放置策略&amp;读写流程&amp;PID文件&amp;常用命令&amp;磁盘均衡</title>
      <link href="/2018/04/04/dw/hadoop-4/"/>
      <url>/2018/04/04/dw/hadoop-4/</url>
      
        <content type="html"><![CDATA[<h3 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h3><p>假设副本数（dfs.replication）为3。</p><p>第一个副本：假如上传节点为DN节点，则存放在本节点上（假如Client不在集群范围内，则随机选取一台磁盘不太慢 、CPU不太繁忙的节点放置）。</p><p>第二个副本：存放在与第一个副本不同机架的一个节点上。</p><p>第三个副本：和第二个副本在同一个机架，放在不同的节点上。</p><h3 id="HDFS文件读写流程"><a href="#HDFS文件读写流程" class="headerlink" title="HDFS文件读写流程"></a>HDFS文件读写流程</h3><h4 id="写流程（FSDataOutputStream）"><a href="#写流程（FSDataOutputStream）" class="headerlink" title="写流程（FSDataOutputStream）"></a>写流程（FSDataOutputStream）</h4><p>Client上传文件到HDFS，也就是类似于下面的命令：</p><p>hadoop fs -put test.log /</p><p><img src="https://vinxikk.github.io/img/dw/hdfs-output.png" alt="HDFS文件写流程"></p><p>过程：</p><ol><li>Client 调用FileSystem.create(filePath)方法，与NN进行[RPC]通信，check是否存在及是否有权限创建。假如不OK，就返回错误信息；假如OK，就创建一个新文件，不关联任何的block块，返回一个FSDataOutputStream对象。</li><li>Client调用FSDataOutputStream对象的write()方法，先将第一块的第一个副本写到第一个DN，第一个副本写完就传输给第二个DN，第二个副本写完就传输给第三个DN，直至写完第三个副本。然后返回一个ack packet确认包给第二个DN，第二个DN接收到第三个的ack packet确认包加上自身OK，就返回一个ack packet确认包给第一个DN，第一个DN接收到第二个DN的ack packet确认包加上自身OK，就返回ack packet确认包给FSDataOutputStream对象，标志第一个块的3个副本写完。依次写完余下的块。</li><li>当文件写入数据完成后，Client调用FSDataOutputStream.close()方法，关闭输出流。</li><li>然后调用FileSystem.complete()方法，告诉NN该文件写入成功。</li></ol><h4 id="读流程（FSDataInputStream）"><a href="#读流程（FSDataInputStream）" class="headerlink" title="读流程（FSDataInputStream）"></a>读流程（FSDataInputStream）</h4><p>Client从HDFS下载文件到本地，类似于下面的命令：</p><p>hadoop fs -get /test.log</p><p><img src="https://vinxikk.github.io/img/dw/hdfs-input.png" alt="HDFS文件读流程"></p><p>过程：</p><ol><li><p>Client调用FileSystem.open(filePath)方法，与NN进行[RPC]通信，返回该文件的部分或者全部的block列表，也就是返回FSDataInputStream对象。</p></li><li><p>Client调用FSDataInputStream.read()方法。</p><p>a.与第一个块最近的DN进行read，读取完成后，会check；假如OK，就关闭与当前DN的通信；假如失败，会记录失败块+DN信息，下次不会再读取，会去该块的第二个DN地址读取。</p><p>b.接着去第二个块的最近的DN上通信读取，check后，关闭通信。</p><p>c.假如block列表读取完成后，文件还未结束，FileSystem会再次从NN获取该文件的下一批次的block列表。</p><p>（感觉就是连续的数据流，对于客户端操作是透明无感知的）</p></li><li><p>Client调用FSDataInputStream.close()方法，关闭输入流。</p></li></ol><h3 id="pid文件"><a href="#pid文件" class="headerlink" title="pid文件"></a>pid文件</h3><p>HADOOP和YARN的pid文件默认存储在/tmp目录下。系统会自动清理/tmp文件夹下30天不访问的文件，比如hadoop-env.sh中的配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The directory <span class="built_in">where</span> pid files are stored. /tmp by default.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> NOTE: this should be <span class="built_in">set</span> to a directory that can only be written to by </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">       the user that will run the hadoop daemons.  Otherwise there is the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">       potential <span class="keyword">for</span> a symlink attack.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span></pre></td></tr></table></figure><p>可以看到HADOOP_PID_DIR默认设置为/tmp目录下，所以需要更改pid的存放目录。</p><h4 id="创建用户目录下的tmp文件夹："><a href="#创建用户目录下的tmp文件夹：" class="headerlink" title="创建用户目录下的tmp文件夹："></a>创建用户目录下的tmp文件夹：</h4><p>mkdir /home/vinx/tmp</p><p>chmod -R 777 /home/vinx/tmp</p><h4 id="修改hadoop-env-sh："><a href="#修改hadoop-env-sh：" class="headerlink" title="修改hadoop-env.sh："></a>修改hadoop-env.sh：</h4><p>vi hadoop-env.sh</p><p>export HADOOP_PID_DIR=/home/vinx/tmp</p><h4 id="修改yarn-env-sh"><a href="#修改yarn-env-sh" class="headerlink" title="修改yarn-env.sh"></a>修改yarn-env.sh</h4><p>vi yarn-env.sh</p><p>export YARN_PID_DIR=/home/vinx/tmp</p><p>重新启动HDFS和YARN，可以看到在/home/vinx/tmp下生成了对应的pid文件。</p><h3 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h3><p>hadoop fs == hdfs dfs</p><p>查看hadoop fs命令使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hadoop fs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span></pre></td></tr></table></figure><p>查看hdfs dfs命令使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hdfs dfs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span></pre></td></tr></table></figure><p>可以看到hdfs dfs底层就是调用了hadoop fs。</p><h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><ul><li>hdfs dfs -ls /                     查看hdfs根目录下的文件</li><li>hdfs dfs -put test.log /     将本地test.log文件上传至hdfs根目录</li><li>hdfs dfs -get /test.log ./   将hdfs根目录下的test.log下载至本地当前目录下</li><li>hdfs dfs -rm /test.log       删除hdfs根目录下的test.log，如果设置了回收站，会放入回收站</li><li>hdfs dfs -rm -skipTrash /test.log    直接删除test.log</li><li>hdfs dfs -copyFromLocal test.log /  类似于上面的-put</li><li>hdfs dfs -copyToLocal /test.log ./     类似于上面的-get</li></ul><h4 id="配置回收站"><a href="#配置回收站" class="headerlink" title="配置回收站"></a>配置回收站</h4><p>cd /home/vinx/app/hadoop/etc/hadoop</p><p>vi core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 设置时间为7天，默认单位：minute</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10080<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>start-dfs.sh重新启动hdfs，执行hdfs dfs -rm /test.log，此时test.log文件并没有直接删除，而是会移动到回收站。若想直接删除文件，可以使用hdfs dfs -rm -skipTrash /test.log。CDH默认是开启回收站的。</p><h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4><p>安全模式下只能read，不能write，即只能查看或下载hdfs上的文件，不能修改或者从本地上传文件至hdfs。</p><p>进入安全模式：</p><p>hdfs dfsadmin -safemode enter</p><p>如果NN的log显示进入了safe mode，需要执行以下命令让其离开安全模式：</p><p>hdfs dfsadmin -safemode leave</p><p>注意：尽量在下游HDFS少做安全模式，而是上游来控制数据同步。</p><h4 id="文件系统的检查"><a href="#文件系统的检查" class="headerlink" title="文件系统的检查"></a>文件系统的检查</h4><p>从hdfs根目录开启检查文件系统：</p><p>hdfs fsck /</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hdfs fsck /</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">18/12/08 17:50:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Connecting to namenode via http://hadoop001:50070/fsck?ugi=vinx&amp;path=%2F</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">FSCK started by vinx (auth:SIMPLE) from /192.168.137.130 <span class="keyword">for</span> path / at Sun Dec 08 17:50:14 CST 2018</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">........Status: HEALTHY</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"> Total size:175138 B</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"> Total <span class="built_in">dirs</span>:17</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> Total files:8</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"> Total symlinks:0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"> Total blocks (validated):7 (avg. block size 25019 B)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"> Minimally replicated blocks:7 (100.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"> Over-replicated blocks:0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"> Under-replicated blocks:0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"> Mis-replicated blocks:0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"> Default replication factor:1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"> Average block replication:1.0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"> Corrupt blocks:0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"> Missing replicas:0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"> Number of data-nodes:1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"> Number of racks:1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">FSCK ended at Sun Dec 08 17:50:14 CST 2018 <span class="keyword">in</span> 36 milliseconds</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">The filesystem under path <span class="string">'/'</span> is HEALTHY</span></pre></td></tr></table></figure><p>主要关注两个指标：</p><ul><li>Corrupt blocks    损坏的块</li><li>Missing replicas  丢失的副本</li></ul><h3 id="多节点、单节点的磁盘均衡"><a href="#多节点、单节点的磁盘均衡" class="headerlink" title="多节点、单节点的磁盘均衡"></a>多节点、单节点的磁盘均衡</h3><h4 id="各DN节点的数据均衡"><a href="#各DN节点的数据均衡" class="headerlink" title="各DN节点的数据均衡"></a>各DN节点的数据均衡</h4><p>官网中hdfs-site.xml的balance配置说明：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>dfs.datanode.balance.bandwidthPerSec</td><td>10m</td><td>Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)to specify the size (such as 128k, 512m, 1g, etc.). Or provide complete size in bytes (such as 134217728 for 128 MB).</td></tr></tbody></table><p>参数指定了每个DN节点做数据均衡时，可以利用的最大带宽，单位是bytes/second。</p><p>可以看到dfs.datanode.balance.bandwidthPerSec默认值为10m，生产上一般设为30m。</p><p>默认数据均衡的阈值threshold = 10.0，即所有节点的磁盘used与集群的平均used之差要小于这个阈值。</p><p>指定阈值为10%，并启动数据均衡shell脚本：</p><p>sbin/start-balancer.sh -threshold 10</p><p>每天定时执行sbin/start-balancer.sh，做数据平衡和毛刺修正。</p><p>调度工具可以选择crontab或Azkaban。</p><h4 id="一个DN节点的多个磁盘的数据均衡"><a href="#一个DN节点的多个磁盘的数据均衡" class="headerlink" title="一个DN节点的多个磁盘的数据均衡"></a>一个DN节点的多个磁盘的数据均衡</h4><p>官网HDFS Disk Balancer：</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p><p>其中：dfs.disk.balancer.enabled must be set to true in hdfs-site.xml.即hdfs-site.xml中的dfs.disk.balancer.enabled必须设置为true。</p><p>使用步骤：</p><p>hdfs diskbalancer -plan hadoop001                             生成hadoop001.plan.json<br>hdfs diskbalancer -execute hadoop001.plan.json      执行<br>hdfs diskbalancer -query hadoop001                           查询状态</p><p>什么时候执行diskbalancer：</p><ol><li>新盘加入</li><li>监控磁盘剩余空间，小于阈值10%，发邮件预警，手动执行</li></ol><h4 id="为什么DN在生产上挂载多个物理的磁盘目录"><a href="#为什么DN在生产上挂载多个物理的磁盘目录" class="headerlink" title="为什么DN在生产上挂载多个物理的磁盘目录"></a>为什么DN在生产上挂载多个物理的磁盘目录</h4><p>/data01 disk1<br>/data02 disk2<br>/data03 disk3</p><ol><li>为了高效率读写数据</li><li>提前规划好2-3年存储量 ，避免后期加磁盘维护的工作量</li></ol><p>官网中hdfs-site.xml的相关参数：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>dfs.datanode.data.dir</td><td>file://${hadoop.tmp.dir}/dfs/data</td><td>Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. The default storage type will be DISK if the directory does not have a storage type tagged explicitly. Directories that do not exist will be created if local filesystem permission allows.</td></tr></tbody></table><p>参数指定dfs数据节点应该在本地文件系统上存储其块的位置。</p><p>比如设置dfs.datanode.data.dir = /data01,/data02,/data03,/data04，注意以逗号分隔，comma-delimited</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS架构&amp;NameNode和DataNode工作机制&amp;BLOCK和副本数&amp;小文件问题</title>
      <link href="/2018/04/01/dw/hadoop-3/"/>
      <url>/2018/04/01/dw/hadoop-3/</url>
      
        <content type="html"><![CDATA[<h3 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h3><p>HDFS主要由3个组件构成：NameNode, DataNode, SecondaryNameNode。</p><p>HDFS是以master/slave模式运行的，通常NameNode, SecondaryNameNode运行在master节点，DataNode运行在slave节点。</p><p>HDFS架构图：</p><p><img src="https://vinxikk.github.io/img/dw/hdfs-architecture.png" alt="HDFS主从架构"></p><h4 id="NameNode（名称节点）"><a href="#NameNode（名称节点）" class="headerlink" title="NameNode（名称节点）"></a>NameNode（名称节点）</h4><p>存储：文件元数据信息，包含：</p><ul><li>文件名称</li><li>文件目录结构</li><li>文件的属性（权限，创建时间，副本数）</li><li>文件对应哪些数据块 –&gt; 数据块对应哪些DN节点</li></ul><p>作用：</p><ul><li>管理文件系统命名空间</li><li>维护文件系统树及树中的所有文件和目录</li><li>维护所有这些文件或目录的打开、关闭、移动、重命名等操作</li></ul><h5 id="NameNode工作机制"><a href="#NameNode工作机制" class="headerlink" title="NameNode工作机制"></a>NameNode工作机制</h5><p><img src="https://vinxikk.github.io/img/dw/namenode-process.png" alt="NN工作机制"></p><p>第一阶段：NameNode启动</p><ol><li>第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志（edits）和镜像文件（fsimage）到内存。</li><li>客户端对元数据进行增删改的请求。</li><li>NameNode记录操作日志，更新滚动日志。</li><li>NameNode在内存中对数据进行增删改查。</li></ol><p>第二阶段：SecondaryNameNode工作</p><ol><li>SecondaryNameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。</li><li>SecondaryNameNode请求执行checkpoint。</li><li>NameNode滚动正在写的edits日志。</li><li>将滚动前的编辑日志和镜像文件拷贝到SecondaryNameNode。</li><li>SecondaryNameNode加载编辑日志和镜像文件到内存，并合并。</li><li>生成新的镜像文件fsimage.chkpoint。</li><li>拷贝fsimage.chkpoint到NameNode。</li><li>NameNode将fsimage.chkpoint重新命名成fsimage。</li></ol><p>checkpoint检查时间参数设置：</p><ol><li><p>通常情况下，SecondaryNameNode每隔1小时执行一次</p><p>[hdfs-default.xml]</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure></li><li><p>一分钟检查一次操作次数，当操作次数达到一百万时，SecondaryNameNode执行一次</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure></li></ol><h4 id="DataNode（数据节点）"><a href="#DataNode（数据节点）" class="headerlink" title="DataNode（数据节点）"></a>DataNode（数据节点）</h4><p>存储：数据块，数据块校验和，与NN通信</p><p>作用：</p><ul><li>读写文件的数据块</li><li>接收NN的指示来进行创建、删除、复制等操作</li><li>通过心跳定期向NN发送所存储文件块列表信息</li></ul><h5 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h5><p><img src="https://vinxikk.github.io/img/dw/datanode-detail.png" alt="DN工作机制"></p><ol><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据，包括数据块的长度、块数据的校验和以及时间戳。</li><li>DataNode启动后向NameNode注册，通过后，周期性（1小时）地向NameNode上报所有的块信息。</li><li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一台机器或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li><li>集群运行中可以安全加入和退出一些机器。</li></ol><h5 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h5><ol><li>当DataNode读取block的时候，它会计算checksum校验和。</li><li>如果计算后的checksum，与block创建时值不一样，说明block已经损坏。</li><li>client读取其他DataNode上的block。</li><li>DataNode在其文件创建后周期验证checksum校验和。</li></ol><h5 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h5><p>DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：</p><p>timeout  = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval</p><p>而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p><p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span> dfs.heartbeat.interval <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><h4 id="ScondaryNameNode（第二名称节点）"><a href="#ScondaryNameNode（第二名称节点）" class="headerlink" title="ScondaryNameNode（第二名称节点）"></a>ScondaryNameNode（第二名称节点）</h4><p>存储：命名空间镜像文件fsimage+编辑日志editlog</p><p>作用：</p><ul><li>定期合并fsimage+editlog文件为新的fsimage，推送给NN</li><li>监控HDFS状态，每隔一段时间获取HDFS元数据的快照</li></ul><p>为了解决单点故障，设置了SNN的1小时备份机制，虽然能够减轻单点故障，但是还会有风险，即在那1小时中，还是会有发生单点故障的可能，造成数据丢失。为了解决这个问题，需要部署HDFS -HA高可用。</p><h4 id="机架感知（副本放置策略）"><a href="#机架感知（副本放置策略）" class="headerlink" title="机架感知（副本放置策略）"></a>机架感知（副本放置策略）</h4><p>官网地址：</p><p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html</a></p><p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication</a></p><p>一个hadoop分布式集群会有很多的服务器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，机架内的服务器之间的网络速度通常都会高于跨机架服务器之间的网络速度，并且机架之间服务器的网络通信通常受到上层交换机间网络带宽的限制。</p><p><img src="https://vinxikk.github.io/img/dw/replication-and-rack-awareness.jpg" alt="副本放置策略"></p><p>HDFS对数据文件是分block存储，每个block默认有3个副本（也可以配置大于3），HDFS对副本的存放策略如下：</p><ol><li>第一个副本：放置在Client所在的DN节点上（如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的DN节点）</li><li>第二个副本：放置在与第一个副本不同的机架的节点上（随机选择）</li><li>第三个副本：与第二个副本相同机架的不同节点上</li><li>如果还有更多的副本，随机放在集群的节点中</li></ol><p>这样的策略主要是为了数据的可靠性和数据访问的性能：</p><ul><li>数据分布在不同的机架上，就算当前机架挂掉，其他机架上还有冗余备份，整个集群依然能对外服务。</li><li>数据分布在不同的机架上，运行MR任务时可以就近获取所需的数据。</li></ul><h3 id="块大小和副本数"><a href="#块大小和副本数" class="headerlink" title="块大小和副本数"></a>块大小和副本数</h3><p>块大小和副本数需要在hdfs-site.xml中配置。</p><p>官网中的相应的默认参数如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>dfs.blocksize</td><td>134217728</td><td>The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).</td></tr><tr><td>dfs.replication</td><td>3</td><td>Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.</td></tr></tbody></table><p>可以看到，默认块大小为128M，默认副本数为3.</p><p>这里会有一个常规的面试题：</p><p>问：假如一个文件300M，块128M，副本2。请问实际存储空间多大，多少块？</p><p>答：300 x 2 = 600M，3 x 2 = 6块</p><p>需要注意的是，实际存储空间=文件大小x副本数，并不是块大小，即使44M也占用一个块。</p><h3 id="HDFS小文件问题"><a href="#HDFS小文件问题" class="headerlink" title="HDFS小文件问题"></a>HDFS小文件问题</h3><p>HDFS上每个文件都要在NameNode上建立一个索引，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p><h4 id="小文件优化"><a href="#小文件优化" class="headerlink" title="小文件优化"></a>小文件优化</h4><p>小文件的优化无非以下几种方式：</p><ol><li>在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS</li><li>在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并</li></ol><p>根据上面的思想，可以考虑采用下面的解决方案：</p><ol><li><p>Hadoop Archive：</p><p>是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了namenode的内存使用。</p></li><li><p>Sequence file：</p><p>sequence file由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。</p></li><li><p>CombineFileInputFormat：</p><p>CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。</p></li><li><p>开启JVM重用：</p><p>对于大量小文件job，可以开启JVM重用，会减少45%运行时间。</p><p>JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他map。</p><p>具体设置：mapreduce.job.jvm.numtasks值在10-20之间。</p></li></ol><p>总的来说，解决小文件问题主要就是将小文件合并成大文件，一般约定：尽量使得合并后的大文件&lt;=blocksize，比如110M（假如块大小128M）。</p><p>一般在生产上会设置一个阈值，比如10M，作为小文件的门槛，并使用shell脚本调用程序进行小文件的定期合并。</p><hr><p>参考：</p><p><a href="https://www.jianshu.com/p/39254e03558d" target="_blank" rel="noopener">https://www.jianshu.com/p/39254e03558d</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS架构 </tag>
            
            <tag> NN/DN工作机制 </tag>
            
            <tag> HDFS小文件问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN伪分布式&amp;WordCount案例&amp;jps命令&amp;OOM-kill机制和/tmp目录定时清理</title>
      <link href="/2018/03/30/dw/hadoop-2/"/>
      <url>/2018/03/30/dw/hadoop-2/</url>
      
        <content type="html"><![CDATA[<h3 id="YARN伪分布式"><a href="#YARN伪分布式" class="headerlink" title="YARN伪分布式"></a>YARN伪分布式</h3><p>官网：</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node</a></p><h4 id="伪分布式部署"><a href="#伪分布式部署" class="headerlink" title="伪分布式部署"></a>伪分布式部署</h4><ol><li><p>修改如下的配置文件：</p><p>etc/hadoop/mapred-site.xml:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>etc/hadoop/yarn-site.xml:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 默认yarn的端口号是8088，可以在此修改 --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure></li><li><p>启动yarn：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ sbin/start-yarn.sh </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">3908 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">3768 ResourceManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">3865 NodeManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ netstat -nlp | grep 3768</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">(Not all processes could be identified, non-owned process info</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"> will not be shown, you would have to be root to see it all.)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::8030                     :::*                        LISTEN      3768/java           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::8031                     :::*                        LISTEN      3768/java           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::8032                     :::*                        LISTEN      3768/java           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::8033                     :::*                        LISTEN      3768/java           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 ::ffff:192.168.xxx.xxx:8088 :::*                        LISTEN      3768/java</span></pre></td></tr></table></figure><p>可以看到YARN已经正常启动。</p></li><li><p>查看web界面：</p><p>ResourceManager - <a href="http://localhost:8088/" target="_blank" rel="noopener">http://localhost:8088/</a></p><p><img src="https://vinxikk.github.io/img/dw/yarn-web.png" alt="YARN的web界面"></p></li><li><p>运行一个MR作业</p></li><li><p>停止yarn：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ sbin/stop-yarn.sh</span></pre></td></tr></table></figure></li></ol><h3 id="MR-wordcount案例"><a href="#MR-wordcount案例" class="headerlink" title="MR wordcount案例"></a>MR wordcount案例</h3><p>wordcount是大数据领域的helloworld。</p><p>下面以Hadoop中的example jar演示wordcount案例。</p><p>数据准备：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ hdfs dfs -cat /wordcount/input/words.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">hello</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">world</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">hello</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">hello spark</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">hello flink</span></pre></td></tr></table></figure><p>当前路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ <span class="built_in">pwd</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">/home/vinx/app/hadoop</span></pre></td></tr></table></figure><p>查找名称中有example的jar包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ find ./ -name <span class="string">'*example*.jar'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">./share/hadoop/mapreduce1/hadoop-examples-2.6.0-mr1-cdh5.16.2.jar</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.16.2-test-sources.jar</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.16.2-sources.jar</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar  选用这个jar包</span></pre></td></tr></table></figure><p>启动hdfs和yarn：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ sbin/start-dfs.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ sbin/start-yarn.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">4678 SecondaryNameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">4791 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">3768 ResourceManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">3865 NodeManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4378 NameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">4511 DataNode</span></pre></td></tr></table></figure><p>运行wordcount程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ hadoop \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">&gt; jar /home/vinx/app/hadoop/share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&gt; wordcount \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&gt; /wordcount/input \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">&gt; /wordcount/output02</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">命令解释：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">hadoop \           需要加\换行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">jar /home/vinx/app/hadoop/share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar \        使用的jar包</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">wordcount \            使用的主类</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">/wordcount/input \     输入数据 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">/wordcount/output02    输出路径</span></pre></td></tr></table></figure><p>程序运行成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:29 INFO mapreduce.Job: Running job: job_1575979215182_0001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:43 INFO mapreduce.Job: Job job_1575979215182_0001 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:43 INFO mapreduce.Job:  map 0% reduce 0%</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:50 INFO mapreduce.Job:  map 100% reduce 0%</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:57 INFO mapreduce.Job:  map 100% reduce 100%</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:58 INFO mapreduce.Job: Job job_1575979215182_0001 completed successfully</span></pre></td></tr></table></figure><p>查看程序计算结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ hadoop fs -cat /wordcount/output02/part-r-00000</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">flink1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">hadoop1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">hello4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">spark1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">world1</span></pre></td></tr></table></figure><h3 id="jps的真正使用"><a href="#jps的真正使用" class="headerlink" title="jps的真正使用"></a>jps的真正使用</h3><p>查看jps的位置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ <span class="built_in">which</span> jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">/usr/java/jdk1.8.0_45/bin/jps</span></pre></td></tr></table></figure><p>查看jps命令帮助：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ jps -<span class="built_in">help</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">usage: jps [-<span class="built_in">help</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">       jps [-q] [-mlvV] [&lt;hostid&gt;]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Definitions:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    &lt;hostid&gt;:      &lt;hostname&gt;[:&lt;port&gt;]</span></pre></td></tr></table></figure><p>jps查看详细进程信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ jps -l</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">4678 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">3768 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">3865 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">4378 org.apache.hadoop.hdfs.server.namenode.NameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">8077 sun.tools.jps.Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">4511 org.apache.hadoop.hdfs.server.datanode.DataNode</span></pre></td></tr></table></figure><p>查看相应进程的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># pwd</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">/tmp/hsperfdata_vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">total 160</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 3768</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 3865</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 4378</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 4511</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 4678</span></pre></td></tr></table></figure><p>root用户下使用jps命令查看当前进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># jps</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">4678 -- process information unavailable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">3768 -- process information unavailable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">3865 -- process information unavailable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">4378 -- process information unavailable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">8124 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">4511 -- process information unavailable</span></pre></td></tr></table></figure><p>可以看到，如果是进程所属的用户vinx去执行jps命令，只会显示当前用户相关的进程信息。</p><p>而root用户可以看所有的进程信息，但是不是root用户启动的进程，会显示process information unavailable。</p><p>当root用户执行jps命令后，显示process information unavailable时，我们如果确定进程是否存在呢？可以使用下面的命令查看相关进程的详细信息，比如查看进程4678的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">会显示当前命令的进程，总共2个进程</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># ps -ef | grep 4678</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">过滤自己</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># ps -ef | grep 4678 | grep -v grep</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">进程数</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># ps -ef | grep 4678 | grep -v grep | wc -l</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">1</span></pre></td></tr></table></figure><h3 id="Linux的两个机制：OOM-Killer和-tmp目录定时清理"><a href="#Linux的两个机制：OOM-Killer和-tmp目录定时清理" class="headerlink" title="Linux的两个机制：OOM-Killer和/tmp目录定时清理"></a>Linux的两个机制：OOM-Killer和/tmp目录定时清理</h3><h4 id="OOM-Killer机制"><a href="#OOM-Killer机制" class="headerlink" title="OOM-Killer机制"></a>OOM-Killer机制</h4><p>OOM Killer的全称为Out of Memory (OOM) killer，它的作用简单点说就是，当系统的内存用光的时候，系统内核会自动的Kill掉一个或者一些进程，以使系统能继续的恢复到正常的运行状态。</p><p>查看机器的内存情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">free -m</span></pre></td></tr></table></figure><p>OOM Killer每一次Kill掉进程都会在messages日志里留下记录，检查的命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cat /var/<span class="built_in">log</span>/messages | grep oom</span></pre></td></tr></table></figure><p>部分系统时中日志里的关键字为：Out of memory，对应的检查命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cat /var/<span class="built_in">log</span>/messages | grep -i <span class="string">'Out of memory'</span></span></pre></td></tr></table></figure><p>如果日志里频繁的出现OOM记录，那么就需要考虑调整服务器配置了（软件或者硬件）。</p><p>解决办法：</p><ol><li>限制java进程的max heap，从而降低内存使用</li><li>发现系统没有开启swap，给系统加swap空间（内存数x2）</li></ol><h4 id="tmp自动清理机制"><a href="#tmp自动清理机制" class="headerlink" title="/tmp自动清理机制"></a>/tmp自动清理机制</h4><p>在Linux系统中/tmp文件夹里是存放临时文件的，/tmp默认存储周期 30天，会自动清空不在规则以内的文件。</p><p>/etc/cron.daily/tmpwatch这个文件的作用就是删除/tmp目录下不在规则内且没有访问的文件文件夹，文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# cat &#x2F;etc&#x2F;cron.daily&#x2F;tmpwatch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">#! &#x2F;bin&#x2F;sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">flags&#x3D;-umc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#x2F;usr&#x2F;sbin&#x2F;tmpwatch &quot;$flags&quot; -x &#x2F;tmp&#x2F;.X11-unix -x &#x2F;tmp&#x2F;.XIM-unix \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-x &#x2F;tmp&#x2F;.font-unix -x &#x2F;tmp&#x2F;.ICE-unix -x &#x2F;tmp&#x2F;.Test-unix \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">-X &#39;&#x2F;tmp&#x2F;hsperfdata_*&#39; 10d &#x2F;tmp</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#x2F;usr&#x2F;sbin&#x2F;tmpwatch &quot;$flags&quot; 30d &#x2F;var&#x2F;tmp</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">for d in &#x2F;var&#x2F;&#123;cache&#x2F;man,catman&#125;&#x2F;&#123;cat?,X11R6&#x2F;cat?,local&#x2F;cat?&#125;; do</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    if [ -d &quot;$d&quot; ]; then</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">&#x2F;usr&#x2F;sbin&#x2F;tmpwatch &quot;$flags&quot; -f 30d &quot;$d&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    fi</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">done</span></pre></td></tr></table></figure><p>解释：</p><p>/usr/sbin/tmpwatch “$flags” 30d /var/tmp这一行的30d就决定了30天清理/tmp下不访问的文件。如果你想一天一清理的话，就把这个30d改成1d。</p><p>但有个问题要注意，如果你设置更短的时间来清理的话，比如说30分钟、10秒等，重启系统后你会发现设置无效，这是因为tmpwatch的上层目录是/etc/cron.daily/，而这个目录是一天执行一次计划任务，所以说，你设置了比一天更短的时间，他就不起作用了。</p><p>在Centos6中，系统自动清理/tmp文件夹的默认时限是30天</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YARN伪分布式 </tag>
            
            <tag> WordCount案例 </tag>
            
            <tag> jps命令 </tag>
            
            <tag> OOM-kill机制 </tag>
            
            <tag> /tmp定时清理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS伪分布式&amp;HDFS常用命令</title>
      <link href="/2018/03/27/dw/hadoop-1/"/>
      <url>/2018/03/27/dw/hadoop-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Hadoop概述"><a href="#Hadoop概述" class="headerlink" title="Hadoop概述"></a>Hadoop概述</h3><p>apache hadoop软件：</p><p>1.x 基本不用</p><p>2.x 企业主流 ==&gt; CDH5.X系列</p><p>3.x 尝试使用 ==&gt; CDH6.X系列</p><p>CDH的网址：</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a></p><p>Hadoop版本选择hadoop-2.6.0-cdh5.16.2：</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</a></p><p>选择CDH的好处：</p><p>不必考虑版本兼容性</p><h4 id="Hadoop的三大组件"><a href="#Hadoop的三大组件" class="headerlink" title="Hadoop的三大组件"></a>Hadoop的三大组件</h4><p>HDFS - 存储</p><p>MapReduce - 计算</p><p>YARN - 资源（内存、VCORE） + 作业调度</p><h3 id="HDFS伪分布式"><a href="#HDFS伪分布式" class="headerlink" title="HDFS伪分布式"></a>HDFS伪分布式</h3><p>官网：</p><p>Apache版本：</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation</a></p><p>CDH版本：</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/hadoop-project-dist/hadoop-common/SingleCluster.html</a></p><h4 id="伪分布式部署"><a href="#伪分布式部署" class="headerlink" title="伪分布式部署"></a>伪分布式部署</h4><p>Hadoop集群有三种模式，这里我们安装Hadoop的伪分布式模式。</p><ul><li>Local (Standalone) Mode 本地模式</li><li>Pseudo-Distributed Mode 伪分布式模式</li><li>Fully-Distributed Mode  分布式模式、集群模式</li></ul><p>环境要求：</p><ol><li>jdk 这里选择jdk1.8，需要在/etc/profile中全局配置JAVA_HOME</li><li>ssh 系统已默认安装（配置免密登录需要用到）</li></ol><p>使用root创建用户vinx：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">useradd vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">切换至用户vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">su - vinx</span></pre></td></tr></table></figure><p>上传压缩包并修改权限（root用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mv /tmp/hadoop-2.6.0-cdh5.16.2.tar.gz  /home/vinx/software/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">chown vinx:vinx /home/vinx/software/*</span></pre></td></tr></table></figure><p>解压（vinx用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/</span></pre></td></tr></table></figure><p>设置软连接：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ ln -s hadoop-2.6.0-cdh5.16.2 hadoop</span></pre></td></tr></table></figure><p>配置JAVA_HOME进hadoop-env.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi hadoop-env.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">更改：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.8.0_45</span></pre></td></tr></table></figure><p>修改etc/hadoop/core-site.xml - NN：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>修改etc/hadoop/hdfs-site.xml：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>配置ssh免密码登录（vinx用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">$ cp .ssh .ssh_copy</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">$ ssh-keygen</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">然后一路回车</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> .ssh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">修正权限（普通用户）</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">$ chmod 0600 ~/.ssh/authorized_keys</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">验证</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">$ ssh hadoop001 date</span></pre></td></tr></table></figure><p>配置个人环境变量（vinx用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/ruoze/app/hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin:<span class="variable">$&#123;HADOOP_HOME&#125;</span>/sbin:<span class="variable">$PATH</span></span></pre></td></tr></table></figure><p>生效环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ <span class="built_in">source</span> .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ <span class="built_in">which</span> hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">~/app/hadoop/bin/hadoop</span></pre></td></tr></table></figure><p>-format格式化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hdfs namenode -format</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">... has been successfully formatted.</span></pre></td></tr></table></figure><p>配置/etc/hadoop/slaves - DN：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi slaves</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">hadoop001</span></pre></td></tr></table></figure><p>配置/etc/hadoop/hdfs-site.xml - SNN：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi hdfs-site.xml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    &lt;value&gt;hadoop001:50090&lt;/value&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&lt;/property&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    &lt;value&gt;hadoop001:50091&lt;/value&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&lt;/property&gt;</span></pre></td></tr></table></figure><p>启动HDFS：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ start-dfs.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">151203 DataNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">151530 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">150875 NameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">151406 SecondaryNameNode</span></pre></td></tr></table></figure><p>查看HDFS的web页面：</p><p>网址：<a href="http://hadoop001:50070" target="_blank" rel="noopener">http://hadoop001:50070</a></p><p><img src="https://vinxikk.github.io/img/dw/hdfs-website.png" alt="HDFS的web界面"></p><p>停止HDFS：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ stop-dfs.sh</span></pre></td></tr></table></figure><h4 id="HDFS主从架构"><a href="#HDFS主从架构" class="headerlink" title="HDFS主从架构"></a>HDFS主从架构</h4><p>NameNode 名称节点，主节点，读写请求先经过它</p><p>DataNode 数据节点，从节点，存储数据，检索数据</p><p>SecondaryNameNode 第二名称节点，h+1 checkpoint检查点机制</p><p>大数据组件基本都是主从架构。</p><p>HBase（读写请求不经过HMaster进程）。</p><h3 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h3><p>hadoop fs == hdfs dfs</p><p>常用命令如下：</p><table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody><tr><td>hadoop fs -mkdir</td><td>创建文件夹</td></tr><tr><td>hadoop fs -put</td><td>上传文件至HDFS</td></tr><tr><td>hadoop fs -get</td><td>下载文件至Client</td></tr><tr><td>hadoop fs -cat</td><td>查看文件内容</td></tr><tr><td>hadoop fs -rm</td><td>删除文件</td></tr><tr><td>hadoop fs -ls</td><td>查看指定目录下的所有文件</td></tr></tbody></table><h3 id="JDK安装-amp-SSH-amp-etc-hosts文件"><a href="#JDK安装-amp-SSH-amp-etc-hosts文件" class="headerlink" title="JDK安装&amp;SSH&amp;/etc/hosts文件"></a>JDK安装&amp;SSH&amp;/etc/hosts文件</h3><h4 id="jdk1-8安装"><a href="#jdk1-8安装" class="headerlink" title="jdk1.8安装"></a>jdk1.8安装</h4><p>使用root用户安装，并全局配置JAVA_HOME（/etc/profile）。</p><p>创建文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/java</span></pre></td></tr></table></figure><p>解压：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tar -xzvf jdk-8u45-linux-x64.gz</span></pre></td></tr></table></figure><p>修正权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">chown -R root:root jdk1.8.0_45</span></pre></td></tr></table></figure><p>配置环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.8.0_45</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span></pre></td></tr></table></figure><p>生效环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 java]<span class="comment"># source /etc/profile</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 java]<span class="comment"># which java</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">/usr/java/jdk1.8.0_45/bin/java</span></pre></td></tr></table></figure><h4 id="ssh配置600权限"><a href="#ssh配置600权限" class="headerlink" title="ssh配置600权限"></a>ssh配置600权限</h4><p>如果是普通用户，配置ssh免密登录时需要修改~/.ssh/authorized_keys的权限为600。</p><p><img src="https://vinxikk.github.io/img/dw/ssh-600.png" alt="SSH 600权限"></p><h4 id="etc-hosts不要删除前两行"><a href="#etc-hosts不要删除前两行" class="headerlink" title="/etc/hosts不要删除前两行"></a>/etc/hosts不要删除前两行</h4><p>/etc/hosts文件配置内网IP和hostname时，不要删除前两行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 java]<span class="comment"># cat /etc/hosts</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">192.168.xxx.xxx hadoop001</span></pre></td></tr></table></figure><p>查看内网IP：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">ifconfig</span></pre></td></tr></table></figure><hr><p>参考：</p><p>故障：<a href="http://blog.itpub.net/30089851/viewspace-2127102/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2127102/</a>  </p><p>ssh多台：<a href="http://blog.itpub.net/30089851/viewspace-1992210/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-1992210/</a>  </p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS伪分布式 </tag>
            
            <tag> HDFS常用命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL的常用关键字：WHERE/GROUP BY/JOIN&amp;TopN问题</title>
      <link href="/2018/03/24/dw/sql-2/"/>
      <url>/2018/03/24/dw/sql-2/</url>
      
        <content type="html"><![CDATA[<h3 id="SQL-WHERE"><a href="#SQL-WHERE" class="headerlink" title="SQL - WHERE"></a>SQL - WHERE</h3><p>MySQL WHERE 子句的作用是有条件地从表中选取数据，常见于SELECT语句中，另外也可用于DELETE或UPDATE命令。</p><h4 id="WHERE在SELECT语句中"><a href="#WHERE在SELECT语句中" class="headerlink" title="WHERE在SELECT语句中"></a>WHERE在SELECT语句中</h4><p>假设表中有如下数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| wangwu   | product03 |     1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     5 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><ol><li><p>找出customer是zhangsan的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer='zhangsan';</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 搭配in</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer in('zhangsan');</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li><li><p>找出customer不是zhangsan的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer!='zhangsan';</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 搭配not in</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer not in('zhangsan');</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| wangwu   | product03 |     1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     5 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">3 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span></pre></td></tr></table></figure></li><li><p>给出所有购入商品为两种或两种以上的购物人的购物记录：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer in (select customer from `orders-detail` group by customer having count(*) &gt;= 2);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     5 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL拆分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">* </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="string">`orders-detail`</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">customer <span class="keyword">IN</span> ( <span class="keyword">SELECT</span> customer <span class="keyword">FROM</span> <span class="string">`orders-detail`</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> customer <span class="keyword">HAVING</span> <span class="keyword">count</span>(*) &gt;= <span class="number">2</span> );</span></pre></td></tr></table></figure></li></ol><h4 id="WHERE在UPDATE语句中"><a href="#WHERE在UPDATE语句中" class="headerlink" title="WHERE在UPDATE语句中"></a>WHERE在UPDATE语句中</h4><p>更新customer=lisi，且product=product04的count数量为6：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; update `orders-detail` set count=6 where customer='lisi' and product='product04';</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Query OK, 1 row affected (0.03 sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| wangwu   | product03 |     1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     6 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL拆分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> <span class="string">`orders-detail`</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> <span class="keyword">count</span> = <span class="number">6</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">customer = <span class="string">'lisi'</span> <span class="keyword">AND</span> product = <span class="string">'product04'</span>;</span></pre></td></tr></table></figure><h4 id="WHERE在DELETE语句中"><a href="#WHERE在DELETE语句中" class="headerlink" title="WHERE在DELETE语句中"></a>WHERE在DELETE语句中</h4><ol><li><p>删除customer为wangwu的记录：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; delete from `orders-detail` where customer='wangwu';</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Query OK, 1 row affected (0.02 sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     6 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 等价于如下SQL，搭配in</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="string">`orders-detail`</span> <span class="keyword">where</span> customer <span class="keyword">in</span> (<span class="string">'wangwu'</span>);</span></pre></td></tr></table></figure></li><li><p>删除count大于4的记录：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; delete from `orders-detail` where count &gt; 4;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Query OK, 1 row affected (0.10 sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">3 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li></ol><h3 id="SQL-GROUP-BY"><a href="#SQL-GROUP-BY" class="headerlink" title="SQL - GROUP BY"></a>SQL - GROUP BY</h3><p>GROUP BY子句根据一个或者多个类对结果集进行分组。</p><p>在分组的列上我们可以使用COUNT, SUM, AVG等聚合函数。</p><p>表数据如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- date: 当日首次登录时间</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- login: 当日登录次数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `employee_tbl`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----+--------+---------------------+--------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| id | name   | date                | login  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----+--------+---------------------+--------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">|  1 | 小明   | 2016-04-22 15:25:33 |      1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">|  2 | 小王   | 2016-04-20 15:25:47 |      3 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|  3 | 小丽   | 2016-04-19 15:26:02 |      2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|  4 | 小王   | 2016-04-07 15:26:14 |      4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|  5 | 小明   | 2016-04-11 15:26:40 |      4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|  6 | 小明   | 2016-04-04 15:26:54 |      2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----+--------+---------------------+--------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><ol><li><p>按名字分组，并统计每人的记录数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select name,count(*) from `employee_tbl` group by name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| name   | count(*) |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 小明   |        3 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 小王   |        2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 小丽   |        1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">3 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li><li><p>按名字分组，并统计每个人登录的次数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- WITH ROLLUP可以实现在分组统计基础上，再进行相同的统计（SUM,AVG,COUNT...）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select name,sum(login) as login_count from `employee_tbl` group by name with rollup;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| name   | login_count  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 小丽   |            2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 小明   |            7 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 小王   |            7 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| NULL   |           16 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- NULL表示所有人的登录次数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 可以使用 coalesce 来设置一个可以取代NUll的名称，coalesce 语法：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">coalesce</span>(a,b,c);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 说明：如果a==null，则选择b；如果b==null，则选择c；如果a!=null，则选择a；如果abc都为null，则返回null</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 如下：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select coalesce(name, '总数'),sum(login) as login_count from `employee_tbl` group by name with rollup;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------------------------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">| coalesce(name, '总数')   | login_count  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------------------------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">| 小丽                     |            2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">| 小明                     |            7 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">| 小王                     |            7 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">| 总数                     |           16 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------------------------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li></ol><h3 id="SQL-JOIN"><a href="#SQL-JOIN" class="headerlink" title="SQL - JOIN"></a>SQL - JOIN</h3><p>MySQL中常用的JOIN有三种：left join, right join, inner join。</p><p>left join: 左连接，以左表为主，显示左表的全部记录；右表是匹配的，匹配不到就以NULL显示。</p><p>right join: 右连接，以右表为主，显示其全部记录；左表是匹配的，匹配不到就以NULL显示，正好与left join相反。</p><p>inner join: 内连接，根据某个字段关联，只显示左右表中都存在的记录。</p><p>具体见我的另一篇博客：</p><p>[MySQL的JOIN] <a href="https://vinxikk.github.io/https:/vinxikk.github.io/2017/05/15/mysql/mysql-join/">https://vinxikk.github.io/https:/vinxikk.github.io/2017/05/15/mysql/mysql-join/</a></p><h3 id="MySQL的TopN问题"><a href="#MySQL的TopN问题" class="headerlink" title="MySQL的TopN问题"></a>MySQL的TopN问题</h3><p>需要创建视图，将查询结果保存为视图，后续在视图中继续执行SQL查询。</p><p>可以参考：</p><p><a href="https://vinxikk.github.io/https:/vinxikk.github.io/2018/01/15/mysql/mysql-example-topn/">https://vinxikk.github.io/https:/vinxikk.github.io/2018/01/15/mysql/mysql-example-topn/</a></p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL-WHERE </tag>
            
            <tag> SQL-GROUP BY </tag>
            
            <tag> SQL-JOIN </tag>
            
            <tag> MySQL案例 </tag>
            
            <tag> MySQL-TopN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL权限</title>
      <link href="/2018/03/21/dw/sql-1/"/>
      <url>/2018/03/21/dw/sql-1/</url>
      
        <content type="html"><![CDATA[<h3 id="赋予用户MySQL权限"><a href="#赋予用户MySQL权限" class="headerlink" title="赋予用户MySQL权限"></a>赋予用户MySQL权限</h3><p>创建数据库：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> testdb;</span></pre></td></tr></table></figure><p>赋予权限：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> vinx@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'123456'</span>;</span></pre></td></tr></table></figure><p>刷新权限：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL权限 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux软连接&amp;crontab使用&amp;rundeck部署</title>
      <link href="/2018/03/20/dw/linux-5/"/>
      <url>/2018/03/20/dw/linux-5/</url>
      
        <content type="html"><![CDATA[<h3 id="crontab使用"><a href="#crontab使用" class="headerlink" title="crontab使用"></a>crontab使用</h3><h4 id="shell脚本编写"><a href="#shell脚本编写" class="headerlink" title="shell脚本编写"></a>shell脚本编写</h4><p>新建test.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi test.sh</span></pre></td></tr></table></figure><p>编辑以下内容并保存：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">date</span></pre></td></tr></table></figure><p>此时，如果直接执行，会提示没有权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll ./test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--. 1 root root 18 Dec 14 20:31 ./test.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ./test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-bash: ./test.sh: Permission denied</span></pre></td></tr></table></figure><p>修改权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># chmod 744 test.sh </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll ./test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">-rwxr--r--. 1 root root 18 Dec 14 20:31 ./test.sh</span></pre></td></tr></table></figure><p>执行脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ./test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:37:18 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 或者</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># sh test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:38:21 CST 2017</span></pre></td></tr></table></figure><h4 id="crontab编写"><a href="#crontab编写" class="headerlink" title="crontab编写"></a>crontab编写</h4><p>crontab在线工具：</p><p><a href="https://tool.lu/crontab" target="_blank" rel="noopener">https://tool.lu/crontab</a></p><p>查看crontab命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># crontab --help</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">crontab: invalid option -- <span class="string">'-'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">crontab: usage error: unrecognized option</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">usage:crontab [-u user] file</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">crontab [-u user] [ -e | -l | -r ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(default operation is replace, per 1003.2)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-e(edit user<span class="string">'s crontab)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="string">-l(list user'</span>s crontab)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">-r(delete user<span class="string">'s crontab)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">-i(prompt before deleting user'</span>s crontab)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">-s(selinux context)</span></pre></td></tr></table></figure><p>编辑crontab：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">crontab -e</span></pre></td></tr></table></figure><p>填写如下内容并保存：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">* * * * * &#x2F;root&#x2F;test.sh &gt;&gt; &#x2F;root&#x2F;test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">分 时 日 周 月</span></pre></td></tr></table></figure><p>监控日志文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># tail -F test.log</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:50:02 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:51:01 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:52:01 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:53:01 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">...</span></pre></td></tr></table></figure><h4 id="crontab每隔10s执行一次"><a href="#crontab每隔10s执行一次" class="headerlink" title="crontab每隔10s执行一次"></a>crontab每隔10s执行一次</h4><p>新建shell脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi test.sh</span></pre></td></tr></table></figure><p>编写以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">for((i&#x3D;1;i&lt;&#x3D;6;i++));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">do</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        date</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        sleep 10s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">done</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">exit</span></pre></td></tr></table></figure><p>编辑crontab：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">crontab -e</span></pre></td></tr></table></figure><p>每隔1分钟执行一次：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">* * * * * &#x2F;root&#x2F;test.sh &gt;&gt; &#x2F;root&#x2F;test.log</span></pre></td></tr></table></figure><p>监控日志文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># tail -F test.log </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:05:02 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:01 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:11 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:21 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:31 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:41 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">...</span></pre></td></tr></table></figure><h4 id="脚本后台执行"><a href="#脚本后台执行" class="headerlink" title="脚本后台执行"></a>脚本后台执行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">./test.sh &amp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">nohup ./test.sh &amp;  手动启动脚本，看日志，开发维护、测试</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">nohup ./test.sh &gt; /root/test.log 2&gt;&amp;1 &amp;  生产上</span></pre></td></tr></table></figure><h3 id="Linux软连接"><a href="#Linux软连接" class="headerlink" title="Linux软连接"></a>Linux软连接</h3><p>应用场景：</p><ol><li>软件版本升级</li><li>日志转移：系统盘 ==&gt; 数据盘</li></ol><h4 id="软件版本升级"><a href="#软件版本升级" class="headerlink" title="软件版本升级"></a>软件版本升级</h4><p>比如原来的mysql5.6需要升级到5.7，如果设置了软连接：</p><p>ln -s mysql5.6 mysql</p><p>此时只要删除mysql5.6，安装上5.7，并且重新设置软连接即可：</p><p>ln -s mysql5.7 mysql</p><h4 id="日志转移"><a href="#日志转移" class="headerlink" title="日志转移"></a>日志转移</h4><p>假设：</p><p>系统盘 / 50G</p><p>数据盘 /data01 2T</p><p>一般CDH log:</p><p>​    /var/log/hbase/xxx01.log 1G</p><p>​    /var/log/hbase/xxx02.log 1G</p><p>​    …</p><p>​    /var/log/hbase/xxx10.log 1G</p><p>此时系统盘中的日志需要转移到数据盘，那么：</p><p>​    mkdir /data01/log/</p><p>​    mv /var/log/hbase /data01/log/</p><p>​    ln -s /data01/log/hbase /var/log/hbase</p><p>但是，需要注意权限的问题。</p><h3 id="rundeck部署"><a href="#rundeck部署" class="headerlink" title="rundeck部署"></a>rundeck部署</h3><p>官网：</p><p><a href="https://www.rundeck.com/" target="_blank" rel="noopener">https://www.rundeck.com/</a></p><p>官网下载页（选择社区版）：</p><p><a href="https://www.rundeck.com/open-source/download" target="_blank" rel="noopener">https://www.rundeck.com/open-source/download</a></p><p>环境要求：</p><p>JDK1.8</p><h4 id="运行步骤"><a href="#运行步骤" class="headerlink" title="运行步骤"></a>运行步骤</h4><p>启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">nohup java -jar rundeck-launcher-2.10.1.jar &amp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">tail -F nohup.out</span></pre></td></tr></table></figure><p>web界面：</p><p><a href="https://hadoop001:4440" target="_blank" rel="noopener">https://hadoop001:4440</a></p><p>默认用户和密码都是admin</p><p>新建项目：</p><p><img src="https://vinxikk.github.io/img/dw/rundeck-create-project.png" alt="rundeck新建项目"></p><p>填写项目名称，其他保持默认，然后点击Create。</p><p><img src="https://vinxikk.github.io/img/dw/rundeck-save.png" alt="rundeck保存项目"></p><p>选择默认配置，点击Save。</p><p>新建Job：</p><p><img src="https://vinxikk.github.io/img/dw/rundeck-new-job.png" alt="rundeck新建Job"></p><p>进入testpro项目，在右上角点击Create Job即可新建Job。</p><p>停止：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">ps -ef | grep rundeck</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 pid</span></pre></td></tr></table></figure><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux软连接 </tag>
            
            <tag> crontab </tag>
            
            <tag> rundeck </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL二进制部署&amp;DBeaver安装</title>
      <link href="/2018/03/20/dw/mysql-install/"/>
      <url>/2018/03/20/dw/mysql-install/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL二进制部署"><a href="#MySQL二进制部署" class="headerlink" title="MySQL二进制部署"></a>MySQL二进制部署</h3><p>生产上，一般：</p><p>mysql服务 ==&gt; mysql用户去维护</p><p>hadoop服务 ==&gt; hadoop用户</p><p>CDH的hdfs服务 ==&gt; hdfs用户</p><p>hbase服务 ==&gt; hbase用户</p><p>安装步骤：</p><ol><li><p>上传安装包（root用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传mysql-5.6.23的安装包至此目录中</span></span></pre></td></tr></table></figure></li><li><p>检查是否已安装了mysql（root用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看mysql进程</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">ps -ef | grep mysqld</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看mysql相关的rpm包</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">rpm -qa | grep -i mysql</span></pre></td></tr></table></figure></li><li><p>解压tar包（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tar xzvf mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置软连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ln -s mysql-5.6.23-linux-glibc2.5-x86_64 mysql</span></pre></td></tr></table></figure></li><li><p>新建用户和用户组（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">groupadd -g 101 dba</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">useradd -u 514 -g dba -G root -d /usr/<span class="built_in">local</span>/mysql mysqladmin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时su - mysqladmin，会出现样式丢失，执行以下命令修复问题：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">cp /etc/skel/.* /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时再次su - mysqladmin，可以看到修复了样式丢失的问题</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">id mysqladmin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改用户密码</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">passwd mysqladmin</span></pre></td></tr></table></figure></li><li><p>配置文件（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cp /etc/my.cnf /etc/my.cnf20180320</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">vi /etc/my.cnf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># gg dG清空文件，并拷贝如下内容：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[client]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">port            = 3306</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">socket          = /usr/<span class="built_in">local</span>/mysql/data/mysql.sock</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">[mysqld]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">port            = 3306</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">socket          = /usr/<span class="built_in">local</span>/mysql/data/mysql.sock</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">skip-external-locking</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">key_buffer_size = 256M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">sort_buffer_size = 2M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">read_buffer_size = 2M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">read_rnd_buffer_size = 4M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">query_cache_size= 32M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">max_allowed_packet = 16M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">myisam_sort_buffer_size=128M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">tmp_table_size=32M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">table_open_cache = 512</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">thread_cache_size = 8</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">wait_timeout = 86400</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">interactive_timeout = 86400</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">max_connections = 600</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Try number of CPU's*2 for thread_concurrency</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">thread_concurrency = 32</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#isolation level and default engine </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">default-storage-engine = INNODB</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">transaction-isolation = READ-COMMITTED</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">server-id  = 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">basedir     = /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">datadir     = /usr/<span class="built_in">local</span>/mysql/data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">pid-file     = /usr/<span class="built_in">local</span>/mysql/data/hostname.pid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#open performance schema</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">log</span>-warnings</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">sysdate-is-now</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">binlog_format = MIXED</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">log_bin_trust_function_creators=1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">log</span>-error  = /usr/<span class="built_in">local</span>/mysql/data/hostname.err</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">log</span>-bin=/usr/<span class="built_in">local</span>/mysql/arch/mysql-bin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#other logs</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#general_log =1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#general_log_file  = /usr/local/mysql/data/general_log.err</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#slow_query_log=1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#slow_query_log_file=/usr/local/mysql/data/slow_log.err</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#for replication slave</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#log-slave-updates </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#sync_binlog = 1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#for innodb options </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">innodb_data_home_dir = /usr/<span class="built_in">local</span>/mysql/data/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">innodb_data_file_path = ibdata1:500M:autoextend</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">innodb_log_group_home_dir = /usr/<span class="built_in">local</span>/mysql/arch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">innodb_log_files_in_group = 2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">innodb_log_file_size = 200M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#product tuning</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">innodb_buffer_pool_size = 1024M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">innodb_additional_mem_pool_size = 50M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">innodb_log_buffer_size = 16M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">innodb_lock_wait_timeout = 100</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#innodb_thread_concurrency = 0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">innodb_flush_log_at_trx_commit = 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">innodb_locks_unsafe_for_binlog=1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#innodb io features: add for mysql5.5.8</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line">performance_schema</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line">innodb_read_io_threads=4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">innodb-write-io-threads=4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line">innodb-io-capacity=200</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#purge threads change default(0) to 1 for purge</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line">innodb_purge_threads=1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line">innodb_use_native_aio=on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#case-sensitive file names and separate tablespace</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line">innodb_file_per_table = 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line">lower_case_table_names=1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">[mysqldump]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">quick</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">max_allowed_packet = 16M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line">[mysql]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line">no-auto-rehash</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line">[mysqlhotcopy]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line">interactive-timeout</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line">[myisamchk]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line">key_buffer_size = 256M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line">sort_buffer_size = 256M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line">read_buffer = 2M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">write_buffer = 2M</span></pre></td></tr></table></figure></li><li><p>修改权限（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">chown mysqladmin:dba /etc/my.cnf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">chmon 640 /etc/my.cnf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">chown -R mysqladmin:dba /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">chown -R mysqladmin:dba /usr/<span class="built_in">local</span>/mysql/*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">chown -R mysqladmin:dba /usr/<span class="built_in">local</span>/mysql-5.6.23-linux-glibc2.5-x86_64</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">chmod -R 755 /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">chmod -R 755 /usr/<span class="built_in">local</span>/mysql/*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">chmod -R 755 /usr/<span class="built_in">local</span>/mysql-5.6.23-linux-glibc2.5-x86_64</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">su - mysqladmin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 放binlog归档文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">$ mkdir arch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">$ scripts/mysql_install_db  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">--user=mysqladmin \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">--basedir=/usr/<span class="built_in">local</span>/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">--datadir=/usr/<span class="built_in">local</span>/mysql/data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#缺少libaio.so包</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">Installing MySQL system tables..../bin/mysqld: error <span class="keyword">while</span> loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># root用户安装缺少的包，然后再重新执行命令</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">yum install -y perl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">yum install -y autoconf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">yum install -y libaio</span></pre></td></tr></table></figure></li><li><p>配置mysql service and boot auto start（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">cp support-files/mysql.server /etc/rc.d/init.d/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">chmod +x /etc/rc.d/init.d/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">chkconfig --del mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">chkconfig --add mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">chkconfig --level 345 mysql on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">vi /etc/rc.local</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加以下内容：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">su - mysqladmin -c <span class="string">"/etc/init.d/mysql start --federated"</span></span></pre></td></tr></table></figure></li><li><p>启动mysql并查看进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">su - mysqladmin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">$ rm -rf my.cnf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">$ mysqld_safe &amp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">$ service mysql start</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">$ service mysql status</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">$ mysql -uroot -p</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">$ ps -ef | grep mysqld</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">$ netstat -tulnp | grep mysql</span></pre></td></tr></table></figure></li></ol><h3 id="MySQL设置"><a href="#MySQL设置" class="headerlink" title="MySQL设置"></a>MySQL设置</h3><p>使用mysqladmin用户，mysql -uroot -p登录mysql，然后：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">password</span>,host <span class="keyword">from</span> <span class="keyword">user</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置root用户的密码</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'password'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'root'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除空账号</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">''</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span></pre></td></tr></table></figure><p>设置环境变量（mysqladmin用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加以下内容：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MYSQL_HOME=/usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$MYSQL_HOME</span>/bin:<span class="variable">$PATH</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">PS1=`uname -n`<span class="string">":"</span><span class="string">'$USER'</span><span class="string">":"</span><span class="string">'$PWD'</span><span class="string">":&gt;"</span>; <span class="built_in">export</span> PS1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生效环境变量</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看环境变量是否生效</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="variable">$MYSQL_HOME</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">which</span> mysql</span></pre></td></tr></table></figure><h3 id="重新部署"><a href="#重新部署" class="headerlink" title="重新部署"></a>重新部署</h3><p>假如部署出了问题，需要重新部署（mysqladmin用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">rm -rf arch/*  <span class="comment">#binlog文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">rm -rf data/*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--user=mysqladmin \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--basedir=/usr/<span class="built_in">local</span>/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--datadir=/usr/<span class="built_in">local</span>/mysql/data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">service mysql start</span></pre></td></tr></table></figure><h3 id="DBeaver部署"><a href="#DBeaver部署" class="headerlink" title="DBeaver部署"></a>DBeaver部署</h3><p>下载社区版：<a href="https://dbeaver.io/download/" target="_blank" rel="noopener">https://dbeaver.io/download/</a></p><p>需要先安装JDK1.8，然后下载社区版安装即可（可能需要安装connector）。</p><p>配置权限：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 赋予root用户所有数据库的权限</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- % 任意ip访问</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> root@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'xxx'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 刷新权限</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span></pre></td></tr></table></figure><hr><p>参考：</p><p><a href="https://github.com/Hackeruncle/mysql" target="_blank" rel="noopener">https://github.com/Hackeruncle/mysql</a>  </p><p><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt</a></p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL部署 </tag>
            
            <tag> DBeaver安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux vi&amp;进程和端口号&amp;连接拒绝&amp;高危命令&amp;安装卸载和压缩解压</title>
      <link href="/2018/03/17/dw/linux-4/"/>
      <url>/2018/03/17/dw/linux-4/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux-vi"><a href="#Linux-vi" class="headerlink" title="Linux vi"></a>Linux vi</h3><p><img src="https://vinxikk.github.io/img/linux/vi.png" alt="Linux vi的三种模式"></p><p>编辑文件的流程：</p><ol><li>vi test.log新建并打开一个文件，此时按i进入编辑模式（或者按o跳到光标下一行，并进入编辑模式）</li><li>编辑文件后，按esc退出编辑模式，然后shift+:进入尾行模式，输入wq后回车，保存退出</li><li>此时可以cat test.log查看编辑后的文件内容</li></ol><h4 id="尾行模式常用命令"><a href="#尾行模式常用命令" class="headerlink" title="尾行模式常用命令"></a>尾行模式常用命令</h4><ul><li>:/ERROR 回车，会定位为ERROR的位置，按N键寻找下一个</li><li>:set nu  显示行号</li><li>:set nonu  不显示行号</li></ul><h4 id="命令模式常用命令"><a href="#命令模式常用命令" class="headerlink" title="命令模式常用命令"></a>命令模式常用命令</h4><ul><li>dd  删除当前行</li><li>dG  删除光标所在行和以下的所有行</li><li>ndd  删除光标所在及以下的n行</li><li>gg  跳转到第一行的第一个字母</li><li>G  跳转到最后一行的第一个字母</li><li>shift + $  行尾</li><li>gg + dG  清空文件内容</li></ul><h4 id="编辑配置文件注意备份"><a href="#编辑配置文件注意备份" class="headerlink" title="编辑配置文件注意备份"></a>编辑配置文件注意备份</h4><p>在编辑配置文件的时候，需要将以前的配置文件备份，再添加新的配置文件，以防不测。</p><p>比如原来的配置文件core-site.xml需要修改其中的参数，可以先备份再编辑：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cp core-site.xml core-site.xml20180317</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">vi core-site.xml</span></pre></td></tr></table></figure><p>这样当想要回退配置文件的时候，可以及时找到。</p><h3 id="查看系统资源"><a href="#查看系统资源" class="headerlink" title="查看系统资源"></a>查看系统资源</h3><p>查看磁盘：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># df -h</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">/dev/sda3        38G  7.6G   28G  22% /</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">tmpfs          1000M   80K 1000M   1% /dev/shm</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">/dev/sda1       194M   34M  151M  19% /boot</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">/dev/sr0        4.2G  4.2G     0 100% /media/CentOS_6.5_Final</span></pre></td></tr></table></figure><p>查看内存和CPU：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># free -m</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">             total       used       free     shared    buffers     cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Mem:          1998       1365        633          0         59        700</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-/+ buffers/cache:        605       1393</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Swap:         2047          0       2047</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># free -g</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">             total       used       free     shared    buffers     cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">Mem:             1          1          0          0          0          0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">-/+ buffers/cache:          0          1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">Swap:            1          0          1</span></pre></td></tr></table></figure><p>查看系统负载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># top</span></span></pre></td></tr></table></figure><h3 id="查看进程和端口号"><a href="#查看进程和端口号" class="headerlink" title="查看进程和端口号"></a>查看进程和端口号</h3><p>查看所有进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ps -ef</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root         1     0  0 20:28 ?        00:00:02 /sbin/init</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root         2     0  0 20:28 ?        00:00:00 [kthreadd]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">root         3     2  0 20:28 ?        00:00:00 [migration/0]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">root         4     2  0 20:28 ?        00:00:00 [ksoftirqd/0]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">root         5     2  0 20:28 ?        00:00:00 [migration/0]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">root         6     2  0 20:28 ?        00:00:00 [watchdog/0]</span></pre></td></tr></table></figure><p>查看ssh相关的进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ps -ef | grep ssh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root      1455     1  0 20:28 ?        00:00:00 /usr/sbin/sshd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root      3655  1455  0 20:30 ?        00:00:01 sshd: root@pts/0 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root      4558  1455  0 22:06 ?        00:00:04 sshd: root@notty </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">root      4562  4558  0 22:06 ?        00:00:02 /usr/libexec/openssh/sftp-server</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">root      5239  3659  0 23:50 pts/0    00:00:00 grep ssh</span></pre></td></tr></table></figure><p>查看ssh相关进程并过滤当前查找进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ps -ef | grep ssh | grep -v grep</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root      1455     1  0 20:28 ?        00:00:00 /usr/sbin/sshd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root      3655  1455  0 20:30 ?        00:00:01 sshd: root@pts/0 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root      4558  1455  0 22:06 ?        00:00:04 sshd: root@notty </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">root      4562  4558  0 22:06 ?        00:00:02 /usr/libexec/openssh/sftp-server</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">进程用户 进程pid 父id                             进程的内容（进程所属的目录，<span class="built_in">log</span>, -Xmx, -Xms）</span></pre></td></tr></table></figure><p>查看指定进程的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># netstat -nlp | grep 1455</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      1455/sshd           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::22                       :::*                        LISTEN      1455/sshd</span></pre></td></tr></table></figure><h3 id="Connection-refused问题"><a href="#Connection-refused问题" class="headerlink" title="Connection refused问题"></a>Connection refused问题</h3><p>场景：Centos部署大数据组件时，发生错误：Connection refused。</p><p>解决方法：</p><ol><li><p>ping ip</p></li><li><p>telnet ip port</p></li><li><p>检查防火墙</p></li></ol><p>常见问题：如何登录A服务器，打开xxx软件的web界面？</p><p>解答：寻找ip和端口号</p><p>ps -ef | grep xxx</p><p>netstat -nlp | grep pid ==&gt; port</p><h3 id="高危命令"><a href="#高危命令" class="headerlink" title="高危命令"></a>高危命令</h3><h4 id="删除命令"><a href="#删除命令" class="headerlink" title="删除命令"></a>删除命令</h4><p>在生产中要谨慎使用rm -rf，一不小心就是删库到跑路。</p><h4 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><p>一般先做备份：cp core-site.xml core-site.xml20180317</p><p>然后再编辑：vi core-site.xml</p><h4 id="杀掉进程"><a href="#杀掉进程" class="headerlink" title="杀掉进程"></a>杀掉进程</h4><p>杀进程之前，先ps -ef找到相关的进程，搞清楚哪些是要杀的，不然造成生产事故。</p><p>相关命令：</p><ul><li>ps -ef | grep keyword  寻找keyword相关进程</li><li>echo $(pgrep -f keyword)  显示相关进程的进程号</li><li>kill -9 pid  杀掉进程号为pid的进程</li><li>kill -9 $(pgrep -f keyword)  杀掉keyword相关的所有进程</li></ul><h3 id="安装卸载和压缩解压"><a href="#安装卸载和压缩解压" class="headerlink" title="安装卸载和压缩解压"></a>安装卸载和压缩解压</h3><p>yum相关命令：</p><ul><li><p>yum search xxx  搜索</p></li><li><p>yum install -y xxx  安装</p></li><li><p>yum remove -y xxx  卸载</p></li><li><p>yum –help  命令帮助</p></li><li><p>man yum  命令帮助</p></li></ul><p>rpm命令：</p><ul><li>rpm -qa | grep http  搜索</li><li>rpm -e httpd-2.4.6-90.el7.centos.x86_64  卸载</li><li>rpm -e –nodeps httpd-tools-2.4.6-90.el7.centos.x86_64  不校验，直接删除</li></ul><p>wget下载安装包：</p><ul><li>wget <a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</a>  下载到当前目录</li></ul><p>压缩解压：</p><ul><li>yum install -y zip unzip  安装zip和unzip</li><li>zip -r result.zip ./*  压缩当前目录下的所有文件进result.zip</li><li>zip -r result.zip data/*  压缩data目录下的所有文件进result.zip</li><li>unzip result.zip  解压result.zip</li><li>tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz  解压</li><li>tar -czvf hadoop-2.6.0-cdh5.16.2.tar.gz  hadoop-2.6.0-cdh5.16.2/*  压缩</li></ul><p>命令帮助tar –help:<br>  tar -cf archive.tar foo bar  # Create archive.tar from files foo and bar.<br>  tar -tvf archive.tar         # List all files in archive.tar verbosely.<br>  tar -xf archive.tar          # Extract all files from archive.tar.</p><hr><p>参考：</p><p><a href="http://blog.itpub.net/30089851/viewspace-2131678/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2131678/</a></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux常用命令 </tag>
            
            <tag> Linux vi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux用户用户组&amp;/etc/passwd文件&amp;权限拒绝</title>
      <link href="/2018/03/16/dw/linux-3/"/>
      <url>/2018/03/16/dw/linux-3/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux用户用户组"><a href="#Linux用户用户组" class="headerlink" title="Linux用户用户组"></a>Linux用户用户组</h3><p>相关命令：</p><ul><li>ll /usr/sbin/user*  查看用户相关命令</li><li>ll /usr/sbin/group*  查看用户组相关命令</li><li>useradd vinx  创建普通用户vinx，默认创建同名用户组vinx，且设置这个用户主组为vinx</li><li>id vinx  查看用户vinx相关信息</li><li>userdel vinx  删除用户vinx</li></ul><h4 id="切换用户丢失样式"><a href="#切换用户丢失样式" class="headerlink" title="切换用户丢失样式"></a>切换用户丢失样式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟样式丢失</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># ll -a</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">total 12</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">drwx------  2 vinx vinx  59 Nov 16 21:16 .</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx  18 Apr 11  2018 .bash_logout</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx 193 Apr 11  2018 .bash_profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx 231 Apr 11  2018 .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># rm -rf .bash*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su  - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Last login: Sat Nov 16 21:29:09 CST 2017 on pts/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">-bash-4.2$ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">-bash-4.2$ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修正样式</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># cp /etc/skel/.* /home/vinx/</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">cp: omitting directory ‘/etc/skel/.’</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">cp: omitting directory ‘/etc/skel/..’</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># ll -a</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">total 12</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">drwx------  2 vinx vinx  59 Nov 16 21:32 .</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root  root   18 Nov 16 21:32 .bash_logout</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root  root  193 Nov 16 21:32 .bash_profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root  root  231 Nov 16 21:32 .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># chown vinx:vinx .bash*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># ll -a</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">total 12</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">drwx------  2 vinx vinx  59 Nov 16 21:32 .</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx  18 Nov 16 21:32 .bash_logout</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx 193 Nov 16 21:32 .bash_profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx 231 Nov 16 21:32 .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">Last login: Sat Nov 16 21:30:23 CST 2017 on pts/2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$</span></pre></td></tr></table></figure><h4 id="修改用户组"><a href="#修改用户组" class="headerlink" title="修改用户组"></a>修改用户组</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">添加vinx用户到另外一个组 bigdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># groupadd bigdata</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># cat /etc/group |grep bigdata</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">bigdata:x:1003:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># usermod -a -G bigdata vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">uid=1002(vinx) gid=1002(vinx) groups=1002(vinx),1003(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">修改bigdata为vinx的主组</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># usermod -a -G bigdata vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">uid=1002(vinx) gid=1002(vinx) groups=1002(vinx),1003(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># usermod --gid  bigdata vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">uid=1002(vinx) gid=1003(bigdata) groups=1003(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># usermod -a -G vinx vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">uid=1002(vinx) gid=1003(bigdata) groups=1003(bigdata),1002(vinx)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment">#</span></span></pre></td></tr></table></figure><h4 id="设置密码"><a href="#设置密码" class="headerlink" title="设置密码"></a>设置密码</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># passwd vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Changing password <span class="keyword">for</span> user vinx.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">New password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">BAD PASSWORD: The password is shorter than 8 characters</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Retype new password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">passwd: all authentication tokens updated successfully.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment">#</span></span></pre></td></tr></table></figure><h4 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h4><p>相关命令：</p><ul><li><p>su vinx</p></li><li><p>su - vinx  切换至该用户的家目录，且执行环境变量文件</p></li></ul><p>.bash_profile文件  su vinx不会执行，su - vinx 执行<br>.bashrc文件       su vinx和su - vinx都会执行</p><h4 id="普通用户获取root的最大权限"><a href="#普通用户获取root的最大权限" class="headerlink" title="普通用户获取root的最大权限"></a>普通用户获取root的最大权限</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改以下内容</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">vinx ALL=(root) NOPASSWD:ALL</span></pre></td></tr></table></figure><p>root用户创建文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo "www.example.com" &gt;&gt; test.log</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 18 Nov 16 21:58 test.log</span></pre></td></tr></table></figure><p>vinx用户验证sudo命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 root]$ sudo ls -l</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 18 Nov 16 21:58 test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 root]$ ls -l</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">ls: cannot open directory .: Permission denied</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 root]$ cat test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">cat: test.log: Permission denied</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 root]$ sudo cat test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">www.example.com</span></pre></td></tr></table></figure><h4 id="etc-profile和-bash-profile区别"><a href="#etc-profile和-bash-profile区别" class="headerlink" title="/etc/profile和~/.bash_profile区别"></a>/etc/profile和~/.bash_profile区别</h4><p><code>/etc/profile</code>：</p><p>为系统的每个用户设置环境信息和启动程序，当用户第一次登录时，该文件被执行，其配置对所有登录的用户都有效。当被修改时，重启或使用命令<code>source /etc/profile</code>才会生效。英文描述：System wide environment and startup programs, for login setup.</p><p><code>~/.bash_profile</code>：</p><p>为当前用户设置专属的环境信息和启动程序，当用户登录时该文件执行一次。默认情况下，它用于设置环境变量，并执行当前用户的<code>.bashrc</code>文件。理念类似于<code>/etc/profile</code>，只不过只对当前用户有效，需要重启或使用命令<code>source ~/.bash_profile</code>才能生效。（注意：Centos7系统命名为<code>.bash_profile</code>，其他系统可能是<code>.bash_login</code>或<code>.profile</code>）</p><h3 id="etc-passwd文件"><a href="#etc-passwd文件" class="headerlink" title="/etc/passwd文件"></a>/etc/passwd文件</h3><p>查看/etc/passwd文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi /etc/passwd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定位到vinx用户一行</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">vinx:x:1002:1003::/home/vinx:/bin/bash</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># /bin/bash解释器</span></span></pre></td></tr></table></figure><p>假如修改/etc/passwd文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vinx:x:1002:1003::/home/vinx:/bin/<span class="literal">false</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Last login: Sat Nov 16 21:57:32 CST 2017 on pts/0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">vinx:x:1002:1003::/home/vinx:/sbin/nologin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">Last login: Sat Nov 16 22:08:52 CST 2017 on pts/0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">This account is currently not available.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment">#</span></span></pre></td></tr></table></figure><p>此时会发现切换用户失败。</p><p>场景：CDH中有很多组件，一般每一个组件都会创建一个相应的用户去管理，比如：</p><p>hdfs组件 ==&gt; hdfs用户</p><p>yarn组件 ==&gt; yarn用户</p><p>hbase组件 ==&gt; hbase用户</p><p>假如在切换用户中，发现切换不过去，需要修改/etc/passwd文件为/bin/bash。</p><h3 id="Permission-denied问题"><a href="#Permission-denied问题" class="headerlink" title="Permission denied问题"></a>Permission denied问题</h3><p>查看文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x 2 root root  6 Nov 16 22:15 testdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 18 Nov 16 21:58 test.log</span></pre></td></tr></table></figure><p>-rw-r–r–</p><p>第一个字母：d文件夹，-文件，l连接</p><p>rw- r– r–</p><p>r: read 4</p><p>w: write 2</p><p>x: 执行 1</p><p>-: 没权限 0</p><p>7=rwx</p><p>5=r-x</p><p>3=-wx</p><p>rwx 第一组，7，代表该文件所属用户的权限，读写执行</p><p>r-x 第二组，5，代表该文件所属用户组的权限，读执行</p><p>r-x 第三组，5，代表其他组用户对该文件的权限，读执行</p><p>修改文件所属用户用户组和权限：</p><ul><li>chmod -R 777 test.log  修改文件权限，777代表任意的用户用户组，都有读写执行的权限</li><li>chown -R 用户:用户组 test.log  修改文件所属用户和用户组</li></ul><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><p>查看文件大小：</p><ul><li>ll 列出文件信息，相当于ls -l</li><li>du -sh test.log  查看文件大小</li><li>du -sh test  查看文件夹大小</li></ul><p>查找文件：</p><ul><li><code>find / -name &#39;*hadoop*&#39;</code>  从根目录开始查找包含hadoop的文件</li><li><code>find /usr/local - name &#39;*hadoop*&#39;</code>  指定目录查找包含hadoop的文件</li></ul><p>查看历史命令：</p><ul><li>history</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux常用命令 </tag>
            
            <tag> Linux用户用户组 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux环境变量&amp;alias别名&amp;history和rm -rf</title>
      <link href="/2018/03/13/dw/linux-2/"/>
      <url>/2018/03/13/dw/linux-2/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux环境变量"><a href="#Linux环境变量" class="headerlink" title="Linux环境变量"></a>Linux环境变量</h3><p>全局环境变量：/etc/profile，所有用户都可以使用。</p><p>个人环境变量有两个：<code>~/.bash_profile</code>和<code>~/.bashrc</code>，只供所属用户使用</p><p>生效环境变量文件：<code>source ./bashrc</code>或者<code>. ~/.bashrc</code></p><p>查看环境变量是否生效：<code>which java</code></p><p>打印环境变量：<code>echo $PATH</code></p><p>一般生效环境变量文件之后，习惯性的会which一下，看环境变量是否生效。</p><p><strong>command not found问题：</strong></p><p>一般出现command not found，可以which看看，然后查看软件是否安装，环境变量是否已配置</p><h3 id="alias别名"><a href="#alias别名" class="headerlink" title="alias别名"></a>alias别名</h3><p>命令的alias别名可以简化我们的操作。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># alias</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> cp=<span class="string">'cp -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> egrep=<span class="string">'egrep --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> fgrep=<span class="string">'fgrep --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> grep=<span class="string">'grep --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> l.=<span class="string">'ls -d .* --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ll=<span class="string">'ls -l --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ls=<span class="string">'ls --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> mv=<span class="string">'mv -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> rm=<span class="string">'rm -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> <span class="built_in">which</span>=<span class="string">'alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment">#</span></span></pre></td></tr></table></figure><p>编辑.bashrc：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># vi .bashrc</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># .bashrc</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># User specific aliases and functions</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> rm=<span class="string">'rm -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> cp=<span class="string">'cp -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> mv=<span class="string">'mv -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Source global definitions</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ -f /etc/bashrc ]; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        . /etc/bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fi</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加以下内容</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> j=<span class="string">'cd /home/vinx'</span></span></pre></td></tr></table></figure><p>生效环境变量文件并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># source .bashrc</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># v</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment">#</span></span></pre></td></tr></table></figure><h3 id="history命令"><a href="#history命令" class="headerlink" title="history命令"></a>history命令</h3><p>history可以查看历史记录的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># history</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    1  <span class="built_in">history</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    2  ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    3  ls</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    4  <span class="built_in">which</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    5  <span class="built_in">history</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># !2   #执行历史指定的命令</span></span></pre></td></tr></table></figure><h3 id="删除命令"><a href="#删除命令" class="headerlink" title="删除命令"></a>删除命令</h3><p>生成新文件：</p><ul><li>touch test.log</li><li>cat /dev/null &gt; test.log</li><li>vi test.log</li></ul><p>删除文件：</p><ul><li>rm -f test.log  删除文件</li><li>rm -rf test  删除文件夹</li></ul><p><code>rm -rf</code>是高危命令，但在有些场景中又必须使用，比较安全的做法是设置回收站。</p><p>在shell脚本中也可以使用<code>rm -rf</code></p><p>比如shell脚本中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">K='/home/vinx/data'</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">判断$K是否存在</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">rm -rf $K/*</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux环境变量 </tag>
            
            <tag> Linux常用命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux用户和用户组</title>
      <link href="/2018/03/11/linux/linux-user-group/"/>
      <url>/2018/03/11/linux/linux-user-group/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux用户和用户组"><a href="#Linux用户和用户组" class="headerlink" title="Linux用户和用户组"></a>Linux用户和用户组</h3><hr><h4 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 查看关于user的命令</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# ll &#x2F;usr&#x2F;sbin&#x2F;user*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 118192 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;useradd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root  80360 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;userdel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-rws--x--x. 1 root root  40312 Jun 10  2014 &#x2F;usr&#x2F;sbin&#x2F;userhelper</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 113840 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;usermod</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rwsr-xr-x. 1 root root  11288 Aug  4  2017 &#x2F;usr&#x2F;sbin&#x2F;usernetctl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"># 查看关于group的命令</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# ll &#x2F;usr&#x2F;sbin&#x2F;group*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 65480 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;groupadd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 57016 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;groupdel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 57064 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;groupmems</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 76424 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;groupmod</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"># 添加用户vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# useradd vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">uid&#x3D;1001(vinx) gid&#x3D;1001(vinx) groups&#x3D;1001(vinx)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">释义：创建一个普通用户vinx，默认创建用户的用户组vinx，且设置用户的主组为vinx，并且创建&#x2F;home&#x2F;vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"># 查看linux上的所有普通用户</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# cd &#x2F;home</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]# ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">drwx------. 23 hadoop  hadoop  4096 Nov 24 04:02 hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">drwx------   3 hadoop2 hadoop2   78 Nov 24 04:26 hadoop2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">drwx------   8 vinx    vinx     221 Nov 22 04:27 vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"># 添加vinx用户到另外一个组bigdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# groupadd bigdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# cat &#x2F;etc&#x2F;group | grep bigdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">bigdata:x:1004:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# usermod -a -G bigdata vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">uid&#x3D;1003(vinx) gid&#x3D;1003(vinx) groups&#x3D;1003(vinx),1004(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"># 修改bigdata为vinx的主组</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# usermod --gid bigdata vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">uid&#x3D;1003(vinx) gid&#x3D;1004(bigdata) groups&#x3D;1004(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# usermod -a -G vinx vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">uid&#x3D;1003(vinx) gid&#x3D;1004(bigdata) groups&#x3D;1004(bigdata),1003(vinx)</span></pre></td></tr></table></figure><h4 id="删除用户"><a href="#删除用户" class="headerlink" title="删除用户"></a>删除用户</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@hadoop000 home]<span class="comment"># userdel vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">id: vinx: no such user</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># cat /etc/passwd | grep vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># cat /etc/group | grep vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">因为vinx用户组只有vinx用户，当这个用户删除时，组会校验就它自己，会自动删除</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">但是/home下仍然有vinx文件夹</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">drwx------. 23 hadoop  hadoop  4096 Nov 24 04:02 hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">drwx------   3 hadoop2 hadoop2   78 Nov 24 04:26 hadoop2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">drwx------   8    1001    1001  221 Nov 22 04:27 vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新创建用户vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># useradd vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">useradd: warning: the home directory already exists.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">Not copying any file from skel directory into it.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">Creating mailbox file: File exists</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">uid=1003(vinx) gid=1003(vinx) groups=1003(vinx)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">drwx------. 23 hadoop  hadoop  4096 Nov 24 04:02 hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">drwx------   3 hadoop2 hadoop2   78 Nov 24 04:26 hadoop2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">drwx------   8    1001    1001  221 Nov 22 04:27 vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时切换至vinx用户时</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">Last failed login: Sun Nov 24 05:01:57 CST 2017 on pts/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">There was 1 failed login attempt since the last successful login.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">su: warning: cannot change directory to /home/vinx: Permission denied</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">-bash: /home/vinx/.bash_profile: Permission denied</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">-bash-4.2$ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理切换用户出现-bash-4.2$的情况</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]<span class="comment"># rm -rf .bash*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]<span class="comment"># cp /etc/skel/.* /home/vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># chown vinx:vinx vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]<span class="comment"># chown vinx:vinx .bash*</span></span></pre></td></tr></table></figure><h4 id="设置密码"><a href="#设置密码" class="headerlink" title="设置密码"></a>设置密码</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]<span class="comment"># passwd vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Changing password <span class="keyword">for</span> user vinx.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">New password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">BAD PASSWORD: The password is shorter than 8 characters</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Retype new password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">passwd: all authentication tokens updated successfully.</span></pre></td></tr></table></figure><h4 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">su vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">su - vinx  切换至该用户的家目录，且执行环境变量文件</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">.bash_profile  su vinx不会执行，su - vinx执行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">.bashrc        su vinx和su - vinx都执行</span></pre></td></tr></table></figure><h4 id="普通用户获取root的最大权限"><a href="#普通用户获取root的最大权限" class="headerlink" title="普通用户获取root的最大权限"></a>普通用户获取root的最大权限</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在文件中添加</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root ALL=(ALL) ALL</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">vinx ALL=(root) NOPASSWD:ALL</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">切换至vinx用户，命令前使用sudo</span></pre></td></tr></table></figure><h4 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用格式</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">chmod -R 777 directory/file</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">chown -R user:group directory/file</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-----------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x  2 root root    6 Nov 24 06:14 <span class="built_in">test</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root root    0 Nov 24 06:14 test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">第一个字母：d文件夹，-文件，l连接</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">rwx r-x r-x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">r: <span class="built_in">read</span> 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">w: write 2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">x: 执行 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">-: 没权限 0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">7=rwx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">3=-wx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">5=r-x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">rwx 第一组 7 代表文件或文件夹的用户root的权限：读写执行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">r-x 第二组 5 代表文件或文件夹的用户组root的权限：读执行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">r-x 第三组 5 代表其他组的所属用户对这个文件或文件夹的权限：读执行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">777 代表任意的用户用户组，都读写执行</span></pre></td></tr></table></figure><h4 id="查看大小"><a href="#查看大小" class="headerlink" title="查看大小"></a>查看大小</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">du -sh xxx.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件夹</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">du -sh xxx</span></pre></td></tr></table></figure><h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从根目录全局查找</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">find / -name <span class="string">'*hadoop*'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定目录中查找</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">find /usr/<span class="built_in">local</span> -name <span class="string">'*hadoop*'</span></span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux用户用户组 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令&amp;定位ERROR</title>
      <link href="/2018/03/10/dw/linux-1/"/>
      <url>/2018/03/10/dw/linux-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux常用命令"><a href="#Linux常用命令" class="headerlink" title="Linux常用命令"></a>Linux常用命令</h3><ul><li>pwd  查看当前目录路径</li><li>cd /home  切换到/home目录</li><li>cd -  回退到上一次目录</li><li>cd 切换到用户家目录</li><li>cd ~ 同上</li><li>cd ../  回退到上一层目录</li><li>cd ../../  回退到上两层目录</li><li>ls  查看当前目录的所有文件</li><li>ll  查看所有文件，相当于ls -l</li><li>ll -a 查看所有文件，包括隐藏文件</li><li>ll -h  查看所有文件，并显示文件大小</li><li>ll -rt  查看所有文件，并按时间排序</li><li>clear  清空屏幕</li><li>ls –help  查看ls相关的命令帮助</li><li>mkdir  创建文件夹</li><li>mkdir -p a/b/c  级联创建文件夹</li><li>mkdir a b c  创建多个文件夹</li><li>mv  移动文件</li><li>cp  拷贝文件</li><li>touch test.log  创建一个新文件</li><li>echo “” &gt; test.log  创建一个新文件（慎用，会产生一个字节）</li><li>cat /dev/null &gt; test.log  把一个文件设置为空</li></ul><h3 id="如何定位ERROR"><a href="#如何定位ERROR" class="headerlink" title="如何定位ERROR"></a>如何定位ERROR</h3><h4 id="查看文件内容"><a href="#查看文件内容" class="headerlink" title="查看文件内容"></a>查看文件内容</h4><ul><li>cat  文件内容全部显示</li><li>more  文件内容一页一页的往下翻，按空格键翻页，按q退出</li><li>less  查看文件内容，按上下箭头往上下翻页，按q键退出</li></ul><h4 id="监控日志文件"><a href="#监控日志文件" class="headerlink" title="监控日志文件"></a>监控日志文件</h4><ul><li>tail -f test.log</li><li>tail -F test.log  ==&gt; -f + retry</li><li>tail -300f test.log  实时监控倒数300行</li></ul><h4 id="定位ERROR"><a href="#定位ERROR" class="headerlink" title="定位ERROR"></a>定位ERROR</h4><p>场景1：文件内容很小，几十M。</p><p>解决方法：下载到windows，使用编辑器查找ERROR</p><p>安装上传下载工具：<code>yum install -y lrzsz</code></p><p>场景2：文件内容很大，几百M。</p><p>解决方法：<code>cat test.log | grep ERROR</code></p><p><strong>定位ERROR的3种命令：</strong></p><ul><li><code>cat test.log | grep -A 10 ERROR</code>  后10行</li><li><code>cat test.log | grep -B 10 ERROR</code>  前10行</li><li><code>cat test.log | grep -C 30 ERROR</code>  前后各30行</li><li><code>cat test.log | grep -C 30 ERROR &gt; error.log</code>  错误信息写入到error.log文件</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux常用命令 </tag>
            
            <tag> 定位ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux vi模式&amp;查看系统资源</title>
      <link href="/2018/02/14/linux/linux-vi/"/>
      <url>/2018/02/14/linux/linux-vi/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux-vi命令"><a href="#Linux-vi命令" class="headerlink" title="Linux vi命令"></a>Linux vi命令</h3><hr><h4 id="vi命令的三种模式"><a href="#vi命令的三种模式" class="headerlink" title="vi命令的三种模式"></a>vi命令的三种模式</h4><p><img src="https://vinxikk.github.io/img/linux/vi.png" alt="三种模式的关系"></p><p>模拟流程：vi test.log进入命令模式，i键进入编辑模式，esc键退出编辑模式并进入命令模式，shift+:进入尾行模式，输入wq并回车保存并退出。</p><h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><p>shift+:进入尾行模式后，/keyword，回车后自动匹配，N键寻找下一个</p><h4 id="设置行号"><a href="#设置行号" class="headerlink" title="设置行号"></a>设置行号</h4><p>进入尾行模式：</p><ul><li>set nu 显示行号</li><li>set nonu  取消行号显示</li></ul><h4 id="清空文件内容"><a href="#清空文件内容" class="headerlink" title="清空文件内容"></a>清空文件内容</h4><ul><li>cat /dev/null &gt; test.log</li><li>echo ‘’ &gt; test.log  存在1个字节，慎用</li></ul><h4 id="命令模式常用快捷键"><a href="#命令模式常用快捷键" class="headerlink" title="命令模式常用快捷键"></a>命令模式常用快捷键</h4><ul><li><p>dd  删除当前行</p></li><li><p>dG  删除光标当前及以下的所有行</p></li><li><p>ndd  删除光标当前及以下的n行</p></li><li><p>gg  跳转到第一行的第一个字母</p></li><li><p>G  跳转到最后一行的第一个字母</p></li><li><p>shift + $  行尾</p></li><li><p>gg + dG  清空文件</p></li></ul><h3 id="系统命令"><a href="#系统命令" class="headerlink" title="系统命令"></a>系统命令</h3><h4 id="查看系统资源"><a href="#查看系统资源" class="headerlink" title="查看系统资源"></a>查看系统资源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 磁盘</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# df -h</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;sda3        38G  3.3G   33G  10% &#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">tmpfs          1000M  8.0K 1000M   1% &#x2F;dev&#x2F;shm</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;sda1       194M   34M  151M  19% &#x2F;boot</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;sr0        4.2G  4.2G     0 100% &#x2F;media&#x2F;CentOS_6.5_Final</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"># 内存</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# free -m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">             total       used       free     shared    buffers     cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Mem:          1998        591       1407          0         22        279</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">-&#x2F;+ buffers&#x2F;cache:        289       1709</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">Swap:         2047          0       2047</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# free -g</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">             total       used       free     shared    buffers     cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">Mem:             1          0          1          0          0          0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">-&#x2F;+ buffers&#x2F;cache:          0          1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">Swap:            1          0          1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"># 负载</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# top</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">top - 20:04:15 up 22 min,  3 users,  load average: 0.02, 0.02, 0.03</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">Tasks: 141 total,   1 running, 140 sleeping,   0 stopped,   0 zombie</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">Cpu(s):  0.0%us,  0.3%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">Mem:   2046592k total,   704988k used,  1341604k free,    22964k buffers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">Swap:  2097144k total,        0k used,  2097144k free,   383432k cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"> 2309 root      20   0 15028 1336 1004 R  0.7  0.1   0:02.45 top                                                                                                           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">    7 root      20   0     0    0    0 S  0.3  0.0   0:02.54 events&#x2F;0                                                                                                      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">    1 root      20   0 19364 1536 1228 S  0.0  0.1   0:02.41 init                                                                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    2 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kthreadd                                                                                                      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">    3 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration&#x2F;0                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">    4 root      20   0     0    0    0 S  0.0  0.0   0:00.05 ksoftirqd&#x2F;0                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">    5 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration&#x2F;0                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">    6 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 watchdog&#x2F;0                                                                                                    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">    8 root      20   0     0    0    0 S  0.0  0.0   0:00.00 cgroup                                                                                                        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">    9 root      20   0     0    0    0 S  0.0  0.0   0:00.01 khelper                                                                                                       </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">   10 root      20   0     0    0    0 S  0.0  0.0   0:00.00 netns                                                                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">   11 root      20   0     0    0    0 S  0.0  0.0   0:00.00 async&#x2F;mgr                                                                                                     </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">   12 root      20   0     0    0    0 S  0.0  0.0   0:00.00 pm                                                                                                            </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">   13 root      20   0     0    0    0 S  0.0  0.0   0:00.02 sync_supers                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">   14 root      20   0     0    0    0 S  0.0  0.0   0:00.01 bdi-default                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">   15 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kintegrityd&#x2F;0                                                                                                 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">   16 root      20   0     0    0    0 S  0.0  0.0   0:00.71 kblockd&#x2F;0                                                                                                     </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">   17 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kacpid                                                                                                        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">   18 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kacpi_notify                                                                                                  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">   19 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kacpi_hotplug                                                                                                 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">   20 root      20   0     0    0    0 S  0.0  0.0   0:00.00 ata_aux                                                                                                       </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">   21 root      20   0     0    0    0 S  0.0  0.0   0:00.13 ata_sff&#x2F;0                                                                                                     </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">   22 root      20   0     0    0    0 S  0.0  0.0   0:00.00 ksuspend_usbd                                                                                                 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">   23 root      20   0     0    0    0 S  0.0  0.0   0:00.02 khubd                                                                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">   24 root      20   0     0    0    0 S  0.0  0.0   0:00.02 kseriod                                                                                                       </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">   25 root      20   0     0    0    0 S  0.0  0.0   0:00.00 md&#x2F;0                                                                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">   26 root      20   0     0    0    0 S  0.0  0.0   0:00.00 md_misc&#x2F;0</span></pre></td></tr></table></figure><h4 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# ps -ef | grep ssh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root      1451     1  0 19:42 ?        00:00:00 &#x2F;usr&#x2F;sbin&#x2F;sshd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root      2223  1451  0 19:44 ?        00:00:00 sshd: root@pts&#x2F;1 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root      2353  2227  0 20:08 pts&#x2F;1    00:00:00 grep ssh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# ps -ef | grep ssh | grep -v grep</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">root      1451     1  0 19:42 ?        00:00:00 &#x2F;usr&#x2F;sbin&#x2F;sshd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">root      2223  1451  0 19:44 ?        00:00:00 sshd: root@pts&#x2F;1 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">进程用户  进程pid 父id                              进程用户的内容</span></pre></td></tr></table></figure><h4 id="查看端口"><a href="#查看端口" class="headerlink" title="查看端口"></a>查看端口</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# netstat -nlp | grep 1451</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      1451&#x2F;sshd           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::22                       :::*                        LISTEN      1451&#x2F;sshd</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux vi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL案例&amp;TopN实现</title>
      <link href="/2018/01/15/mysql/mysql-example-topn/"/>
      <url>/2018/01/15/mysql/mysql-example-topn/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL案例及面试题"><a href="#MySQL案例及面试题" class="headerlink" title="MySQL案例及面试题"></a>MySQL案例及面试题</h3><hr><h4 id="员工信息统计"><a href="#员工信息统计" class="headerlink" title="员工信息统计"></a>员工信息统计</h4><p>三张表：</p><ul><li>dept - 部门表</li><li>salgrade - 工资等级表</li><li>emp - 员工信息表（类比于流水表）</li></ul><p><strong>表结构如下：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 部门表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># dept部门表(deptno部门编号/dname部门名称/loc地点)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    deptno <span class="built_in">numeric</span>(<span class="number">2</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    dname <span class="built_in">varchar</span>(<span class="number">14</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    loc <span class="built_in">varchar</span>(<span class="number">13</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 工资等级表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># salgrade工资等级表(grade 等级/losal此等级的最低/hisal此等级的最高)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> salgrade (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    grade <span class="built_in">numeric</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    losal <span class="built_in">numeric</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    hisal <span class="built_in">numeric</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 员工表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># emp员工表(empno员工号/ename员工姓名/job工作/mgr上级编号/hiredate受雇日期/sal薪金/comm佣金/deptno部门编号)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 工资 ＝ 薪金 ＋ 佣金</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    empno <span class="built_in">numeric</span>(<span class="number">4</span>) <span class="keyword">not</span> <span class="literal">null</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">    ename <span class="built_in">varchar</span>(<span class="number">10</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">    job <span class="built_in">varchar</span>(<span class="number">9</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    mgr <span class="built_in">numeric</span>(<span class="number">4</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    hiredate datetime,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">    sal <span class="built_in">numeric</span>(<span class="number">7</span>, <span class="number">2</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    comm <span class="built_in">numeric</span>(<span class="number">7</span>, <span class="number">2</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">    deptno <span class="built_in">numeric</span>(<span class="number">2</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure><p><strong>题目和SQL语句：</strong></p><ol><li><p>查询出部门编号为30的所有员工的编号和姓名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> empno,ename <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span>;</span></pre></td></tr></table></figure></li><li><p>找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> (deptno=<span class="number">10</span> <span class="keyword">and</span> job=<span class="string">'MANAGER'</span>) <span class="keyword">or</span> (deptno=<span class="number">20</span> <span class="keyword">and</span> job=<span class="string">'SALESMAN'</span>);</span></pre></td></tr></table></figure></li><li><p>查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> (sal+<span class="keyword">ifnull</span>(comm,<span class="number">0</span>)) <span class="keyword">desc</span>,hiredate <span class="keyword">asc</span>;</span></pre></td></tr></table></figure></li><li><p>列出最低薪金大于1500的各种工作及从事此工作的员工人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：使用having</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> job,<span class="keyword">count</span>(job) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> job </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">having</span> <span class="keyword">min</span>(sal) &gt; <span class="number">1500</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> job,<span class="keyword">count</span>(<span class="number">0</span>) <span class="keyword">from</span> emp <span class="keyword">where</span> job <span class="keyword">in</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> temp.job <span class="keyword">from</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">(<span class="keyword">select</span> job,<span class="keyword">min</span>(sal) minsal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> job) <span class="keyword">as</span> temp <span class="keyword">where</span> temp.minsal&gt;<span class="number">1500</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">group</span> <span class="keyword">by</span> job;</span></pre></td></tr></table></figure></li><li><p>列出在销售部工作的员工的姓名，假定不知道销售部的部门编号</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno,ename <span class="keyword">from</span> emp <span class="keyword">where</span> deptno <span class="keyword">in</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno <span class="keyword">from</span> dept <span class="keyword">where</span> dname=<span class="string">'SALES'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure></li><li><p>查询姓名以S开头的、以S结尾的、包含S字符、第二个字母为L的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">like</span> <span class="string">'S%'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">like</span> <span class="string">'%S'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">like</span> <span class="string">'%S%'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">like</span> <span class="string">'_L%'</span>;</span></pre></td></tr></table></figure></li><li><p>查询每种工作的最高工资、最低工资、人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> job,<span class="keyword">max</span>(sal+<span class="keyword">ifnull</span>(comm,<span class="number">0</span>)) maxsal,<span class="keyword">min</span>(sal+<span class="keyword">ifnull</span>(comm,<span class="number">0</span>)) minsal,<span class="keyword">count</span>(empno) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> job;</span></pre></td></tr></table></figure></li><li><p>列出薪金高于公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename,d.dname,m.ename,e.sal+<span class="keyword">ifnull</span>(e.comm,<span class="number">0</span>) salsum,s.grade <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> emp m <span class="keyword">on</span> e.mgr=m.empno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> salgrade s <span class="keyword">on</span> e.sal+<span class="keyword">ifnull</span>(e.comm,<span class="number">0</span>) <span class="keyword">between</span> s.losal <span class="keyword">and</span> s.hisal</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; (<span class="keyword">select</span> <span class="keyword">avg</span>(sal) <span class="keyword">from</span> emp);</span></pre></td></tr></table></figure></li></ol><ol start="9"><li><p>列出薪金高于在部门30工作的所有/任何一个员工的薪金的员工姓名和薪金、部门名称</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有 可以用all，也可以用max</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># max</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.sal,e.deptno,d.dname <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">max</span>(sal) maxsal <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.sal,e.deptno,d.dname <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">all</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> sal <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- -----------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 任何 可以用any，也可以用min</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># min</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.sal,e.deptno,d.dname <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">min</span>(sal) maxsal <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># any</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.sal,e.deptno,d.dname <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">any</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> sal <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure></li></ol><h4 id="MySQL的TopN实现"><a href="#MySQL的TopN实现" class="headerlink" title="MySQL的TopN实现"></a>MySQL的TopN实现</h4><p>求出每个部门每个职业的薪水和，薪水和最高的2个职位（实际就是topN的问题）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建视图sal，保存每个部门每个职业的薪水和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> sal <span class="keyword">as</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno,job,<span class="keyword">sum</span>(sal) sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno,job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求出每个职业的top1，求top2可以将0改为1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">a.*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sal a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> sal b <span class="keyword">where</span> a.deptno=b.deptno <span class="keyword">and</span> a.sal&lt;b.sal</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">) = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.deptno;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
            <tag> MySQL Top-N </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gson的使用</title>
      <link href="/2017/10/02/java/gson-1/"/>
      <url>/2017/10/02/java/gson-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Gson入门"><a href="#Gson入门" class="headerlink" title="Gson入门"></a>Gson入门</h3><p>Github地址：<a href="https://github.com/google/gson" target="_blank" rel="noopener">https://github.com/google/gson</a></p><p>Gson是Google公司发布的一个开放源代码的Java库，主要用途为序列化Java对象为JSON字符串，或反序列化JSON字符串成Java对象。</p><p>而JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，易于人阅读和编写，同时也易于机器解析和生成，广泛应用于各种数据的交互中，尤其是服务器与客户端的交互。</p><p>Java对象和Json之间的互转，一般用的比较多的两个类库是Jackson和Gson。</p><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p>Serialization：序列化，Java对象转换到Json字符串的过程。</p><p>Deserialization：反序列化，字符串转换成Java对象。</p><h4 id="Gson使用"><a href="#Gson使用" class="headerlink" title="Gson使用"></a>Gson使用</h4><p>使用Maven管理，pom.xml导入gson的依赖：</p><p>Maven地址：<a href="https://mvnrepository.com/artifact/com.google.code.gson/gson" target="_blank" rel="noopener">https://mvnrepository.com/artifact/com.google.code.gson/gson</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.code.gson<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>gson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr></table></figure><p>Gson的两个基础方法：</p><p><code>toJson();</code></p><p><code>fromJson();</code></p><p><strong>Gson的创建方式一：直接new Gson对象</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//Gson创建方式一：直接new Gson对象</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">newGson</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    Gson gson = <span class="keyword">new</span> Gson();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    User user = <span class="keyword">new</span> User(<span class="number">1L</span>, <span class="string">"tom"</span>, <span class="number">21</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//toJson 将bean对象转换为json字符串</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    String jsonStr = gson.toJson(user, User<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    System.out.println(jsonStr);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//fromJson将json字符串转为bean对象</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    User userBean = gson.fromJson(jsonStr, User<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    System.out.println(userBean);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p><strong>Gson的创建方式二：使用GsonBuilder</strong></p><p>使用new Gson()，此时会创建一个带有默认配置选项的Gson实例，如果不想使用默认配置，那么就可以使用GsonBuilder。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//serializeNulls(): 当字段值为空或null时，依然对该字段进行转换</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Gson gson = <span class="keyword">new</span> GsonBuilder().serializeNulls().create();</span></pre></td></tr></table></figure><p>使用GsonBuilder创建Gson实例的步骤：</p><ol><li>首先创建GsonBuilder,然后调用GsonBuilder提供的各种配置方法进行配置</li><li>最后调用GsonBuilder的create方法，将基于当前的配置创建一个Gson实例。</li></ol><p>GsonBuilder的一些配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">Gson gson = <span class="keyword">new</span> GsonBuilder()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    .excludeFieldsWithoutExposeAnnotation() <span class="comment">//不对没有用@Expose注解的属性进行操作</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    .enableComplexMapKeySerialization() <span class="comment">//当Map的key为复杂对象时,需要开启该方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    .serializeNulls() <span class="comment">//当字段值为空或null时，依然对该字段进行转换</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    .setDateFormat(<span class="string">"yyyy-MM-dd HH:mm:ss:SSS"</span>) <span class="comment">//时间转化为特定格式</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    .setPrettyPrinting() <span class="comment">//对结果进行格式化，增加换行</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    .disableHtmlEscaping() <span class="comment">//防止特殊字符出现乱码</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    .registerTypeAdapter(User<span class="class">.<span class="keyword">class</span>,<span class="title">new</span> <span class="title">UserAdapter</span>()) //为某特定对象设置固定的序列或反序列方式，自定义<span class="title">Adapter</span>需实现<span class="title">JsonSerializer</span>或者<span class="title">JsonDeserializer</span>接口</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="class">    .<span class="title">create</span>()</span>;</span></pre></td></tr></table></figure><p>例如，Gosn对复杂Map的处理时需要用到其中的<code>enableComplexMapKeySerialization()</code>配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">Gson gson = <span class="keyword">new</span> GsonBuilder()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    .enableComplexMapKeySerialization()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    .create(); <span class="comment">//开启复杂处理Map方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Map&lt;List&lt;User&gt;, String&gt; map = <span class="keyword">new</span> HashMap&lt;List&lt;User&gt;, String&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO... 向map中添加数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">String jsonStr = gson.toJson(map);  <span class="comment">//toJson</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">Map&lt;List&lt;User&gt;, String&gt; resultMap = gson.fromJson(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    jsonStr,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">new</span> TypeToken&lt;Map&lt;List&lt;User&gt;, String&gt;&gt;() &#123;&#125;.getType()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">); <span class="comment">//fromJson</span></span></pre></td></tr></table></figure><p>注意：如果Map的key为String，则可以不使用GsonBuilder的<code>enableComplexMapKeySerialization()</code>方法，或者直接<code>new Gson();</code></p><h4 id="Gson的注解"><a href="#Gson的注解" class="headerlink" title="Gson的注解"></a>Gson的注解</h4><p>@Expose注解：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="meta">@Expose</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> String firstName;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="meta">@Expose</span>(serialize = <span class="keyword">false</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> String lastName;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="meta">@Expose</span>(deserialize = <span class="keyword">false</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> String emailAddress;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> String password;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>@Expose中serialize和deserialize属性是可选的，默认两个都为true。</p><p>如果serialize为true，调用toJson时会序列化该属性；如果deserialize为true，调用fromJson生成Java对象时不会进行反序列化。</p><p>注意：如果采用new Gson()方式创建Gson，@Expose没有任何效果。需要使用 <code>gsonBuilder.excludeFieldsWithoutExposeAnnotation()</code>方法。</p><p>@SerializedName注解，能指定该字段在序列化成json时的名称：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">@SerializedName</span>(<span class="string">"w"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> width;</span></pre></td></tr></table></figure><hr><p>参考：</p><p><a href="https://www.jianshu.com/p/fc5c9cdf3aab" target="_blank" rel="noopener">https://www.jianshu.com/p/fc5c9cdf3aab</a></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JSON </tag>
            
            <tag> Gson </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL的锁：表锁、行锁，乐观锁、悲观锁</title>
      <link href="/2017/09/15/mysql/mysql-locking/"/>
      <url>/2017/09/15/mysql/mysql-locking/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL锁概述"><a href="#MySQL锁概述" class="headerlink" title="MySQL锁概述"></a>MySQL锁概述</h3><p>锁是计算机协调多个进程或线程并发访问某一资源的机制。</p><p>在数据库中，除传统的计算资源（如CPU、RAM、I/O等）的争用以外，数据也是一种供许多用户共享的资源。</p><p>MySQL用到了很多的锁机制，比如行锁、表锁，读锁、写锁等，都是在操作之前先上锁，这些锁统称为悲观锁（Pessimistic Lock）。</p><p>相对于其他数据库而言，MySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。比如，MyISAM和MEMORY存储引擎采用的是表级锁（table-level locking）；BDB存储引擎采用的是页面锁（page-level locking），但也支持表级锁；InnoDB存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁。</p><p><strong>表级锁：</strong></p><p>开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。</p><p><strong>行级锁：</strong></p><p>开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。</p><p><strong>页面锁：</strong></p><p>开销和加锁时间介于表锁和行锁之间；会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般。</p><p>仅从锁的角度来说，表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统。</p><p><strong>乐观锁：</strong></p><p>乐观锁就是很乐观地认为，别人不会修改自己要拿的数据，不需要上锁。</p><p>只是在更新的时候，判断下数据是否发生了变更（可以通过版本号来实现），如果发生变更，就重新取数据，更新数据时，再次判断是否变更。</p><p><strong>悲观锁：</strong></p><p>悲观锁就是很悲观地认为，自己取数据时，别人一定会修改，因此直接给要拿的数据加上锁，直到更新完释放了锁，才允许别人修改。</p><p>根据加锁对象的不同分为行锁和表锁（MySQL中，只有InnoDB存储引擎才有行锁，MyISAM只有表锁），根据加锁机制不同分为共享锁和排他锁。</p><ul><li><p>行锁和表锁</p><p>MyISAM操作数据都是使用的表锁，更新一条记录就要锁整个表，因此性能较低，并发不高，但同时不会存在死锁问题。</p><p>InnoDB与MyISAM的最大不同有两点：一是支持事务；二是采用了行级锁。</p><p>在MySQL中，行级锁并不是直接锁记录，而是锁索引。</p><p>索引分为主键索引和非主键索引两种，如果一条SQL语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。</p><p>InnoDB行锁是通过给索引项加锁实现的，如果没有索引，InnoDB会通过隐藏的聚簇索引来对记录加锁。也就是说：如果不通过索引条件检索数据，那么InnoDB将对表中所有数据加锁，实际效果跟表锁一样。因为没有了索引，找到某一条记录就得扫描全表，要扫描全表，就得锁定表。</p></li><li><p>排他锁和共享锁</p><p>数据库的增删改操作默认都会加排他锁，而查询不会加任何锁。</p><p>共享锁：对某一资源加共享锁，自身可以读该资源，其他人也可以读该资源（也可以再继续加共享锁，即共享锁可以多个共存），但无法修改。要想修改就必须等所有共享锁都释放完之后。语法为：</p><p><code>select * from tb_name lock in share mode;</code></p><p>排他锁：对某一资源加排他锁，自身可以进行增删改查，其他人无法进行任何操作。</p><p><code>select * from tb_name for update;</code></p></li></ul><h3 id="MyISAM表锁"><a href="#MyISAM表锁" class="headerlink" title="MyISAM表锁"></a>MyISAM表锁</h3><p>MySQL的表级锁有两种模式：表共享读锁（Table Read Lock）和表独占写锁（Table Write Lock）。</p><p>对MyISAM表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；对MyISAM表的写操作，则会阻塞其他用户对同一表的读和写操作；MyISAM表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作，其他线程的读、写操作都会等待，直到锁被释放为止。</p><h4 id="如何加表锁"><a href="#如何加表锁" class="headerlink" title="如何加表锁"></a>如何加表锁</h4><p>MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁；在执行更新操作（UPDATE、DELETE、INSERT等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用<code>LOCK TABLE</code>命令给MyISAM表显式加锁。</p><p>给MyISAM表显式加锁，一般是为了在一定程序上模拟事务操作，实现对某一时间点多个表的一致性读取。</p><p>例如， 有一个订单表orders，其中记录有各订单的总金额total，同时还有一个订单明细表order_detail，其中记录有各订单每一产品的金额小计 subtotal，假设我们需要检查这两个表的金额合计是否相符，可能就需要执行如下两条SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(total) <span class="keyword">from</span> orders;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(subtotal) <span class="keyword">from</span> order_detail;</span></pre></td></tr></table></figure><p>这时，如果不先给两个表加锁，就可能产生错误的结果，因为第一条语句执行过程中，order_detail表可能已经发生了改变。因此，正确的方法应该是：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lock</span> <span class="keyword">tables</span> orders <span class="keyword">read</span> <span class="keyword">local</span>, order_detail <span class="keyword">read</span> <span class="keyword">local</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(total) <span class="keyword">from</span> orders;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">sum</span>(subtotal) <span class="keyword">from</span> order_detail;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">unlock</span> <span class="keyword">tables</span>;</span></pre></td></tr></table></figure><p>特别说明：</p><ol><li>上面的例子在<code>LOCK TABLES</code>时加了“local”选项，其作用就是在满足MyISAM表并发插入条件的情况下，允许其他用户在表尾并发插入记录。</li><li>用<code>LOCK TABLES</code>给表显式加表锁时，必须同时取得所有涉及到表的锁，并且MySQL不支持锁升级。也就是说，在执行LOCK TABLES后，只能访问显式加锁的这些表，不能访问未加锁的表；同时，如果加的是读锁，那么只能执行查询操作，而不能执行更新操作。其实，在自动加锁的情况下也基本如此，MyISAM总是一次获得SQL语句所需要的全部锁。这也正是MyISAM表不会出现死锁（Deadlock Free）的原因。</li></ol><p>当使用<code>LOCK TABLES</code>时，不仅需要一次锁定用到的所有表，而且，同一个表在SQL语句中出现多少次，就要通过与SQL语句中相同的别名锁定多少次，否则也会出错。举例如下：</p><ol><li><p>对actor表获得读锁：</p><p><code>lock table actor read;</code></p></li><li><p>但是通过别名访问会提示错误：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a.first_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a.last_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">b.first_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">b.last_name</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> actor a, actor b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> a.first_name = b.first_name</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">and</span> a.first_name = <span class="string">'Lisa'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">and</span> a.last_name = <span class="string">'Jennifer'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">and</span> a.last_name &lt;&gt; b.last_name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">ERROR 1100 (HY000): Table ‘a’ was not locked <span class="keyword">with</span> <span class="keyword">LOCK</span> <span class="keyword">TABLES</span></span></pre></td></tr></table></figure></li><li><p>需要对别名分别锁定：</p><p><code>lock table actor as a read, actor as b read;</code></p></li><li><p>按照别名查询可以正确执行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a.first_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a.last_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">b.first_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">b.last_name</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> actor a, actor b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> a.first_name = b.first_name</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">and</span> a.first_name = <span class="string">'Lisa'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">and</span> a.last_name = <span class="string">'Jennifer'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">and</span> a.last_name &lt;&gt; b.last_name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+————+———–+————+———–+ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| first_name | last_name | first_name | last_name | </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+————+———–+————+———–+ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">| Lisa | Jennifer | LISA | MONROE | </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">+————+———–+————+———–+ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li></ol><h4 id="查询表级锁争用情况"><a href="#查询表级锁争用情况" class="headerlink" title="查询表级锁争用情况"></a>查询表级锁争用情况</h4><p>可以通过检查<code>table_locks_waited</code>和<code>table_locks_immediate</code>状态变量来分析系统上的表锁定争夺情况：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">status</span> <span class="keyword">like</span> <span class="string">'table%'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Variable_name | Value </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Table_locks_immediate | 2979 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Table_locks_waited | 0 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec))</span></pre></td></tr></table></figure><p>如果<code>Table_locks_waited</code>的值比较高，则说明存在着较严重的表级锁争用情况。</p><h4 id="并发插入（Concurrent-Inserts）"><a href="#并发插入（Concurrent-Inserts）" class="headerlink" title="并发插入（Concurrent Inserts）"></a>并发插入（Concurrent Inserts）</h4><p>上面提到过MyISAM表的读和写是串行的，但这是就总体而言的。在一定条件下，MyISAM表也支持查询和插入操作的并发进行。 </p><p>MyISAM存储引擎有一个系统变量<code>concurrent_insert</code>，专门用以控制其并发插入的行为，其值分别可以为0、1或2。</p><ul><li>当<code>concurrent_insert</code>设置为0时，不允许并发插入。</li><li>当<code>concurrent_insert</code>设置为1时，如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个进程读表的同时，另一个进程从表尾插入记录。这也是MySQL的默认设置。</li><li>当<code>concurrent_insert</code>设置为2时，无论MyISAM表中有没有空洞，都允许在表尾并发插入记录。</li></ul><p>可以利用MyISAM存储引擎的并发插入特性，来解决应用中对同一表查询和插入的锁争用。例如，将<code>concurrent_insert</code>系统变量设为2，总是允许并发插入；同时，通过定期在系统空闲时段执行<code>OPTIMIZE TABLE</code>语句来整理空间碎片，收回因删除记录而产生的中间空洞。</p><h4 id="MyISAM的锁调度"><a href="#MyISAM的锁调度" class="headerlink" title="MyISAM的锁调度"></a>MyISAM的锁调度</h4><p>MyISAM存储引擎的读锁和写锁是互斥的，读写操作是串行的。那么，一个进程请求某个 MyISAM表的读锁，同时另一个进程也请求同一表的写锁，MySQL如何处理呢？答案是写进程先获得锁。不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插到读锁请求之前！这是因为MySQL认为写请求一般比读请求要重要。这也正是MyISAM表不太适合于有大量更新操作和查询操作应用的原因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。这种情况有时可能会变得非常糟糕！幸好我们可以通过一些设置来调节MyISAM 的调度行为。</p><ul><li>通过指定启动参数<code>low-priority-updates</code>，使MyISAM引擎默认给予读请求以优先的权利。</li><li>通过执行命令<code>SET LOW_PRIORITY_UPDATES=1</code>，使该连接发出的更新请求优先级降低。</li><li>通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性，降低该语句的优先级。</li></ul><p>虽然上面3种方法都是要么更新优先，要么查询优先的方法，但还是可以用其来解决查询相对重要的应用（如用户登录系统）中，读锁等待严重的问题。 </p><p>另外，MySQL也提供了一种折中的办法来调节读写冲突，即给系统参数<code>max_write_lock_count</code>设置一个合适的值，当一个表的读锁达到这个值后，MySQL就暂时将写请求的优先级降低，给读进程一定获得锁的机会。</p><p>上面已经讨论了写优先调度机制带来的问题和解决办法。这里还要强调一点：一些需要长时间运行的查询操作，也会使写进程“饿死”！因此，应用中应尽量避免出现长时间运行的查询操作，不要总想用一条SELECT语句来解决问题，因为这种看似巧妙的SQL语句，往往比较复杂，执行时间较长，在可能的情况下可以通过使用中间表等措施对SQL语句做一定的“分解”，使每 一步查询都能在较短时间完成，从而减少锁冲突。如果复杂查询不可避免，应尽量安排在数据库空闲时段执行，比如一些定期统计可以安排在夜间执行。</p><h3 id="InnoDB锁"><a href="#InnoDB锁" class="headerlink" title="InnoDB锁"></a>InnoDB锁</h3><p>InnoDB与MyISAM的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁。行级锁与表级锁本来就有许多不同之处，另外，事务的引入也带来了一些新问题。</p><p><strong>事务（Transaction）及其ACID属性：</strong></p><ul><li>原子性（Actomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。</li><li>一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以操持完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。</li><li>隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。</li><li>持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。</li></ul><p><strong>并发事务带来的问题：</strong></p><p>相对于串行处理来说，并发事务处理能大大增加数据库资源的利用率，提高数据库系统的事务吞吐量，从而可以支持可以支持更多的用户。但并发事务处理也会带来一些问题，主要包括以下几种情况。</p><ul><li>更新丢失（Lost Update）：当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题——最后的更新覆盖了其他事务所做的更新。</li><li>脏读（Dirty Reads）：一个事务正在对一条记录做修改，在这个事务并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”的数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做“脏读”。</li><li>不可重复读（Non-Repeatable Reads）：一个事务在读取某些数据已经发生了改变、或某些记录已经被删除了，这种现象叫做“不可重复读”。</li><li>幻读（Phantom Reads）：一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。</li></ul><p><strong>事务隔离级别：</strong></p><p>在并发事务处理带来的问题中，“更新丢失”通常应该是完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。</p><p>“脏读”、“不可重复读”和“幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。数据库实现事务隔离的方式，基本可以分为以下两种。</p><ul><li>一种是在读取数据前，对其加锁，阻止其他事务对数据进行修改。</li><li>另一种是不用加任何锁，通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot），并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度，好像是数据库可以提供同一数据的多个版本，因此，这种技术叫做数据多版本并发控制（MultiVersion Concurrency Control，简称MVCC或MCC），也经常称为多版本数据库。</li></ul><p>在MVCC并发控制中，读操作可以分成两类：快照读 (snapshot read)与当前读 (current read)。快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。</p><p>在一个支持MVCC并发控制的系统中，哪些读操作是快照读？哪些操作又是当前读呢？以MySQL InnoDB为例：</p><ul><li><p>快照读：简单的select操作，属于快照读，不加锁。(当然，也有例外)</p><p><code>select * from tb_name where ?;</code></p></li><li><p>当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。</p><p>下面语句都属于当前读，读取记录的最新版本。并且，读取之后，还需要保证其他并发事务不能修改当前记录，对读取记录加锁。其中，除了第一条语句，对读取记录加S锁 (共享锁)外，其他的操作，都加的是X锁 (排它锁)。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_name <span class="keyword">where</span> ? <span class="keyword">lock</span> <span class="keyword">in</span> <span class="keyword">share</span> <span class="keyword">mode</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_name <span class="keyword">where</span> ? <span class="keyword">for</span> <span class="keyword">update</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_name <span class="keyword">values</span>(...);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> tb_name <span class="keyword">set</span> ? <span class="keyword">where</span> ?;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_name <span class="keyword">where</span> ?;</span></pre></td></tr></table></figure></li></ul><p>数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏 感，可能更关心数据并发访问的能力。</p><p>为了解决“隔离”与“并发”的矛盾，ISO/ANSI SQL92定义了4个事务隔离级别，每个级别的隔离程度不同，允许出现的副作用也不同，应用可以根据自己的业务逻辑要求，通过选择不同的隔离级别来平衡 “隔离”与“并发”的矛盾。下表很好地概括了这4个隔离级别的特性。</p><table><thead><tr><th></th><th>读数据一致性</th><th>脏读</th><th>不可重复读</th><th>幻读</th></tr></thead><tbody><tr><td>Read uncommitted</td><td>最低级别，只能保证不读取物理上损坏的数据</td><td>√</td><td>√</td><td>√</td></tr><tr><td>Read committed</td><td>语句级</td><td>x</td><td>√</td><td>√</td></tr><tr><td>Repeatable read</td><td>事务级</td><td>x</td><td>x</td><td>√</td></tr><tr><td>Serializable</td><td>最高级别，事务级</td><td>x</td><td>x</td><td>x</td></tr></tbody></table><h4 id="获取InnoDB行锁争用情况"><a href="#获取InnoDB行锁争用情况" class="headerlink" title="获取InnoDB行锁争用情况"></a>获取InnoDB行锁争用情况</h4><p>可以通过检查<code>innodb_row_lock</code>状态变量来分析系统上的行锁的争夺情况：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">status</span> <span class="keyword">like</span> <span class="string">'innodb_row_lock%'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------------------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| Variable_name                 | Value |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------------------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| Innodb_row_lock_current_waits | 0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| Innodb_row_lock_time          | 0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| Innodb_row_lock_time_avg      | 0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| Innodb_row_lock_time_max      | 0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| Innodb_row_lock_waits         | 0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------------------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.11</span> sec)</span></pre></td></tr></table></figure><p>如果发现锁争用比较严重，如<code>Innodb_row_lock_waits</code>和<code>Innodb_row_lock_time_avg</code>的值比较高，还可以通过设置InnoDB Monitors来进一步观察发生锁冲突的表、数据行等，并分析锁争用的原因。</p><h4 id="InnoDB的行锁模式及加锁方法"><a href="#InnoDB的行锁模式及加锁方法" class="headerlink" title="InnoDB的行锁模式及加锁方法"></a>InnoDB的行锁模式及加锁方法</h4><p>InnoDB实现了以下两种类型的行锁：</p><ul><li>共享锁（s）：又称读锁。允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。若事务T对数据对象A加上S锁，则事务T可以读A但不能修改A，其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。</li><li>排他锁（Ｘ）：又称写锁。允许获取排他锁的事务更新数据，阻止其他事务取得相同的数据集共享读锁和排他写锁。若事务T对数据对象A加上X锁，事务T可以读A也可以修改A，其他事务不能再对A加任何锁，直到T释放A上的锁。</li></ul><p>对于共享锁，就是多个事务只能读数据不能改数据。</p><p>排他锁指的是，一个事务在一行数据加上排他锁后，其他事务不能再在其上加其他的锁。mysql InnoDB引擎默认的修改数据语句：update,delete,insert都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型，如果加排他锁可以使用<code>select … for update</code>语句，加共享锁可以使用<code>select … lock in share mode</code>语句。所以加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过<code>select … from …</code>查询数据，因为普通查询没有任何锁机制。</p><p>另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。</p><ul><li>意向共享锁（IS）：事务打算给数据行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。</li><li>意向排他锁（IX）：事务打算给数据行加排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。</li></ul><p><strong>InnoDB行锁模式兼容性列表:</strong> </p><table><thead><tr><th></th><th>X</th><th>IX</th><th>S</th><th>IS</th></tr></thead><tbody><tr><td>X</td><td>冲突</td><td>冲突</td><td>冲突</td><td>冲突</td></tr><tr><td>IX</td><td>冲突</td><td>兼容</td><td>冲突</td><td>兼容</td></tr><tr><td>S</td><td>冲突</td><td>冲突</td><td>兼容</td><td>兼容</td></tr><tr><td>IS</td><td>冲突</td><td>兼容</td><td>兼容</td><td>兼容</td></tr></tbody></table><p>如果一个事务请求的锁模式与当前的锁兼容，InnoDB就请求的锁授予该事务；反之，如果两者两者不兼容，该事务就要等待锁释放。</p><p>意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；对于普通SELECT语句，InnoDB不会加任何锁。</p><p>事务可以通过以下语句显式给记录集加共享锁或排他锁：</p><ul><li>共享锁（S）：<code>SELECT * FROM tb_name WHERE ... LOCK IN SHARE MODE</code></li><li>排他锁（X）：<code>SELECT * FROM tb_name WHERE ... FOR UPDATE</code></li></ul><p>用<code>SELECT ... IN SHARE MODE</code>获得共享锁，主要用在需要数据依存关系时来确认某行记录是否存在，并确保没有人对这个记录进行UPDATE或者DELETE操作。但是如果当前事务也需要对该记录进行更新操作，则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用<code>SELECT… FOR UPDATE</code>方式获得排他锁。</p><h4 id="InnoDB行锁实现方式"><a href="#InnoDB行锁实现方式" class="headerlink" title="InnoDB行锁实现方式"></a>InnoDB行锁实现方式</h4><p>InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。</p><p>在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。下面通过一些实际例子来加以说明。</p><ol><li><p>在不通过索引条件查询的时候，InnoDB确实使用的是表锁，而不是行锁。</p><p><code>create table tb_no_index(id int,name varchar(10)) engine=innodb;</code></p><p><code>insert into tb_no_index values(1,&#39;zhangsan&#39;),(2,&#39;lisi&#39;),(3,&#39;wangwu&#39;),(4,&#39;tom&#39;);</code></p><p><img src="https://vinxikk.github.io/img/mysql/mysql-lock-innodb-1.png" alt="没有索引时，InnoDB使用表锁"></p><p>在上面的例子中，看起来session_1只给一行加了排他锁，但session_2在请求其他行的排他锁时，却出现了锁等待！原因就是在没有索引的情况下，InnoDB只能使用表锁。当我们给其增加一个索引后，InnoDB就只锁定了符合条件的行，如下例所示：</p><p>创建tb_with_index表，id字段有普通索引：</p><p><code>create table tb_with_index(id int,name varchar(10)) engine=innodb;</code></p><p><code>alter table tb_with_index add index id(id);</code></p><p><img src="https://vinxikk.github.io/img/mysql/mysql-lock-innodb-2.png" alt="有索引时，InnoDB使用行锁"></p></li><li><p>由于MySQL的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的，应用设计的时候要注意这一点。</p><p>在下面的例子中，表tb_with_index的id字段有索引，name字段没有索引：</p><p><code>alter table tb_with_index drop index name;</code></p><p><code>insert into tb_with_index  values(1,&#39;tom&#39;);</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from tb_with_index where id = 1;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| id   | name     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">|    1 | tom      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><p>InnoDB存储引擎使用相同索引键的阻塞例子：</p><p><img src="https://vinxikk.github.io/img/mysql/mysql-lock-innodb-3.png" alt="使用相同索引时，阻塞"></p></li><li><p>当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，另外，不论是使用主键索引、唯一索引或普通索引，InnoDB都会使用行锁来对数据加锁。</p><p>在下面的例子中，表tab_with_index的id字段有主键索引，name字段有普通索引：</p><p><code>alter table tb_with_index add index name(name);</code></p><p>InnoDB存储引擎的表使用不同索引的阻塞例子：</p><p><img src="https://vinxikk.github.io/img/mysql/mysql-lock-innodb-4.png" alt="使用不同索引时，阻塞"></p></li><li><p>即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的。如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。</p><p>比如，在tb_with_index表里的name字段有索引，但是name字段是varchar类型的，检索值的数据类型与索引字段不同，虽然MySQL能够进行数据类型转换，但却不会使用索引，从而导致InnoDB使用表锁。通过用explain检查两条SQL的执行计划，我们可以清楚地看到了这一点。</p><p><code>explain select * from tb_with_index where name = 1 \G</code></p><p><code>explain select * from tb_with_index where name = &#39;zhangsan&#39; \G</code></p></li></ol><h4 id="间隙锁（Next-Key）"><a href="#间隙锁（Next-Key）" class="headerlink" title="间隙锁（Next-Key）"></a>间隙锁（Next-Key）</h4><p>当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的 索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁 （Next-Key锁）。</p><p>举例来说，假如emp表中只有101条记录，其empid的值分别是 1,2,…,100,101，下面的SQL：</p><p><code>select * from emp where empid &gt; 100 for update;</code></p><p>是一个范围条件的检索，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。</p><p>InnoDB使用间隙锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求，对于上面的例子，要是不使用间隙锁，如果其他事务插入了empid大于100的任何记录，那么本事务如果再次执行上述语句，就会发生幻读；另外一方面，是为了满足其恢复和复制的需要。</p><p>很显然，在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，在实际应用开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。</p><p>还要特别说明的是，InnoDB除了通过范围条件加锁时使用间隙锁外，如果使用相等条件请求给一个不存在的记录加锁，InnoDB也会使用间隙锁。下面这个例子假设emp表中只有101条记录，其empid的值分别是1,2,……,100,101。</p><p>InnoDB存储引擎的间隙锁阻塞例子：</p><p><img src="https://vinxikk.github.io/img/mysql/mysql-lock-innodb-5.png" alt="间隙锁阻塞"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于MyISAM的表锁：</p><ol><li>共享读锁（S）之间是兼容的，但共享读锁（S）与排他写锁（X）之间，以及排他写锁（X）之间是互斥的，也就是说读和写是串行的。</li><li>在一定条件下，MyISAM允许查询和插入并发执行，我们可以利用这一点来解决应用中对同一表查询和插入的锁争用问题。</li><li>MyISAM默认的锁调度机制是写优先，这并不一定适合所有应用，用户可以通过设置LOW_PRIORITY_UPDATES参数，或在INSERT、UPDATE、DELETE语句中指定LOW_PRIORITY选项来调节读写锁的争用。</li><li>由于表锁的锁定粒度大，读写之间又是串行的，因此，如果更新操作较多，MyISAM表可能会出现严重的锁等待，可以考虑采用InnoDB表来减少锁冲突。</li></ol><p>对于InnoDB表：</p><ol><li>InnoDB的行锁是基于索引实现的，如果不通过索引访问数据，InnoDB会使用表锁。</li><li>在不同的隔离级别下，InnoDB的锁机制和一致性读策略不同。</li></ol><p>在了解InnoDB锁特性后，用户可以通过设计和SQL调整等措施减少锁冲突和死锁，包括：</p><ul><li>尽量使用较低的隔离级别； 精心设计索引，并尽量使用索引访问数据，使加锁更精确，从而减少锁冲突的机会；</li><li>选择合理的事务大小，小事务发生锁冲突的几率也更小；</li><li>给记录集显式加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁；</li><li>不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大大减少死锁的机会；</li><li>尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响； 不要申请超过实际需要的锁级别；除非必须，查询时不要显示加锁；</li><li>对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能。</li></ul>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL的聚簇索引和二级索引</title>
      <link href="/2017/08/17/mysql/mysql-index/"/>
      <url>/2017/08/17/mysql/mysql-index/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL的索引"><a href="#MySQL的索引" class="headerlink" title="MySQL的索引"></a>MySQL的索引</h3><p>每个InnoDB表具有一个特殊的索引，称为聚集索引。</p><p>如果表中定义有主键，该主键索引就是聚集索引。如果未定义主键，MySQL取第一个唯一索引（unique）而且只含非空列（NOT NULL）作为主键，InnoDB使用它作为聚集索引。如果没有这样的列，InnoDB就自己产生一个这样的ID值，它有六个字节，而且是隐藏的，使其作为聚集索引。</p><p>表中的聚集索引（clustered index）就是一级索引，除此之外，表中的其它非聚集索引都是二级索引（secondary indexes），又叫辅助索引。</p><p>二级索引建立在经常使用的列上。在统计一个表总的精确行数<code>count(*)</code>，优化器就会选择表中最小的索引来作为统计目标索引，因为它占用空间最小，IO也会最小。</p><p><strong>聚簇索引（主键索引）</strong>：索引数据和存储数据都在同一颗树上，比如根据id查找，只要找到该主键就找到数据了。</p><p><strong>二级索引（辅助索引）：</strong>比如根据name查找某个商品，就会先找到这个商品对应的主键id，然后根据id再去查找该商品。</p><p><strong>回表：</strong></p><p>回表就是在使用二级索引时，因为二级索引存储了部分数据，如果根据键值查找到的数据不能包括全部目标数据，就需要二级索引指针，也就是键值对中的值，来找到聚簇索引的所有数据，然后根据完整的数据取出所需要列的过程。</p><p>覆盖索引不需要回表。</p><h3 id="MySQL的聚簇索引"><a href="#MySQL的聚簇索引" class="headerlink" title="MySQL的聚簇索引"></a>MySQL的聚簇索引</h3><p>一个表可以建立多个索引，但每一个表都有一个存储了所有数据的索引。聚簇索引在每个表中只有一个，且是建立在主键上面的，这个主键包含的列可以是被隐藏的rowid列，也可以是自增列，还可以被明确定义为不含NULL值的组合列等，也称为聚集索引。</p><h3 id="MySQL的二级索引"><a href="#MySQL的二级索引" class="headerlink" title="MySQL的二级索引"></a>MySQL的二级索引</h3><p>二级索引又称为辅助索引、非聚集索引（no-clustered index），B+ tree结构。</p><p>二级索引的叶子节点不保存记录中的所有列，其叶子节点保存的是<code>&lt;键值, (记录)地址&gt;</code>。类似聚集索引中非叶子节点保存的信息，不同的是二级索引保存的是记录地址，而聚集索引保存的是下一层节点地址。</p><p>记录的地址一般可以保存为两种形式：</p><ol><li>记录的物理地址，页号:槽号:偏移量</li><li>记录的主键值</li></ol><p>InnoDB引擎是索引组织表，所有记录都放在聚集索引里，因此其辅助索引中的记录地址存放的是主键的键值。</p><p>二级索引，叶子节点中存储主键值，每次查找数据时，根据索引找到叶子节点中的主键值，根据主键值再到聚簇索引中得到完整的一行记录。</p><h4 id="InnoDB读操作"><a href="#InnoDB读操作" class="headerlink" title="InnoDB读操作"></a>InnoDB读操作</h4><p>通过二级索引查询记录仅能得到主键值，要查询完整的记录还需要再通过一次聚集索引查询，这种查询方式为书签查找（bookmark lookup）。</p><p><strong>聚集索引：</strong></p><p>例如查字典，每个字所在的位置有一个页码，如果知道一个字的所在具体页数便可以直接翻到相应的页，此时可以把字段中的页码看成主键。</p><p>字典的正文就是一个查询目录而不需要再通过其他目录来查找。</p><p>这种正文本身具有一定的规则排序的目录称为聚集索引。</p><p><strong>二级索引：</strong></p><p>此操作就如查字典，先找到生字的偏旁部首，查找到字在哪一页（获得主键的位置），然后在翻到生字的具体章节页。</p><h4 id="InnoDB写操作"><a href="#InnoDB写操作" class="headerlink" title="InnoDB写操作"></a>InnoDB写操作</h4><p>仅当主键发生改变时，才会更新二级索引。</p><p>二级索引的非叶子节点存放的记录格式为<code>&lt;键值,主键值,地址&gt;</code>，二级索引的非叶子节点依然存在主键信息。二级索引节点的记录不保存隐藏列xid和roll ptr。聚集索引的非叶子节点保存的是下一层节点地址。</p><p>由于二级索引不包含记录的完整信息，在InnoDB存储引擎中二级索引的树高度比聚集索引的树高度小，二级索引效率低。</p><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol><li><p>相比于叶子节点中存储行指针，二级索引存储主键值会占用更多的空间，那为什么要这样设计呢？</p><p>InnoDB在移动行时，无需维护二级索引，因为叶子节点中存储的是主键值，而不是指针。</p></li><li><p>InnoDB有了聚簇索引，为什么还要有二级索引？</p><p>聚簇索引的叶子节点存储了一行完整的数据，而二级索引只存储了主键值，相比于聚簇索引，占用的空间要少。当需要为表建立多个索引时，如果都是聚簇索引，那将占用大量内存空间。InnoDB中主键所建立的是聚簇索引，而唯一索引、普通索引、前缀索引等都是二级索引。</p></li><li><p>为什么一般情况下，建表的时候都会使用一个自增的id作为主键？</p><p>InnoDB表中的数据是直接存储在主键聚簇索引的叶子节点中的，每插入一条记录，其实都是增加一个叶子节点，如果主键是顺序的，只需要把新增的一条记录存储在上一条记录的后面，当页达到最大填充因子的时候，下一条记录就会写入新的页中，这种情况下，主键页就会近似于被顺序的记录填满。</p><p>若表的主键不是顺序的id，而是无规律数据，比如字符串，InnoDB无法简单地把一行记录插入到索引的最后，而是需要找一个合适的位置（已有数据的中间位置），甚至产生大量的页分裂并且移动大量数据，在寻找合适位置进行插入时，目标也可能不在内存中，这就导致了大量的随机IO操作，影响插入效率。除此之外，大量的页分裂会导致大量的内存碎片。</p></li></ol><h3 id="索引设计"><a href="#索引设计" class="headerlink" title="索引设计"></a>索引设计</h3><p>任务性能涉及：</p><ol><li>内存大小</li><li>CPU运算速度</li><li>IO速度</li></ol><p>索引是一种存储方式，与此相关的最重要部分是磁盘，所以磁盘的性能直接影响数据在数据库中的查询效率。索引的设计必须要尽可能地降低无效数据的读写访问。</p><p>关系型数据库的结构特点：</p><ol><li>数据都是以行为单位存储的，一行中包括一个表（聚簇索引）或一个索引（二级索引）中定义的所有类，多行数据可以连续一起存储。</li><li>一行数据一般会有一个键，以及其他附属的列，称之为值，可以理解为一行数据就是一个键值对。如果没有主键内部会加上一个rowid值的一个主键，默认主键，一样是一个键值对。</li><li>在键值对中，键值可以排序还可以组合键值。B+树中，非叶子节点存储了行数据中的键，而叶子节点存储了所有的行数据。通过非叶子节点的键值及一个位置信息，非叶子节点与下层节点或叶子节点之间的指针，就可以找到其孩子节点。</li></ol><h3 id="B-树和B树的区别"><a href="#B-树和B树的区别" class="headerlink" title="B+树和B树的区别"></a>B+树和B树的区别</h3><p>InnoDB是使用B+树来实现索引功能的。</p><p>区别：</p><ol><li><p>B树中的同一键不会出现多次，可能在叶子节点上也可能在非叶子节点上；</p><p>B+树的键一定会出现在叶子节点上，同时也可能在非叶子节点上重复出现。</p><p>简单的说，B+树的非叶子节点存储的都是键值，键值对应的具体数据都存储在叶子节点上。</p></li><li><p>B树的每个节点存储的是真实数据，会导致每个节点的存储的数据量变小，所以整个B树的高度会相对变高。随着数据量的变大，维护代价也增加；</p><p>B+树的非叶子节点只存储的是键值，相对而言，一个非叶子节点存储的记录个数要比B树多的多。 B+树是横向扩展，随着数据增加，会变成一个矮胖子，B树是纵向扩展，最终树的高度越来越高（高瘦子）。</p></li><li><p>B树的查询效率与键在B树的位置有关系，在叶子节点的时候最大复杂度与B+树相同；B+树复杂度对某个建成的树是固定的。</p></li><li><p>B树的键的位置不固定并且整个树结构中只出现一次，使得增删改查操作复杂度增加；B+树种，非叶子节点对于叶子节点来说就像一个索引，增删改的时候只要找到键值（索引）的位置，再一层层的向下找即可，只有在遇到一个节点存储满了会对B+树分裂。</p></li><li><p>B树中所有的数据都只存储一份；B+树除存储了所有数据的叶子节点外，还有之存储键值数据的非叶子节点。所以，B+树比B树会多占存储空间，多占的空间就是B+树的非叶子节点的所有空间。</p></li></ol><hr><p>参考：</p><p><a href="https://www.cnblogs.com/hustcat/archive/2009/10/28/1591648.html" target="_blank" rel="noopener">https://www.cnblogs.com/hustcat/archive/2009/10/28/1591648.html</a></p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maven项目打包为jar的几种方式</title>
      <link href="/2017/08/11/java/maven-package/"/>
      <url>/2017/08/11/java/maven-package/</url>
      
        <content type="html"><![CDATA[<h3 id="直接打包，不打包依赖包"><a href="#直接打包，不打包依赖包" class="headerlink" title="直接打包，不打包依赖包"></a>直接打包，不打包依赖包</h3><p>直接打包，不打包依赖包，仅打包出项目中的代码到jar包中。</p><p>在pom中添加如下plugin即可，随后执行maven install。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr></table></figure><h3 id="将依赖jar包输出到lib目录方式"><a href="#将依赖jar包输出到lib目录方式" class="headerlink" title="将依赖jar包输出到lib目录方式"></a>将依赖jar包输出到lib目录方式</h3><p>将项目中的jar包的依赖包输出到指定的目录下，修改outputDirectory配置，如下面的<code>${project.build.directory}/lib</code>。如想将打包好的jar包可以通过命令直接运行，如<code>java -jar xxx.jar</code>，还需要制定manifest配置的classpathPrefix与上面配置的相对应，如上面把依赖jar包输出到了lib，则这里的classpathPrefix也应指定为lib/；同时，并指定出程序的入口类，在配置mainClass节点中配好入口类的全类名。</p><p>这种打包方式对于Java项目是通用的，不管是SpringBoot的项目还是传统的Java项目，都可行。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- java编译插件 --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-jar-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">addClasspath</span>&gt;</span>true<span class="tag">&lt;/<span class="name">addClasspath</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">classpathPrefix</span>&gt;</span>lib/<span class="tag">&lt;/<span class="name">classpathPrefix</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.yourpakagename.mainClassName<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-dependency-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>copy<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">phase</span>&gt;</span>install<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>copy-dependencies<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/lib<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span></pre></td></tr></table></figure><p>有时为了方便，可以把classpath指定在当前目录上，默认的classpath会在jar包内，可以在main方法配置后加上manifestEntries配置，指定classpath，如：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-jar-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">classesDirectory</span>&gt;</span>target/classes/<span class="tag">&lt;/<span class="name">classesDirectory</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">archive</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">manifest</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">&lt;!-- 主函数的入口 --&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.yourpakagename.mainClassName<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">&lt;!-- 打包时 MANIFEST.MF文件不记录的时间戳版本 --&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">useUniqueVersions</span>&gt;</span>false<span class="tag">&lt;/<span class="name">useUniqueVersions</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">addClasspath</span>&gt;</span>true<span class="tag">&lt;/<span class="name">addClasspath</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">classpathPrefix</span>&gt;</span>lib/<span class="tag">&lt;/<span class="name">classpathPrefix</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">manifestEntries</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">Class-Path</span>&gt;</span>.<span class="tag">&lt;/<span class="name">Class-Path</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">manifestEntries</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">archive</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr></table></figure><h3 id="将项目依赖包和项目打为一个包"><a href="#将项目依赖包和项目打为一个包" class="headerlink" title="将项目依赖包和项目打为一个包"></a>将项目依赖包和项目打为一个包</h3><p>这种打包方式会将项目中的依赖包和项目代码都打为一个jar包，其配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.5.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">archive</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">manifest</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.xxg.Main<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">archive</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr></table></figure><p>此种方式对于传统的Java项目打包没问题，但是打有Spring框架的jar包就不可以了。可以采用maven-shade-plugin的插件来打包，实现Spring框架的打包。</p><h3 id="SpringBoot项目打包"><a href="#SpringBoot项目打包" class="headerlink" title="SpringBoot项目打包"></a>SpringBoot项目打包</h3><p>SpringBoot项目打包最常用且最简单的方式是用SpringBoot的打包plugin。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr></table></figure><p>在pom中加入此插件，再点击maven install或package就会把当前项目里所有依赖包和当前项目的源码都打成一个jar包，同时还会将没有依赖包的jar包也打出来。</p>]]></content>
      
      
      <categories>
          
          <category> Maven </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Maven-Package </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>零拷贝的实现原理</title>
      <link href="/2017/08/09/java/zero-copy/"/>
      <url>/2017/08/09/java/zero-copy/</url>
      
        <content type="html"><![CDATA[<h3 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h3><p>伪代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">File.read(file, buf, len);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Socket.send(socket, buf, len);</span></pre></td></tr></table></figure><p>这种方式一共涉及了4次数据拷贝。</p><p><img src="https://vinxikk.github.io/img/java/file-4-copy.png" alt="文件拷贝流程"></p><ol><li>应用程序中调用<code>read()</code>方法，这里会涉及到一次上下文切换（用户态 -&gt; 内核态），底层采用DMA（direct memory access）读取磁盘的文件，并把内容存储到内核地址空间的读取缓存区。</li><li>由于应用程序无法读取内核地址空间的数据，如果应用程序要操作这些数据，必须把这些内容从读取缓冲区拷贝到用户缓冲区。这个时候，<code>read()</code>调用返回，且引发一次上下文切换（内核态 -&gt; 用户态），现在数据已经被拷贝到了用户地址空间缓冲区，这时，如果有需要，应用程序可以操作修改这些内容。</li><li>我们最终目的是把这个文件内容通过Socket传到另一个服务中，调用Socket的<code>send()</code>方法，这里又涉及到一次上下文切换（用户态 -&gt; 内核态），同时，文件内容被进行第三次拷贝，被再次拷贝到内核地址空间缓冲区，但是这次的缓冲区与目标套接字相关联，与读取缓冲区没有半点关系。</li><li><code>send()</code>调用返回，引发第四次的上下文切换，同时进行第四次的数据拷贝，通过DMA把数据从目标套接字相关的缓存区传到协议引擎进行发送。</li></ol><p>在整个过程中，过程1和4是由DMA负责，并不会消耗CPU，只有过程2和3的拷贝需要CPU参与。</p><p>如果在应用程序中，不需要操作内容，过程2和3就是多余的，如果可以直接把内核态读取缓存区数据直接拷贝到套接字相关的缓存区，就可以达到优化的目的。</p><p><img src="https://vinxikk.github.io/img/java/file-3-copy.png" alt="文件拷贝流程"></p><p>这种实现，可以有以下几点改进：</p><ul><li>上下文切换的次数从四次减少到了两次</li><li>数据拷贝次数从四次减少到了三次（其中DMA copy 2次，CPU copy 1次）</li></ul><p>在Java中，<code>FileChannel</code>的<code>transferTo()</code>方法可以实现这个过程，该方法将数据从文件通道传输到给定的可写字节通道，上面的<code>file.read()</code>和<code>socket.send()</code>调用动作可以替换为<code>transferTo()</code>调用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transferTo</span><span class="params">(<span class="keyword">long</span> position, <span class="keyword">long</span> count, WritableByteChannel target)</span></span>;</span></pre></td></tr></table></figure><p>在UNIX和各种Linux系统中，此调用被传递到sendfile()系统调用中，最终实现将数据从一个文件描述符传输到了另一个文件描述符。</p><p>如果底层网络接口卡支持收集操作的话，可以进一步的优化。</p><p>在Linux内核2.4及后期版本中，针对套接字缓冲区描述符做了相应调整，DMA自带了收集功能，对于用户方面，用法还是一样的，但是内部操作已经发生了改变：</p><p><img src="https://vinxikk.github.io/img/java/file-2-copy.png" alt="文件拷贝流程"></p><p>第一步，transferTo()方法引发DMA将文件内容拷贝到内核读取缓冲区。</p><p>第二步，把包含数据位置和长度信息的描述符追加到套接字缓冲区，避免了内容整体的拷贝，DMA引擎直接把数据从内核缓冲区传到协议引擎，从而消除了最后一次CPU参与的拷贝动作。</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zero-Copy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL常用的存储引擎及其区别</title>
      <link href="/2017/07/17/mysql/mysql-engines/"/>
      <url>/2017/07/17/mysql/mysql-engines/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL中的存储引擎"><a href="#MySQL中的存储引擎" class="headerlink" title="MySQL中的存储引擎"></a>MySQL中的存储引擎</h3><p>存储引擎其实就是对于数据库文件的一种存取机制，如何实现存储数据，如何为存储的数据建立索引以及如何更新、查询数据。</p><p>MySQL中的存储引擎主要有：MyISAM、InnoDB、MEMORY、ARCHIVE、CSV等。</p><ul><li><p><code>show engines \g;</code>  – 查看mysql所支持的存储引擎</p><p><img src="https://vinxikk.github.io/img/mysql/mysql-engines.png" alt="查看MySQL中的存储引擎"></p></li><li><p><code>show variables like &#39;%storage_engine&#39;;</code>  – 查看mysql默认的存储引擎</p></li><li><p><code>show create table table tb_name \G;</code>  – 查看建表语句，可以看到所使用的存储引擎</p></li><li><p><code>show table status from db_name where name=&quot;tb_name&quot; \G;</code>  –查看某个数据库中的某张表的详细信息，包括所使用的存储引擎</p></li></ul><h3 id="存储引擎的区别"><a href="#存储引擎的区别" class="headerlink" title="存储引擎的区别"></a>存储引擎的区别</h3><h4 id="MyISAM"><a href="#MyISAM" class="headerlink" title="MyISAM"></a>MyISAM</h4><p>MySQL5.6之前的默认引擎，最常用的非事务型存储引擎。</p><p>不支持事务，也不支持外键，优势是访问速度快，对事务完整性没有要求或者以select，insert为主的应用基本上可以用这个引擎来创建表。</p><p>数据是以堆表的方式进行存储的，数据没有特定顺序，不存在聚集索引的概念，读写都会对数据进行加锁。</p><p>支持3种不同的存储格式，分别是：静态表，动态表，压缩表。</p><p>静态表：表中的字段都是非变长字段，这样每个记录都是固定长度的，优点存储非常迅速，容易缓存，出现故障容易恢复；缺点是占用的空间通常比动态表多（因为存储时会按照列的宽度定义补足空格）。在取数据的时候，默认会把字段后面的空格去掉，如果不注意会把数据本身带的空格也会忽略。</p><p>动态表：记录不是固定长度的，这样存储的优点是占用的空间相对较少；缺点：频繁的更新、删除数据容易产生碎片，需要定期执行<code>OPTIMIZE TABLE</code>或者<code>myisamchk-r</code>命令来改善性能。</p><p>压缩表：因为每个记录是被单独压缩的，所以只有非常小的访问开支。</p><h4 id="InnoDB"><a href="#InnoDB" class="headerlink" title="InnoDB"></a>InnoDB</h4><p>MySQL5.6之后的默认引擎，最常用的事务型存储引擎。</p><p>该存储引擎提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比MyISAM引擎，写的处理效率会差一些，并且会占用更多的磁盘空间以保留数据和索引。</p><p>特点：支持事务，行级锁，外键约束，自动增长列。</p><h4 id="MEMORY"><a href="#MEMORY" class="headerlink" title="MEMORY"></a>MEMORY</h4><p>使用存在于内存中的内容来创建表，是一种易失性非事务型存储引擎，读写速度快。</p><p>每个memory表只实际对应一个磁盘文件，格式是.frm。memory类型的表访问非常的快，因为它的数据是放在内存中的，并且默认使用HASH索引，但是一旦服务关闭，表中的数据就会丢失掉。 </p><p>MEMORY存储引擎的表可以选择使用BTREE索引或者HASH索引，两种不同类型的索引有其不同的使用范围。</p><p>Hash索引优点：</p><p>Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。</p><p>Hash索引缺点： </p><p>因为hash算法是基于等值计算的，所以对于“like”等范围查找，hash索引无效，不支持。</p><p>Memory类型的存储引擎主要用于哪些内容变化不频繁的代码表，或者作为统计操作的中间结果表，便于高效地对中间结果进行分析并得到最终的统计结果。</p><p>对存储引擎为memory的表进行更新操作要谨慎，因为数据并没有实际写入到磁盘中，所以一定要对下次重新启动服务后如何获得这些修改后的数据有所考虑。</p><h4 id="ARCHIVE"><a href="#ARCHIVE" class="headerlink" title="ARCHIVE"></a>ARCHIVE</h4><p>只允许查询和新增数据而不允许修改的非事务型存储引擎。</p><p>该存储引擎非常适合存储大量独立的、作为历史记录的数据。区别于InnoDB和MyISAM这两种引擎，ARCHIVE提供了压缩功能，拥有高效的插入速度，但是这种引擎不支持索引，所以查询性能较差一些。</p><h4 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h4><p>以CSV格式存储的非事务型存储引擎。</p><p>由于不支持事务，读写时，会对整个表进行加锁。主要用于进行不同系统间的数据交换，不建议作为存储核心业务系统数据库的存储引擎。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据库的范式</title>
      <link href="/2017/07/16/mysql/mysql-normal-form/"/>
      <url>/2017/07/16/mysql/mysql-normal-form/</url>
      
        <content type="html"><![CDATA[<h3 id="数据库的范式"><a href="#数据库的范式" class="headerlink" title="数据库的范式"></a>数据库的范式</h3><p>范式来自英文Normal Form，简称NF。要想设计一个好的关系，必须使关系满足一定的约束条件，此约束已经形成了规范，分成几个等级，一级比一级要求得严格。满足这些规范的数据库是简洁的、结构明晰的，同时，不会发生插入(insert)、删除(delete)和更新(update)操作异常。反之则是乱七八糟，可能存储了大量不需要的冗余信息。</p><p>目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式）。满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范要求的称为第二范式（2NF），其余范式以次类推。一般来说，数据库只需满足第三范式(3NF）就行了。</p><p><strong>第一范式：</strong></p><p>每一数据项是不可再分的，不能是数组或者集合。</p><p><strong>第二范式：</strong></p><p>满足第一范式，同时，所有非主属性依赖于全部候选键。</p><p>例如：(A、B)为主属性，如果非主属性C可以由B推出，而不需要A，这个时候，C就不完全依赖去所有性（仅仅是部分依赖于B），所以不满足第二范式。</p><p><strong>第三范式：</strong></p><p>满足第二范式，同时所有非主属性间不存在依赖传递。</p><p>比如，(A、B)是主属性，C、D是非主属性，但是C可以推出D，这个时候非主属性存在了传递依赖，就不满足第三范式了。</p><p><strong>BC范式：</strong></p><p>是第三范式的纠正，满足第三范式，同时所有属性间不存在传递依赖（相对于第三范式，就是主属性间也不能存在传递依赖）。</p><p><strong>第四范式：</strong></p><p>满足BC范式，同时属性间不允许多指依赖。</p><p>比如，一个关系中有用户名和手机号，但是一个用户名对应多个手机号，这个时候就不满足第四范式。</p><p><code>emp(empno, ename, gender, birthday, tel)</code>这个就满足BC范式，但是，一个员工存在多个电话，这个时候，怎么修改成第四范式呢，如下：</p><p><code>emp(empno, ename, gender, birthday)</code></p><p><code>emptel(empno, tel)</code></p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java的IO：BIO、NIO和AIO</title>
      <link href="/2017/07/09/java/java-io/"/>
      <url>/2017/07/09/java/java-io/</url>
      
        <content type="html"><![CDATA[<h3 id="Java-IO"><a href="#Java-IO" class="headerlink" title="Java IO"></a>Java IO</h3><p>IO，即Input/Output，通常指数据在内存和硬盘或其他设备之间的输入和输出。</p><p>在Java中，提供了一些API，可以供开发者来读写外部数据或文件，这些API称为Java IO。</p><p>Java中有三种IO并存，分别是BIO、NIO和AIO。</p><h4 id="Java-BIO"><a href="#Java-BIO" class="headerlink" title="Java BIO"></a>Java BIO</h4><p>BIO，即Block-IO，是一种同步且阻塞的通信模式。是一个比较传统的通信方式，但并发处理能力低、通信耗时。</p><h4 id="Java-NIO"><a href="#Java-NIO" class="headerlink" title="Java NIO"></a>Java NIO</h4><p>NIO，即Non-Block，是Java SE 1.4后，针对网络传输优化的新功能，是一种非阻塞同步的通信模式。</p><p>NIO与原来的I/O有同样的作用和目的，他们之间最重要的区别是数据打包和传输的方式。原来的I/O以流的方式处理数据，而NIO以块的方式处理数据。</p><p>面向流的I/O系统一次一个字节地处理数据，一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。</p><p>面向块的I/O系统以块的形式处理数据，每一个操作都在一步中产生或消费一个数据块。按块处理数据比按（流式的）字节处理数据要快得多，但是面向块的I/O缺少一些面向流的I/O所具有的优雅和简单。</p><h4 id="Java-AIO"><a href="#Java-AIO" class="headerlink" title="Java AIO"></a>Java AIO</h4><p>AIO，即 Asynchronous IO，是异步非阻塞的IO。在NIO的基础上引入了新的异步通道的概念，并提供了异步文件通道和异步套接字通道的实现。</p><h4 id="三种IO的区别"><a href="#三种IO的区别" class="headerlink" title="三种IO的区别"></a>三种IO的区别</h4><p>BIO （Blocking I/O）：同步阻塞I/O模式。</p><p>NIO （New I/O）：同步非阻塞模式。</p><p>AIO （Asynchronous I/O）：异步非阻塞I/O模型。</p><p><strong>适用场景：</strong></p><p>BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序简单易理解。</p><p>NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。</p><p>AIO方式适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="BIO"><a href="#BIO" class="headerlink" title="BIO"></a>BIO</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java-IO </tag>
            
            <tag> BIO </tag>
            
            <tag> NIO </tag>
            
            <tag> AIO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL DELETE语法错误</title>
      <link href="/2017/06/19/mysql/mysql-delete-error/"/>
      <url>/2017/06/19/mysql/mysql-delete-error/</url>
      
        <content type="html"><![CDATA[<h4 id="DELETE语法错误"><a href="#DELETE语法错误" class="headerlink" title="DELETE语法错误"></a>DELETE语法错误</h4><p>错误信息如下：</p><p><img src="https://vinxikk.github.io/img/mysql/mysql-delete-error.png" alt="错误语句"></p><p>错误原因：</p><p><img src="https://vinxikk.github.io/img/mysql/mysql-delete-error-answer.png" alt="错误原因"></p><p>解决办法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="string">`7-team`</span> <span class="keyword">where</span> <span class="keyword">ID</span> <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">min</span>(<span class="keyword">ID</span>) <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> <span class="string">`7-team`</span>) <span class="keyword">as</span> b <span class="keyword">group</span> <span class="keyword">by</span> b.Name);</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL-ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL行转列和列转行</title>
      <link href="/2017/05/17/mysql/mysql-row-column/"/>
      <url>/2017/05/17/mysql/mysql-row-column/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL行转列、列转行"><a href="#MySQL行转列、列转行" class="headerlink" title="MySQL行转列、列转行"></a>MySQL行转列、列转行</h3><p>行转列主要用于对数据进行聚合统计，如统计某类目的商品在某个时间区间的销售情况。</p><p>列转行主要用于将一条数据拆分成多条。</p><p>数据准备：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ws_view(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">not</span> <span class="literal">null</span> auto_increment,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">website <span class="built_in">varchar</span>(<span class="number">80</span>) <span class="keyword">default</span> <span class="literal">null</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">date</span> <span class="built_in">date</span> <span class="keyword">default</span> <span class="literal">null</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">pageview <span class="built_in">int</span>(<span class="number">11</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">primary <span class="keyword">key</span> (<span class="keyword">id</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-01'</span>,<span class="number">100</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">2</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-01'</span>,<span class="number">200</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">3</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-02'</span>,<span class="number">300</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">4</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-02'</span>,<span class="number">350</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">5</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-03'</span>,<span class="number">310</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">6</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-03'</span>,<span class="number">360</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">7</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-04'</span>,<span class="number">350</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">8</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-04'</span>,<span class="number">380</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">9</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-01'</span>,<span class="number">800</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">10</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-01'</span>,<span class="number">700</span>);</span></pre></td></tr></table></figure><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>主要思路是分组后使用case进行条件判断。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    a.date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">sum</span>(<span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Baidu'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">end</span>) <span class="string">'sum_Baidu'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">max</span>(<span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Baidu'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">end</span>) <span class="string">'max_Baidu'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">sum</span>(<span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Google'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">end</span>) <span class="string">'sum_Google'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">max</span>(<span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Google'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">end</span>) <span class="string">'max_Google'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    ws_view a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="built_in">date</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------+-----------+-----------+------------+------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">| date       | sum_Baidu | max_Baidu | sum_Google | max_Google |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------+-----------+-----------+------------+------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">| 2013-09-01 |       900 |       800 |        900 |        700 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">| 2013-09-02 |       300 |       300 |        350 |        350 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">| 2013-09-03 |       310 |       310 |        360 |        360 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">| 2013-09-04 |       350 |       350 |        380 |        380 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------+-----------+-----------+------------+------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>主要思路也是分组后使用case。</p><p>求每天的pageview总数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  a.date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">concat</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="string">'Baidu:'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">cast</span>(<span class="keyword">sum</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> a.website </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">when</span> <span class="string">'Baidu'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"> ) <span class="keyword">as</span> <span class="built_in">char</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">';'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'Google'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">cast</span>(<span class="keyword">sum</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">when</span> <span class="string">'Google'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="built_in">char</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="string">'sum_str'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    ws_view a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.date;</span></pre></td></tr></table></figure><p>使用mysql提供的函数分组：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a.date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a.website, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group_concat</span>(a.website, <span class="string">'_sum:'</span>, a.pageview) <span class="keyword">sum</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ws_view a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.date,a.website;</span></pre></td></tr></table></figure><p>普通group结合字符串拼接：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    a.date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">concat</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="string">'Baidu_sum:'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">cast</span>(<span class="keyword">sum</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Baidu'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"> ) <span class="keyword">as</span> <span class="built_in">char</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="string">'Baidu'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">concat</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="string">'Google_sum:'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">cast</span>(<span class="keyword">sum</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Google'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="built_in">char</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="string">'Google'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    ws_view a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.date;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL行列互转 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL的JOIN</title>
      <link href="/2017/05/15/mysql/mysql-join/"/>
      <url>/2017/05/15/mysql/mysql-join/</url>
      
        <content type="html"><![CDATA[<h3 id="JOIN的用法"><a href="#JOIN的用法" class="headerlink" title="JOIN的用法"></a>JOIN的用法</h3><hr><p>mysql中join主要用于两张表之间的连接，常用的有内连接、左连接、右连接。如下图：</p><p><img src="https://vinxikk.github.io/img/mysql/join.png" alt="JOIN的几种情况"></p><p>两张表：</p><ul><li>dept - 部门表</li><li>emp - 员工表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部门表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">dno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">dname <span class="built_in">varchar</span>(<span class="number">50</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> dept <span class="keyword">values</span> (<span class="number">10</span>,<span class="string">'A'</span>),(<span class="number">20</span>,<span class="string">'B'</span>),(<span class="number">30</span>,<span class="string">'C'</span>),(<span class="number">40</span>,<span class="string">'D'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 员工表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">eno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">ename <span class="built_in">varchar</span>(<span class="number">50</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">sal <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">dno <span class="built_in">int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> emp <span class="keyword">values</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>,<span class="string">'zhangsan'</span>,<span class="number">500</span>,<span class="number">10</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">(<span class="number">2</span>,<span class="string">'lisi'</span>,<span class="number">300</span>,<span class="number">20</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>,<span class="string">'wangwu'</span>,<span class="number">600</span>,<span class="number">30</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">(<span class="number">4</span>,<span class="string">'alice'</span>,<span class="number">400</span>,<span class="number">20</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">(<span class="number">5</span>,<span class="string">'tom'</span>,<span class="number">700</span>,<span class="number">10</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">(<span class="number">6</span>,<span class="string">'mike'</span>,<span class="number">800</span>,<span class="number">50</span>);</span></pre></td></tr></table></figure><h4 id="CROSS-JOIN-笛卡尔积"><a href="#CROSS-JOIN-笛卡尔积" class="headerlink" title="CROSS JOIN: 笛卡尔积"></a>CROSS JOIN: 笛卡尔积</h4><p>要理解各种join首先要理解笛卡尔积。</p><p>如果A表有m条记录，B表有n条记录，笛卡尔积的结果就是m*n条记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相当于没有指明连接条件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> emp.*,dept.dname <span class="keyword">from</span> emp <span class="keyword">cross</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> emp.*,dept.dname <span class="keyword">from</span> emp <span class="keyword">inner</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> emp.*,dept.dname <span class="keyword">from</span> emp,dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">24 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="INNER-JOIN-内连接"><a href="#INNER-JOIN-内连接" class="headerlink" title="INNER JOIN: 内连接"></a>INNER JOIN: 内连接</h4><p>内连接是最常用的连接操作，用于求两个表的交集，从笛卡尔积的角度看就是从笛卡尔积中挑出ON子句条件成立的记录。</p><p>写法有4种：INNER JOIN, JOIN(省略INNER), WHERE(等值连接), STRAIGHT_JOIN。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下SQL都是等效的，相当于上图中的第1中情况</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">inner</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e,dept d <span class="keyword">where</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">straight_join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="LEFT-JOIN-左连接"><a href="#LEFT-JOIN-左连接" class="headerlink" title="LEFT JOIN: 左连接"></a>LEFT JOIN: 左连接</h4><p>左连接就是求两张表的交集+左表剩下的数据。从笛卡尔积的角度讲，就是先从笛卡尔积中挑出ON子句条件成立的记录，然后加上左表中剩余的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图3</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> d.dno <span class="keyword">is</span> <span class="literal">null</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">| eno  | ename | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">|    6 | mike  |  800 |   50 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="RIGHT-JOIN-右连接"><a href="#RIGHT-JOIN-右连接" class="headerlink" title="RIGHT JOIN: 右连接"></a>RIGHT JOIN: 右连接</h4><p>同理右连接就是求两个表的交集+右表剩下的数据。从笛卡尔积的角度描述，就是从笛卡尔积中挑出ON子句条件成立的记录，然后加上右表中剩余的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图4</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">outer</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| NULL | NULL     | NULL | NULL | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图5</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.dno <span class="keyword">is</span> <span class="literal">null</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">| eno  | ename | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">| NULL | NULL  | NULL | NULL | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="两个集合的并集"><a href="#两个集合的并集" class="headerlink" title="两个集合的并集"></a>两个集合的并集</h4><p>就是从笛卡尔积中挑出ON子句条件成立的记录，然后加上左表中剩余的记录，再加上右表中剩余的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图6</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| NULL | NULL     | NULL | NULL | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">7 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- --------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图7</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> d.dno <span class="keyword">is</span> <span class="literal">null</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.dno <span class="keyword">is</span> <span class="literal">null</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">| eno  | ename | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">|    6 | mike  |  800 |   50 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">| NULL | NULL  | NULL | NULL | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="USING子句"><a href="#USING子句" class="headerlink" title="USING子句"></a>USING子句</h4><p>当两表中关联的列同名时，可以使用USING子句来简化ON语法，格式为：USING(col)。</p><p>SELECT * 时，USING会除去USING指定的列，而ON不会。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 等同于inner join</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from emp inner join dept using(dno);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| dno  | eno  | ename    | sal  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">|   10 |    1 | zhangsan |  500 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">|   20 |    2 | lisi     |  300 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">|   30 |    3 | wangwu   |  600 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|   20 |    4 | alice    |  400 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|   10 |    5 | tom      |  700 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> emp.*,dept.dname <span class="keyword">from</span> emp <span class="keyword">inner</span> <span class="keyword">join</span> dept <span class="keyword">using</span>(dno);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">inner</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="NATURAL-自然连接"><a href="#NATURAL-自然连接" class="headerlink" title="NATURAL: 自然连接"></a>NATURAL: 自然连接</h4><p>自然连接就是USING子句的简化版，它找出两张表中相同的列作为连接条件进行连接。</p><p>在dept和emp表中，相同的列是dno，所以会以dno为连接条件。</p><p>另外：</p><ul><li>自然连接：select * from emp natural join dept;</li><li>笛卡尔积：select * from emp natura join dept;</li><li>笛卡尔积：select * from emp nature join dept;</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自然连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">natural</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">inner</span> <span class="keyword">join</span> dept <span class="keyword">using</span>(dno);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.dno,e.eno,e.ename,e.sal,d.dname <span class="keyword">from</span> emp e </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.dno,e.eno,e.ename,e.sal,d.dname <span class="keyword">from</span> emp e,dept d </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| dno  | eno  | ename    | sal  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">|   10 |    1 | zhangsan |  500 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">|   20 |    2 | lisi     |  300 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|   30 |    3 | wangwu   |  600 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">|   20 |    4 | alice    |  400 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">|   10 |    5 | tom      |  700 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ---------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 左自然连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">natural</span> <span class="keyword">left</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">| dno  | eno  | ename    | sal  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">|   10 |    1 | zhangsan |  500 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">|   10 |    5 | tom      |  700 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">|   20 |    2 | lisi     |  300 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">|   20 |    4 | alice    |  400 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">|   30 |    3 | wangwu   |  600 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">|   50 |    6 | mike     |  800 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- --------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 右自然连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">natural</span> <span class="keyword">right</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+----------+------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">| dno  | dname | eno  | ename    | sal  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+----------+------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">|   10 | A     |    1 | zhangsan |  500 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">|   20 | B     |    2 | lisi     |  300 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">|   30 | C     |    3 | wangwu   |  600 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">|   20 | B     |    4 | alice    |  400 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">|   10 | A     |    5 | tom      |  700 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">|   40 | D     | NULL | NULL     | NULL |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+----------+------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL-JOIN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Centos6.5解决-bash: nc: command not found问题</title>
      <link href="/2017/04/16/linux/linux-netcat/"/>
      <url>/2017/04/16/linux/linux-netcat/</url>
      
        <content type="html"><![CDATA[<h4 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop002 ~]<span class="comment"># nc -lk 9999</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">-bash: nc: <span class="built_in">command</span> not found</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@hadoop002 ~]<span class="comment">#</span></span></pre></td></tr></table></figure><p>问题原因：Linux中netcat没有安装。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>root用户执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop002 ~]<span class="comment"># yum install nc -y</span></span></pre></td></tr></table></figure><p>再次测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop002 ~]<span class="comment"># nc -lk 9999</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">hello,world</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">hello,java</span></pre></td></tr></table></figure><p>问题解决。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux tree命令</title>
      <link href="/2017/04/14/linux/linux-tree/"/>
      <url>/2017/04/14/linux/linux-tree/</url>
      
        <content type="html"><![CDATA[<h4 id="安装tree"><a href="#安装tree" class="headerlink" title="安装tree"></a>安装tree</h4><p>需要以root用户进行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">yum install -y tree</span></pre></td></tr></table></figure><h4 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">yum list installed tree</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/linux/tree-installed.png" alt="tree安装成功"></p><h4 id="tree使用"><a href="#tree使用" class="headerlink" title="tree使用"></a>tree使用</h4><p>直接执行tree命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree</span></pre></td></tr></table></figure><p>查看tree命令参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree --<span class="built_in">help</span></span></pre></td></tr></table></figure><p>只查看当前目录下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 1</span></pre></td></tr></table></figure><p>区分文件和文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 1 -C</span></pre></td></tr></table></figure><p>列出权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 1 -C -p</span></pre></td></tr></table></figure><p>查看3层目录下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 3 -C -p</span></pre></td></tr></table></figure><p>列出相对路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 3 -C -p -f</span></pre></td></tr></table></figure><p>只列出文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 3 -C -p -d</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL常用语句&amp;CURD操作</title>
      <link href="/2017/03/17/mysql/mysql-basic/"/>
      <url>/2017/03/17/mysql/mysql-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL常用语句"><a href="#MySQL常用语句" class="headerlink" title="MySQL常用语句"></a>MySQL常用语句</h3><hr><p>MySQL语法主要分为3种：</p><ul><li>DDL - 定义: create, drop…</li><li>DML - 操作: insert, delete, update, select…</li><li>DCL - 控制</li></ul><h4 id="INSERT"><a href="#INSERT" class="headerlink" title="INSERT"></a>INSERT</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加单条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_a <span class="keyword">value</span> (col1,col2,...);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加多条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_a <span class="keyword">values</span> (col1,col2,...),(col1,col2,...)...;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将表b的查询结果插入到表a</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_a <span class="keyword">select</span> * <span class="keyword">from</span> tb_b;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建b表，并将a表中的数据导入b表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb_b <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> tb_a;</span></pre></td></tr></table></figure><h4 id="DELETE"><a href="#DELETE" class="headerlink" title="DELETE"></a>DELETE</h4><p>在生产上，使用DELETE语句时最好习惯性加上WHERE条件，执行操作需要谨慎，避免数据丢失的灾难。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除指定记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_a <span class="keyword">where</span> ctime=<span class="string">'2017-03-12'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除多条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_a <span class="keyword">where</span> ctime <span class="keyword">in</span> (<span class="string">'2017-03-12'</span>,<span class="string">'2017-03-13'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_a <span class="keyword">where</span> ctime &gt; <span class="string">'2017-03-12'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清空表数据，把where条件去掉</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 不带where的delete语句和truncate table语句都可以删除表中所有内容，效率上truncate比delete快，但truncate删除后不记录mysql日志，不可恢复数据。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- delete效果有点像将表中所有记录一条一条的删除，而truncate相当于保留表结构，重新创建了这个表，所有的状态都相当于新表。</span></span></pre></td></tr></table></figure><h4 id="UPDATE"><a href="#UPDATE" class="headerlink" title="UPDATE"></a>UPDATE</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新指定数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> tb_a <span class="keyword">set</span> col2=<span class="string">'test'</span> <span class="keyword">where</span> col1=<span class="number">2</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 在product_info表中增加列`product_count`，并更新其值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> product_info <span class="keyword">pi</span> <span class="keyword">set</span> pi.<span class="string">`product_count`</span> = (<span class="keyword">select</span> pio.<span class="string">`product_count_old`</span> <span class="keyword">from</span> product_info_old pio <span class="keyword">where</span> pio.<span class="string">`product_id_old`</span> = pi.<span class="string">`product_id`</span>) <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> <span class="literal">null</span> <span class="keyword">from</span> product_info_old pio <span class="keyword">where</span> pio.<span class="string">`product_id_old`</span> = pi.<span class="string">`product_id`</span>);</span></pre></td></tr></table></figure><h4 id="SELECT"><a href="#SELECT" class="headerlink" title="SELECT"></a>SELECT</h4><p>工作中使用最多的就是SELECT，同时可以结合JOIN, UNION等实现复杂的统计需求。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询全部记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询前5条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a <span class="keyword">limit</span> <span class="number">5</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询指定范围数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a <span class="keyword">limit</span> <span class="number">0</span>,<span class="number">2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a <span class="keyword">limit</span> <span class="number">1</span>,<span class="number">2</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1:记录起始位置，2：返回记录的条数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询最后一条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">1</span>;</span></pre></td></tr></table></figure><h4 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 完整复制表到另一个表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.复制表结构</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb_b <span class="keyword">like</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.导入数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_a <span class="keyword">select</span> * <span class="keyword">from</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看表结构（列）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">desc tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- alter -----------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># alter: 修改基本表，对表结构进行操作，比如对字段增加、删除、修改类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># update: 修改表中的数据，修改某一行某一列的值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改表名</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">rename</span> <span class="keyword">to</span> tb_new;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在列名1后添加列名2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">add</span> <span class="keyword">column</span> col_2 col_type <span class="keyword">after</span> col_1;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除列</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">drop</span> <span class="keyword">column</span> col_1;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改列名及数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">change</span> col_old col_new col_new_type;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改列属性</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">modify</span> <span class="keyword">column</span> col_1 attr_new;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- eg:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> product_info <span class="keyword">modify</span> <span class="keyword">column</span> pid <span class="built_in">varchar</span>(<span class="number">50</span>);</span></pre></td></tr></table></figure><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># char和varchar的区别</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># char: 定长，效率高，一般用于固定长度的表单提交数据存储，如：身份证号，手机号，密码等</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># varchar: 不定长，效率偏低</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 效率: char &gt; varchar &gt; text, 但如果使用的是InnoDB引擎的话，推荐使用varchar代替char。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- char和varchar可以有默认值，text不能指定默认值。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 对于int类型，如果不需要存取负值，最好加上unsigned。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 对于经常出现在where语句中的字段，考虑加索引，整型的尤其适合加索引。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 存储价格、金额 -------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用decimal(m,n)来精确表达价格。不要使用float/double等浮点数据类型，因为他们是不精确的，特别是在计算的时候。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">salary decimal(5,2)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 5(precision精度)：代表十进制数字的数目</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 2(scale数据范围)：代表小数点后的数字位数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 在这种情况下，salary列可以存储的值范围是从-99.99到99.99.实际上，mysql在这个列中可以存储的数值可以一直到999.99，因为它没有存储正数的符号。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- decimal和numeric值作为字符串存储，而不是作为二进制浮点数，以便保存那些值的小数精度。一个字符用于值的每一位、小数点（如果scale&gt;0）、“-”符号（对于负值）。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 不使用float或double的原因：float和double是以二进制存储的，所以有一定的误差。</span></span></pre></td></tr></table></figure><h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><p>更新一个包含索引的表比更新一个没有索引的表需要更多的时间，这是由于索引本身也需要更新。因此，理想的做法是仅仅在常常被搜索的列（以及表）上面创建索引。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为某列创建索引</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">index</span> col_index <span class="keyword">on</span> tb_a(col_1);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除某列的索引</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">index</span> col_index <span class="keyword">on</span> tb_a;</span></pre></td></tr></table></figure><h4 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> <span class="keyword">replace</span> <span class="keyword">view</span> view_name <span class="keyword">as</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.name,b.class <span class="keyword">from</span> tb_a a <span class="keyword">inner</span> <span class="keyword">join</span> tb_b b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> a.id=b.id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询视图数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> view_name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> view_name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看视图 --------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> information_schema.VIEWS;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> information_schema.TABLES;</span></pre></td></tr></table></figure><h4 id="存储过程和函数"><a href="#存储过程和函数" class="headerlink" title="存储过程和函数"></a>存储过程和函数</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存储过程</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">procedure</span> <span class="keyword">status</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 函数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">function</span> <span class="keyword">status</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看存储过程或函数的创建代码</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">procedure</span> proc_name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">function</span> func_name;</span></pre></td></tr></table></figure><h4 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">triggers</span> [<span class="keyword">from</span> db_name] [<span class="keyword">like</span> expr]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">triggers</span>\G</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对INFORMATION_SCHEMA数据库中的TRIGGERS表查询</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">triggers</span> T <span class="keyword">where</span> trigger_name=<span class="string">"mytrigger"</span>\G</span></pre></td></tr></table></figure><h4 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h4><p>事务是一个最小的不可再分的工作单元，通常一个事务对应一个完整的业务。例如：银行转账从A卡到B卡，需要扣除A中的钱，然后将钱加入到B中。</p><p>在mysql中如果需要使用事务，那么必须使用InnoDB存储引擎。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL语法 </tag>
            
            <tag> MySQL-CURD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础</title>
      <link href="/2017/02/11/linux/linux-basic/"/>
      <url>/2017/02/11/linux/linux-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux基础命令"><a href="#Linux基础命令" class="headerlink" title="Linux基础命令"></a>Linux基础命令</h3><hr><h4 id="xshell连接linux"><a href="#xshell连接linux" class="headerlink" title="xshell连接linux"></a>xshell连接linux</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以root用户连接至ip地址为ipaddr的linux系统</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">ssh root@ipaddr</span></pre></td></tr></table></figure><h4 id="查看ip"><a href="#查看ip" class="headerlink" title="查看ip"></a>查看ip</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">ifconfig</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Welcome to my blog</title>
      <link href="/2017/01/21/hello-world/"/>
      <url>/2017/01/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p><strong>“ 把问题写下来就已经解决了一半。</strong></p><p><img src="https://vinxikk.github.io/img/snow.png" alt="snow"></p>]]></content>
      
      
      <categories>
          
          <category> Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Other </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
