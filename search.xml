<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>推荐系统：协同过滤算法</title>
      <link href="/2018/08/11/rs/rs-collaborative-filtering/"/>
      <url>/2018/08/11/rs/rs-collaborative-filtering/</url>
      
        <content type="html"><![CDATA[<h3 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h3><p>协同过滤算法是诞生最早，且较为著名的推荐算法，主要用于预测和推荐。</p><p>算法通过对用户历史行为数据的挖掘发现用户的偏好，基于不同的偏好对用户进行群组划分并推荐品味相似的商品。</p><p>协同过滤推荐算法分为两类：基于用户的协同过滤算法（user-based collaborative filtering），基于物品的协同过滤算法（item-based collaborative filtering）。</p><p>简单的说就是：人以群分，物以类聚。</p><h3 id="基于用户的协同过滤算法"><a href="#基于用户的协同过滤算法" class="headerlink" title="基于用户的协同过滤算法"></a>基于用户的协同过滤算法</h3><p>基于用户的协同过滤算法，是通过用户的历史行为数据发现用户对商品或内容的喜好（如商品购买、收藏、内容评论或分享等），并对这些喜好进行度量和打分，根据不同用户对相同商品或内容的态度和偏好程度计算用户之间的关系，最后在有相同喜好的用户间进行商品推荐。</p><p>简单的说就是，假如A、B两个用户都购买了x、y、z三本图书，并且都给出了5星的好评，那么就认为A和B属于同一类用户，可以将A看过的图书w也推荐给用户B。</p><p><img src="https://vinxikk.github.io/img/rs/user-based-cf.png" alt="基于用户的协同过滤"></p><h4 id="寻找偏好相似的用户"><a href="#寻找偏好相似的用户" class="headerlink" title="寻找偏好相似的用户"></a>寻找偏好相似的用户</h4><p>下面我们模拟5个用户对2件商品的评分，来说明如何通过用户对不同商品的态度和偏好寻找到相似的用户。</p><p><img src="https://vinxikk.github.io/img/rs/user-item-score.png" alt="用户对商品的评分"></p><p>在示例中，5个用户分别对两件商品进行了评分。这里的分值可以表现为真实的购买，也可以是用户对商品不同行为的量化指标。例如，浏览商品的次数，向朋友推荐商品，收藏，分享或评论等，这些行为都可以表示用户对商品的态度和偏好程度。</p><p>从表格中很难直观发现5个用户间的联系，我们将5个用户对2件商品的评分用散点图表示，用户间的关系就很容易发现了。</p><p><img src="https://vinxikk.github.io/img/rs/user-item-score-scatter.png" alt="用户间的关系"></p><p>在散点图中，X轴是商品1的评分，Y轴是商品2的评分。</p><p>通过用户的分布情况可以发现，A、C、D三个用户距离较近，即用户A(3.3, 6.5)和用户C(3.6, 6.3)、用户D(3.4, 5.8)对两件商品的评分较为接近，可以认为A、C、D三个用户属于同一类的用户。而用户B和E则形成了另一个群体。</p><p>散点图虽然直观，但无法投入实际的应用，也不能准确地度量用户间的关系。因此我们需要通过数字对用户间的关系进行准确的度量，并依据这些关系完成商品的推荐。</p><h4 id="相似性度量"><a href="#相似性度量" class="headerlink" title="相似性度量"></a>相似性度量</h4><h5 id="欧几里德距离评价"><a href="#欧几里德距离评价" class="headerlink" title="欧几里德距离评价"></a>欧几里德距离评价</h5><p>欧几里德距离评价是一个较为简单的用户关系评价方法，原理是通过计算两个用户在散点图中的距离来判断不同的用户是否有相同的偏好。</p><p><img src="https://vinxikk.github.io/img/rs/euclidean-distance.png" alt="欧氏距离计算公式"></p><p>我们获得了5个用户相互之间的欧几里德系数，也就是用户间的距离。系数越小表示两个用户间的距离越近，偏好也越是接近。不过这里有个问题，太小的数值可能无法准确地表现出不同用户间距离的差异，因此我们对求得的系数取倒数，使用户间的距离约接近，数值越大。</p><h5 id="皮尔逊相关度评价"><a href="#皮尔逊相关度评价" class="headerlink" title="皮尔逊相关度评价"></a>皮尔逊相关度评价</h5><p>皮尔逊相关度评价是另一种计算用户间关系的方法，他比欧几里德距离评价的计算要复杂一些，但对于评分数据不规范时，皮尔逊相关度评价能够给出更好的结果。</p><p>以下是多个用户对多件商品进行评分的示例，这个示例比之前的两件商品的情况要复杂一些，但也更接近真实的情况。</p><p>我们通过皮尔逊相关度评价对用户进行相似性度量和分组，并推荐商品。</p><p><img src="https://vinxikk.github.io/img/rs/user-item-score-2.png" alt="用户对多个商品的评分"></p><h5 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h5><p>皮尔逊相关系数的计算公式如下，结果是一个在-1与1之间的系数，该系数用来说明两个用户间联系的强弱程度。</p><p><img src="https://vinxikk.github.io/img/rs/pearson-correlation-coefficient.png" alt="皮尔逊相关系数计算公式"></p><p>相关系数的分类：</p><ul><li>0.8-1.0  极强相关</li><li>0.6-0.8  强相关</li><li>0.4-0.6  中等程度相关</li><li>0.2-0.4  弱相关</li><li>0.0-0.2  极弱相关或无相关</li></ul><p><img src="https://vinxikk.github.io/img/rs/user-pearson-correlation-coefficient.png" alt="用户间的相似度"></p><p>通过计算5个用户对5件商品的评分，我们获得了用户间的相似度数据，可以看到，用户A&amp;B，C&amp;D，C&amp;E和D&amp;E之间的相似度较高。下面，我们就可以按照相似度对用户进行商品推荐了。</p><h4 id="为相似的用户推荐物品"><a href="#为相似的用户推荐物品" class="headerlink" title="为相似的用户推荐物品"></a>为相似的用户推荐物品</h4><h5 id="为用户C推荐商品"><a href="#为用户C推荐商品" class="headerlink" title="为用户C推荐商品"></a>为用户C推荐商品</h5><p>当我们需要对用户C推荐商品时，首先检查之前的相似度列表，发现用户C和用户D、E的相似度较高。换句话说，这三个用户是同一类群体，拥有相同的偏好，因此，我们可以对用户C推荐D和E的商品。</p><p>但这里有一个问题，我们不能直接推荐前面商品1-5的商品，因为这些商品用户C已经浏览或者购买过了，不能重复推荐，因此我们要推荐用户C还没有浏览或购买过的商品。</p><h5 id="加权排序推荐"><a href="#加权排序推荐" class="headerlink" title="加权排序推荐"></a>加权排序推荐</h5><p>我们提取了用户D和用户E评价过的另外5件商品：商品A-F。并对不同商品的评分进行相似度加权，按加权后的结果对5件商品进行排序，然后推荐给用户C。这样，用户C就获得了与他偏好相似的用户D和E评价的商品。而在具体的推荐顺序和展示上我们按照用户D和E与用户C的相似度进行排序。</p><p><img src="https://vinxikk.github.io/img/rs/user-item-recommend.png" alt="为用户C推荐商品"></p><p>以上是基于用户的协同过滤算法，这个算法依靠用户的历史行为数据来计算相关度，也就是说，必须要有一定的数据积累（冷启动问题）。</p><p>对于新网站或数据量较少的网站，还有一种方法是基于物品的协同过滤算法。</p><h3 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h3><p>基于物品的协同过滤算法与基于用户的协同过滤算法很像，将商品和用户互换，通过计算不同用户对不同物品的评分获得物品间的关系，然后基于物品间的关系对用户进行相似物品的推荐。这里的评分代表用户对商品的态度和偏好。</p><p>简单来说就是，如果用户A同时购买了商品1和商品2，那么说明商品1和商品2的相关度较高。当用户B也购买了商品1时，可以推断他也有购买商品2的需求。</p><p><img src="https://vinxikk.github.io/img/rs/item-based-cf.png" alt="基于物品的协同过滤"></p><h4 id="寻找相似的物品"><a href="#寻找相似的物品" class="headerlink" title="寻找相似的物品"></a>寻找相似的物品</h4><p>表格中是两个用户对5件商品的评分。在这个表格中，用户和商品的位置进行了互换，通过两个用户的评分来获得5件商品之间的相似度情况。单从表格中我们依然很难发现其中的联系，因此我们选择通过散点图进行展示。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-score.png" alt="商品的评分"></p><p>单从表格中我们依然很难发现其中的联系，因此我们选择通过散点图进行展示。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-score-scatter.png" alt="物品间的关系"></p><p>在散点图中，X轴和Y轴分别是两个用户的评分，5件商品按照所获的评分值分布在散点图中。</p><p>我们可以发现，商品1, 3, 4在用户A和B中有着近似的评分，说明这三件商品的相关度较高，而商品5和2则在另一个群体中。</p><h4 id="相似性度量（皮尔逊相关度评价）"><a href="#相似性度量（皮尔逊相关度评价）" class="headerlink" title="相似性度量（皮尔逊相关度评价）"></a>相似性度量（皮尔逊相关度评价）</h4><p>我们选择使用皮尔逊相关度评价来计算多用户与多商品的关系，下面是5个用户对5件商品的评分表，我们通过这些评分计算出商品间的相关度。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-score-2.png" alt="商品的评分表"></p><p>商品间的皮尔逊相关度：</p><p><img src="https://vinxikk.github.io/img/rs/item-pearson-correlation-coefficient.png" alt="物品间的相似度"></p><p>通过计算可以发现，商品1&amp;2，商品3&amp;4，商品3&amp;5和商品4&amp;5相似度较高，下一步我们可以依据这些商品间的相关度对用户进行商品推荐。</p><h4 id="为用户提供基于相似物品的推荐"><a href="#为用户提供基于相似物品的推荐" class="headerlink" title="为用户提供基于相似物品的推荐"></a>为用户提供基于相似物品的推荐</h4><p>这里我们遇到了和基于用户进行商品推荐相同的问题，当需要对用户C基于商品3推荐商品时，需要一张新的商品与已有商品间的相似度列表。在前面的相似度计算中，商品3与商品4和5相似度较高，因此我们计算并获得了商品4, 5与其他商品的相似度列表。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-score-3.png" alt="其他商品评分表"></p><p><code>注意：这里的用户1, 2, 3原则上和上面的用户A-E不同</code></p><p>以下是通过计算获得的新商品与已有商品间的相似度数据。</p><p><img src="https://vinxikk.github.io/img/rs/new-item-pearson-correlation-coefficient.png" alt="新物品与已有物品间的相似度"></p><h5 id="加权排序推荐-1"><a href="#加权排序推荐-1" class="headerlink" title="加权排序推荐"></a>加权排序推荐</h5><p>这里是用户C已经购买过的商品4, 5与新商品A, B, C之间的相似程度。</p><p>我们将用户C对商品4, 5的评分作为权重，对商品A, B, C进行加权排序，用户C评分较高并且与之相似度较高的商品被优先推荐。</p><p><img src="https://vinxikk.github.io/img/rs/item-user-recommend.png" alt="为用户C推荐商品"></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 协同过滤 </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark窗口函数</title>
      <link href="/2018/08/10/spark/spark-window-function/"/>
      <url>/2018/08/10/spark/spark-window-function/</url>
      
        <content type="html"><![CDATA[<h3 id="窗口函数引入"><a href="#窗口函数引入" class="headerlink" title="窗口函数引入"></a>窗口函数引入</h3><hr><p>使用Spark SQL进行复杂的离线统计任务，有时需要计算一些排序特征、窗口特征等，显然不能简单地通过groupBy来完成，这时就需要了解spark中的窗口函数。</p><p>比如下面的统计需求：</p><ol><li>统计订单表，每个店铺每个订单和前一单的价格和。此时如果通过groupBy来完成特别费劲。</li><li>统计订单表，每个店铺每个订单与前一单的差值。此时需要自定义聚合函数。</li><li>还有计算前几单的平均值、计算环比之类的，都要用到窗口函数。</li></ol><h4 id="窗口函数的使用"><a href="#窗口函数的使用" class="headerlink" title="窗口函数的使用"></a>窗口函数的使用</h4><p>下面以订单表推演spark中窗口函数的使用。</p><p>订单表字段：订单id，店铺id，支付时间，支付金额。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowFuncApp</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    .appName(<span class="string">"WindowFuncApp"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    .master(<span class="string">"local"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    .getOrCreate()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> spark.implicits._</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> ordersDF: <span class="type">DataFrame</span> = <span class="type">Seq</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o1"</span>, <span class="string">"s1"</span>, <span class="string">"2017-05-01"</span>, <span class="number">100</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o2"</span>, <span class="string">"s1"</span>, <span class="string">"2017-05-02"</span>, <span class="number">200</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o3"</span>, <span class="string">"s2"</span>, <span class="string">"2017-05-01"</span>, <span class="number">200</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o4"</span>, <span class="string">"s1"</span>, <span class="string">"2017-05-03"</span>, <span class="number">200</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o5"</span>, <span class="string">"s2"</span>, <span class="string">"2017-05-02"</span>, <span class="number">100</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"o6"</span>, <span class="string">"s1"</span>, <span class="string">"2017-05-04"</span>, <span class="number">300</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  ).toDF(<span class="string">"order_id"</span>, <span class="string">"seller_id"</span>, <span class="string">"pay_time"</span>, <span class="string">"price"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">  ordersDF.printSchema()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//1.店铺订单顺序</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.functions._</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> rankSpec: <span class="type">WindowSpec</span> = <span class="type">Window</span>.partitionBy(<span class="string">"seller_id"</span>).orderBy(<span class="string">"pay_time"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"rank"</span>, dense_rank.over(rankSpec)).show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> rankSpec2: <span class="type">WindowSpec</span> = <span class="type">Window</span>.partitionBy(<span class="string">"seller_id"</span>).orderBy(<span class="string">"price"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"rank2"</span>, rank.over(rankSpec2)).show() <span class="comment">//1,2,2,4</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"dense_rank2"</span>, dense_rank.over(rankSpec2)).show() <span class="comment">//1,2,2,3</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">  println(<span class="string">"****************************************"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//定义前一单和本单的窗口</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> winSpec: <span class="type">WindowSpec</span> = <span class="type">Window</span>.partitionBy(<span class="string">"seller_id"</span>).orderBy(<span class="string">"pay_time"</span>).rowsBetween(<span class="number">-1</span>, <span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//本单及前一单的价格和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"sum_pay"</span>, sum(<span class="string">"price"</span>).over(winSpec)).show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//本单与前一单的平均值，用UDAF</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAvgUdaf</span></span>: <span class="type">UserDefinedAggregateFunction</span> = <span class="keyword">new</span> <span class="type">MyAverage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"avg"</span>, getAvgUdaf($<span class="string">"price"</span>).over(winSpec)).show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"avg2"</span>, avg(<span class="string">"price"</span>).over(winSpec)).show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">  println(<span class="string">"****************************************"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//每个店铺当前订单与前一单的差值，需要自定义聚合函数，或者lag函数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMinusUdaf</span></span>: <span class="type">UserDefinedAggregateFunction</span> = <span class="keyword">new</span> <span class="type">MyMinus</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">  ordersDF.withColumn(<span class="string">"rank"</span>, dense_rank.over(rankSpec))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">      .withColumn(<span class="string">"prePrice"</span>, lag(<span class="string">"price"</span>, <span class="number">1</span>).over(rankSpec)) <span class="comment">//前一行的值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">      .withColumn(<span class="string">"minus"</span>, getMinusUdaf($<span class="string">"price"</span>).over(winSpec)) <span class="comment">//在前面的基础上用UDF也行</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">      .show()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">/*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line"><span class="comment">  * lag(field, n): 就是取从当前字段往前第n个值，这里是取前一行的值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"><span class="comment">  * first/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line"><span class="comment">  * lag/lead(field, n): lead就是lag相反的操作，这个用于做数据回测特别有用，结果回推条件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line"><span class="comment">  * */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">  spark.stop()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line"><span class="comment">    * 自定义聚合函数UDAF</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line"><span class="comment">    */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//继承抽象函数必须实现以下方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//输入参数的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//缓冲区中进行聚合时，所处理的数据的类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"count"</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">"sum"</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//初始化给定的聚合缓冲区，即聚合缓冲区的零值（缓冲区内的数组和映射仍然是不可变的）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">      buffer(<span class="number">0</span>) = <span class="number">0</span>L <span class="comment">//表示次数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">      buffer(<span class="number">1</span>) = <span class="number">0.0</span>D <span class="comment">//表示总和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//使用来自input的新输入数据更新给定的聚合缓冲区buffer，每个输入行调用一次</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line">        buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + <span class="number">1</span>L <span class="comment">//次数加1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">        buffer(<span class="number">1</span>) = buffer.getDouble(<span class="number">1</span>) + input.getAs[<span class="type">Long</span>](<span class="number">0</span>).toDouble <span class="comment">//求和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//此函数是否始终在相同输入上返回相同的输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//合并两个聚合缓冲区并将更新的缓冲区值存储回buffer1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//当我们将两个部分聚合的数据合并在一起时调用此方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//Spark是分布式的，所以不同的区需要进行合并</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">      buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>) <span class="comment">//求次数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">      buffer1(<span class="number">1</span>) = buffer1.getDouble(<span class="number">1</span>) + buffer2.getDouble(<span class="number">1</span>) <span class="comment">//求和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//计算最终的结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line">      buffer.getDouble(<span class="number">1</span>) / buffer.getLong(<span class="number">0</span>).toDouble</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//返回值的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyMinus</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">104</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"value"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">105</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">106</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"minus"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">107</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">108</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">109</span></pre></td><td class="code"><pre><span class="line">      buffer(<span class="number">0</span>) = <span class="number">0</span>L <span class="comment">//表示差值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">110</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">111</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">112</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">113</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">114</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//输入的后者减去前者</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">115</span></pre></td><td class="code"><pre><span class="line">        buffer(<span class="number">0</span>) = input.getLong(<span class="number">0</span>) - buffer.getLong(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">116</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">117</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">118</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">119</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//此函数是否始终在相同输入上返回相同的输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">120</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">121</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">122</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">123</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//分区合并，也是后者减前者</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">124</span></pre></td><td class="code"><pre><span class="line">      buffer1(<span class="number">0</span>) = buffer2.getLong(<span class="number">0</span>) - buffer1.getLong(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">125</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">126</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">127</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//计算最终的结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">128</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">129</span></pre></td><td class="code"><pre><span class="line">      buffer.getLong(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">130</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">131</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">132</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//返回值的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">133</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">LongType</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">134</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">135</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark窗口函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD的血缘关系：窄依赖和宽依赖&amp;Spark任务中的Stage</title>
      <link href="/2018/05/22/spark/spark-dependency/"/>
      <url>/2018/05/22/spark/spark-dependency/</url>
      
        <content type="html"><![CDATA[<h3 id="RDD的依赖关系"><a href="#RDD的依赖关系" class="headerlink" title="RDD的依赖关系"></a>RDD的依赖关系</h3><p>RDD和它依赖的父RDD(s)的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p><p><img src="https://vinxikk.github.io/img/spark/rdd-narrow-wide-dependency.png" alt="RDD的窄依赖和宽依赖"></p><p>窄依赖：每个parent RDD的partition最多被child RDD的一个partition使用。</p><p>宽依赖：每个parent RDD的partition被多个child RDD的partition使用。</p><p>窄依赖每个child RDD的partition的生成操作都是可以并行的，而宽依赖则需要所有的parent RDD partition shuffle结果得到后再进行。</p><h3 id="org-apache-spark-Dependency-scala源码"><a href="#org-apache-spark-Dependency-scala源码" class="headerlink" title="org.apache.spark.Dependency.scala源码"></a>org.apache.spark.Dependency.scala源码</h3><p><code>Dependency</code>是一个抽象类：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dependency.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Dependency</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>它有两个子类：<code>NarrowDependency</code> 和 <code>ShuffleDenpendency</code>，分别对应窄依赖和宽依赖。</p><h4 id="NarrowDependency抽象类"><a href="#NarrowDependency抽象类" class="headerlink" title="NarrowDependency抽象类"></a>NarrowDependency抽象类</h4><p>定义了抽象方法<code>getParents</code>，输入<code>partitionId</code>，用于获得child RDD 的某个partition依赖的parent RDD的所有 partitions。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dependency.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">_rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">T</span>] </span>&#123;  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * Get the parent partitions for a child partition.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * @param partitionId a partition of the child RDD</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * @return the partitions of the parent RDD that the child partition depends upon</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">Seq</span>[<span class="type">Int</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>] = _rdd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>窄依赖有两种具体的实现：<code>OneToOneDependency</code>和<code>RangeDependency</code>。</p><p><code>OneToOneDependency</code>指child RDD的partition只依赖于parent RDD 的一个partition，产生<code>OneToOneDependency</code>的算子有map，filter，flatMap等。</p><p>可以看到<code>getParents</code>实现很简单，就是传进去一个<code>partitionId</code>，再把<code>partitionId</code>放在List里面传出去。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(partitionId)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p><code>RangeDependency</code>指child RDD partition在一定的范围内一对一的依赖于parent RDD partition，主要用于union。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//inStart表示parent RDD的开始索引，outStart表示child RDD 的开始索引</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RangeDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>], inStart: <span class="type">Int</span>, outStart: <span class="type">Int</span>, length: <span class="type">Int</span></span>)  </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = &#123;    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="type">List</span>(partitionId - outStart + inStart) <span class="comment">//表示于当前索引的相对位置</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="type">Nil</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h4 id="ShuffleDependency宽依赖"><a href="#ShuffleDependency宽依赖" class="headerlink" title="ShuffleDependency宽依赖"></a>ShuffleDependency宽依赖</h4><p>表示一个parent RDD的partition会被child RDD的partition使用多次，需要经过shuffle才能形成。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],    </span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val partitioner: <span class="type">Partitioner</span>,    </span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="params">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] </span>&#123;  <span class="comment">//shuffle都是基于PairRDD进行的，所以传入的RDD要是key-value类型的</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] = _rdd.asInstanceOf[<span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> keyClassName: <span class="type">String</span> = reflect.classTag[<span class="type">K</span>].runtimeClass.getName</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> valueClassName: <span class="type">String</span> = reflect.classTag[<span class="type">V</span>].runtimeClass.getName</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> combinerClassName: <span class="type">Option</span>[<span class="type">String</span>] =</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="type">Option</span>(reflect.classTag[<span class="type">C</span>]).map(_.runtimeClass.getName)  <span class="comment">//获取shuffleId</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> shuffleId: <span class="type">Int</span> = _rdd.context.newShuffleId()  <span class="comment">//向shuffleManager注册shuffle信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    shuffleId, _rdd.partitions.length, <span class="keyword">this</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(<span class="keyword">this</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>由于shuffle涉及到网络传输，所以要有序列化serializer，为了减少网络传输，可以map端聚合，通过mapSideCombine和aggregator控制，还有key排序相关的keyOrdering，以及重输出的数据如何分区的partitioner，还有一些class信息。Partition之间的关系在shuffle处戛然而止，因此shuffle是划分stage的依据。</p><h4 id="两种依赖的区分"><a href="#两种依赖的区分" class="headerlink" title="两种依赖的区分"></a>两种依赖的区分</h4><p>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。例如，逐个元素地执行map、然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点之间进行Shuffle，这与MapReduce类似。</p><p>第二，窄依赖能够更有效地进行失效节点的恢复，即只需重新计算丢失RDD分区的父分区，而且不同节点之间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。</p><h3 id="Spark任务中的Stage"><a href="#Spark任务中的Stage" class="headerlink" title="Spark任务中的Stage"></a>Spark任务中的Stage</h3><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。</p><p><img src="https://vinxikk.github.io/img/spark/rdd-stage.png" alt="RDD的Stage"></p><p>RDD存在着依赖关系，这些依赖关系形成了有向无环图DAG，DAG通过DAGScheduler进行Stage的划分，并基于每个Stage生成了TaskSet，提交给TaskScheduler。</p><h4 id="作业的提交"><a href="#作业的提交" class="headerlink" title="作业的提交"></a>作业的提交</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// SparkContext.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">progressBar.foreach(_.finishAll())</span></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span></pre></td></tr></table></figure><p>可以看到，SparkContext的<code>runjob</code>方法调用了DAGScheduler的<code>runjob</code>方法正式向集群提交任务，最终调用了<code>submitJob</code>方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    callSite: <span class="type">CallSite</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach &#123; p =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">            <span class="string">"Attempting to access a non-existent partition: "</span> + p + <span class="string">". "</span> +</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            <span class="string">"Total number of partitions: "</span> + maxPartitions)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">// Return immediately if the job is running 0 tasks</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">    assert(partitions.size &gt; <span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//给eventProcessLoop发送JobSubmitted消息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        jobId, rdd, func2, partitions.toArray, callSite, waiter,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        <span class="type">SerializationUtils</span>.clone(properties)))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    waiter</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>这里向<code>eventProcessLoop</code>对象发送了<code>JobSubmitted</code>消息。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[scheduler] <span class="keyword">val</span> eventProcessLoop = <span class="keyword">new</span> <span class="type">DAGSchedulerEventProcessLoop</span>(<span class="keyword">this</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  eventProcessLoop是<span class="type">DAGSchedulerEventProcessLoop</span>类的一个对象。</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">StageCancelled</span>(stageId) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleStageCancellation(stageId)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">JobCancelled</span>(jobId) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleJobCancellation(jobId)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">JobGroupCancelled</span>(groupId) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleJobGroupCancelled(groupId)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">AllJobsCancelled</span> =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.doCancelAllJobs()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorAdded</span>(execId, host) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleExecutorAdded(execId, host)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorLost</span>(execId, reason) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> filesLost = reason <span class="keyword">match</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">case</span> <span class="type">SlaveLost</span>(_, <span class="literal">true</span>) =&gt; <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="literal">false</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleExecutorLost(execId, filesLost)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">BeginEvent</span>(task, taskInfo) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleBeginEvent(task, taskInfo)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">GettingResultEvent</span>(taskInfo) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleGetTaskResult(taskInfo)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> completion: <span class="type">CompletionEvent</span> =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleTaskCompletion(completion)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">TaskSetFailed</span>(taskSet, reason, exception) =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">case</span> <span class="type">ResubmitFailedStages</span> =&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">      dagScheduler.resubmitFailedStages()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure><p><code>DAGSchedulerEventProcessLoop</code>对接收到的消息进行处理，在<code>doOnReceive</code>方法中形成一个event loop。<br>接下来将调用<code>submitStage()</code>方法进行stage的划分。</p><h4 id="stage的划分"><a href="#stage的划分" class="headerlink" title="stage的划分"></a>stage的划分</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> jobId = activeJobForStage(stage)<span class="comment">//查找该Stage的所有激活的job</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (jobId.isDefined) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      logDebug(<span class="string">"submitStage("</span> + stage + <span class="string">")"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)<span class="comment">//得到Stage的父Stage，并排序</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        logDebug(<span class="string">"missing: "</span> + missing)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (missing.isEmpty) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">          logInfo(<span class="string">"Submitting "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">"), which has no missing parents"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">          submitMissingTasks(stage, jobId.get)<span class="comment">//如果Stage没有父Stage，则提交任务集</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">for</span> (parent &lt;- missing) &#123;<span class="comment">//如果有父Stage，递归调用submiStage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            submitStage(parent)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">          waitingStages += stage<span class="comment">//将其标记为等待状态，等待下次提交</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">      abortStage(stage, <span class="string">"No active job for stage "</span> + stage.id, <span class="type">None</span>)<span class="comment">//如果该Stage没有激活的job，则丢弃该Stage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure><p>在<code>submitStage</code>方法中判断Stage的父Stage有没有被提交，直到所有父Stage都被提交，只有等父Stage完成后才能调度子Stage。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>] <span class="comment">//用于存放父Stage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]] <span class="comment">//用于存放已访问过的RDD</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (!visited(rdd)) &#123; <span class="comment">//如果RDD没有被访问过，则进行访问</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        visited += rdd <span class="comment">//添加到已访问RDD的HashSet中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123; <span class="comment">//获取该RDD的依赖</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">            dep <span class="keyword">match</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">              <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;<span class="comment">//若为宽依赖，则该RDD依赖的RDD所在的stage为父stage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)<span class="comment">//生成父Stage</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">if</span> (!mapStage.isAvailable) &#123;<span class="comment">//若父Stage不存在，则添加到父Stage的HashSET中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">                  missing += mapStage</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">                &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">              <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt;<span class="comment">//若为窄依赖，则继续访问父RDD</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">                waitingForVisit.push(narrowDep.rdd)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">    waitingForVisit.push(stage.rdd)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;<span class="comment">//循环遍历所有RDD</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      visit(waitingForVisit.pop())</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    missing.toList</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure><p><code>getmissingParentStages()</code>方法为核心方法。</p><p>Stage是通过shuffle划分的，所以每一个Stage都是以shuffle开始的，若一个RDD是宽依赖，则必然说明该RDD的父RDD在另一个Stage中，若一个RDD是窄依赖，则该RDD所依赖的父RDD还在同一个Stage中，我们可以根据这个逻辑，找到该Stage的父Stage。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark架构和部署&amp;WordCount原理</title>
      <link href="/2018/05/20/spark/spark-basic/"/>
      <url>/2018/05/20/spark/spark-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h3><p>Spark官网：</p><p><a href="http://spark.apache.org/" target="_blank" rel="noopener">http://spark.apache.org/</a></p><p><img src="https://vinxikk.github.io/img/spark/what-is-spark.png" alt="Spark是什么"></p><p>Spark是一个针对大规模数据处理的快速通用引擎。</p><p>Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。目前，Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLlib等子项目，Spark是基于内存计算的大数据并行计算框架。Spark基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。</p><h3 id="Spark的架构"><a href="#Spark的架构" class="headerlink" title="Spark的架构"></a>Spark的架构</h3><p><img src="https://vinxikk.github.io/img/spark/spark-cluster-architecture.png" alt="Spark集群架构"></p><p><img src="https://vinxikk.github.io/img/spark/spark-master-worker.png" alt="Spark主从结构"></p><h3 id="Spark部署"><a href="#Spark部署" class="headerlink" title="Spark部署"></a>Spark部署</h3><p>Spark的部署有以下几种模式：</p><ul><li>Standalone</li><li>YARN</li><li>Mesos</li><li>Amason EC2</li></ul><h4 id="Spark-Standalone伪分布式部署"><a href="#Spark-Standalone伪分布式部署" class="headerlink" title="Spark Standalone伪分布式部署"></a>Spark Standalone伪分布式部署</h4><p>配置文件con/spark-env.sh：</p><p><code>export JAVA_HOME=/usr/java/jdk1.8.0_121</code></p><p><code>export SPARK_MASTER_HOST=rshost001</code></p><p><code>export SPARK_MASTER_PORT=7077</code></p><p>下面的可以不写，默认<br><code>export SPARK_WORKER_CORES=1</code><br><code>export SPARK_WORKER_MEMORY=1024m</code></p><p>配置文件conf/slave：</p><p><code>rshost001</code></p><h4 id="Spark-Standalone全分布式部署"><a href="#Spark-Standalone全分布式部署" class="headerlink" title="Spark Standalone全分布式部署"></a>Spark Standalone全分布式部署</h4><p>配置文件con/spark-env.sh：</p><p><code>export JAVA_HOME=/usr/java/jdk1.8.0_121</code></p><p><code>export SPARK_MASTER_HOST=rshost001</code></p><p><code>export SPARK_MASTER_PORT=7077</code></p><p>下面的可以不写，默认<br><code>export SPARK_WORKER_CORES=1</code><br><code>export SPARK_WORKER_MEMORY=1024m</code></p><p>配置文件conf/slave：</p><p><code>rshost002</code></p><p><code>rshost001</code></p><p>启动Spark集群：<code>start-all.sh</code></p><p>Web UI界面：<a href="http://rshost001:8080/" target="_blank" rel="noopener">http://rshost001:8080/</a></p><h3 id="Spark-Demo"><a href="#Spark-Demo" class="headerlink" title="Spark Demo"></a>Spark Demo</h3><h4 id="Spark-Example"><a href="#Spark-Example" class="headerlink" title="Spark Example"></a>Spark Example</h4><p>示例jar包路径：<code>$SPARK_HOME/examples/jars/spark-examples_2.11-2.1.0.jar</code></p><p>示例程序源码：<code>$SPARK_HOME/examples/src/main</code></p><p>Demo蒙特卡洛求PI：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--master spark://rshost001:7077 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--class org.apache.spark.examples.SparkPi \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">/home/vinx/app/spark-2.1.1-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.1.1.jar \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">100</span></pre></td></tr></table></figure><h4 id="使用Spark-Shell"><a href="#使用Spark-Shell" class="headerlink" title="使用Spark Shell"></a>使用Spark Shell</h4><p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</p><p>启动spark shell：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">./bin/spark-shell \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--master spark://rshost001:7077 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--executor-memory 2g \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--total-executor-cores 1</span></pre></td></tr></table></figure><p>参数说明：<br><code>--master spark://rshost001:7077</code> 指定Master的地址<br><code>--executor-memory 2g</code> 指定每个worker可用内存为2G<br><code>--total-executor-cores 1</code> 指定整个集群使用的cup核数为1个</p><p>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。</p><p>在Spark Shell中编写WordCount程序：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"/home/vinx/data/words.txt"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">.flatMap(_.split(<span class="string">" "</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">.map((_,<span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">.reduceByKey(_+_)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">.saveAsTextFile(<span class="string">"/home/vinx/data/wordcount"</span>)</span></pre></td></tr></table></figure><p>说明：<br>sc是SparkContext对象，该对象是提交spark程序的入口<br><code>textFile(&quot;/home/vinx/data/words.txt&quot;)</code>读取本地数据<br><code>flatMap(_.split(&quot; &quot;))</code> 先map再压平<br><code>map((_,1))</code> 将单词和1构成元组<br><code>reduceByKey(_+_)</code> 按照key进行reduce，并将value累加<br><code>saveAsTextFile(&quot;/home/vinx/data/wordcount&quot;)</code>将结果写入到本地目录</p><h3 id="Spark运行机制"><a href="#Spark运行机制" class="headerlink" title="Spark运行机制"></a>Spark运行机制</h3><h4 id="WordCount执行的流程"><a href="#WordCount执行的流程" class="headerlink" title="WordCount执行的流程"></a>WordCount执行的流程</h4><p><img src="https://vinxikk.github.io/img/spark/spark-wordcount.png" alt="WordCount执行过程"></p><h4 id="Spark提交任务的流程"><a href="#Spark提交任务的流程" class="headerlink" title="Spark提交任务的流程"></a>Spark提交任务的流程</h4><p><img src="https://vinxikk.github.io/img/spark/spark-job.png" alt="Spark任务提交过程"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD的常用算子&amp;persist/cache缓存机制&amp;checkpoint容错机制</title>
      <link href="/2018/05/19/spark/spark-core/"/>
      <url>/2018/05/19/spark/spark-core/</url>
      
        <content type="html"><![CDATA[<h3 id="RDD的基本概念"><a href="#RDD的基本概念" class="headerlink" title="RDD的基本概念"></a>RDD的基本概念</h3><h4 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h4><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</p><p>RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。</p><p>RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><h4 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h4><p><img src="https://vinxikk.github.io/img/spark/rdd-properties.png" alt="RDD的属性"></p><ul><li><p>一组分片（Partition）</p><p>即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p></li><li><p>一个计算每个分区的函数</p><p>Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p></li><li><p>RDD之间的依赖关系</p><p>RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p></li><li><p>一个Partitioner，即RDD的分片函数</p><p>当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于key-value的RDD，才会有Partitioner，非key-value的RDD的Partitioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p></li><li><p>一个列表</p><p>存储每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p></li></ul><h4 id="RDD的创建方式"><a href="#RDD的创建方式" class="headerlink" title="RDD的创建方式"></a>RDD的创建方式</h4><ul><li><p>通过外部的数据文件创建，如HDFS</p><p><code>val rdd1 = sc.textFile(“hdfs://192.168.xxx.xxx:9000/data/data.txt”)</code></p></li><li><p>通过<code>sc.parallelize</code>进行创建</p><p><code>val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))</code></p></li><li><p>RDD的类型</p><p>Transformation和Action</p></li></ul><h4 id="RDD的基本原理"><a href="#RDD的基本原理" class="headerlink" title="RDD的基本原理"></a>RDD的基本原理</h4><p><img src="https://vinxikk.github.io/img/spark/rdd-parallelize.png" alt="RDD的并行原理"></p><h3 id="RDD的算子"><a href="#RDD的算子" class="headerlink" title="RDD的算子"></a>RDD的算子</h3><h4 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h4><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。</p><p>相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。</p><p>只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。</p><p>这种设计让Spark更加有效率地运行。</p><ul><li><p><code>map(func)</code></p><p>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</p></li><li><p><code>filter(func)</code></p><p>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成</p></li><li><p><code>flatMap(func)</code></p><p>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</p></li><li><p><code>mapPartitions(func)</code></p><p>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]</p></li><li><p><code>mapPartitionsWithIndex(func)</code></p><p>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]</p></li><li><p><code>sample((withReplacement, fraction, seed)</code></p><p>根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子</p></li><li><p><code>union(otherDataset)</code></p><p>对源RDD和参数RDD求并集后返回一个新的RDD</p></li><li><p><code>intersection(otherDataset)</code></p><p>对源RDD和参数RDD求交集后返回一个新的RDD</p></li><li><p><code>distinct([numTasks])</code></p><p>对源RDD进行去重后返回一个新的RDD</p></li><li><p><code>groupByKey([numTasks])</code></p><p>在一个(K,V)的RDD上调用，返回一个(K,Iterator[V])的RDD</p></li><li><p><code>reduceByKey(func,[numTasks])</code></p><p>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与<code>groupByKey</code>类似，reduce任务的个数可以通过第二个可选的参数来设置</p></li><li><p><code>aggregateByKey(zeroValue)(seqOp,combOp,[numTasks])</code></p></li><li><p><code>sortByKey([ascending], [numTasks])</code></p><p>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</p></li><li><p><code>sortBy(func,[ascending], [numTasks])</code></p><p>与sortByKey类似，但是更灵活</p></li><li><p><code>join(otherDataset, [numTasks])</code></p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</p></li><li><p><code>cogroup(otherDataset, [numTasks])</code></p><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个<code>(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))</code>类型的RDD</p></li><li><p><code>cartesian(otherDataset)</code></p><p>笛卡尔积</p></li><li><p><code>pipe(command, [envVars])</code></p></li><li><p><code>coalesce(numPartitions)</code></p></li><li><p><code>repartition(numPartitions)</code></p></li><li><p><code>repartitionAndSortWithinPartitions(partitioner)</code></p></li></ul><h4 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h4><ul><li><p><code>reduce(func)</code></p><p>通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的</p></li><li><p><code>collect()</code></p><p>在驱动程序中，以数组的形式返回数据集的所有元素</p></li><li><p><code>count()</code></p><p>返回RDD的元素个数</p></li><li><p><code>first()</code></p><p>返回RDD的第一个元素（类似于take(1)）</p></li><li><p><code>take(n)</code></p><p>返回一个由数据集的前n个元素组成的数组</p></li><li><p><code>takeSample(withReplacement,num, [seed])</code></p><p>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</p></li><li><p><code>takeOrdered(n, [ordering])</code></p></li><li><p><code>saveAsTextFile(path)</code></p><p>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark会调用toString方法，将它转换为文件中的文本</p></li><li><p><code>saveAsSequenceFile(path)</code></p><p>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以是HDFS或其他Hadoop支持的文件系统</p></li><li><p><code>saveAsObjectFile(path)</code></p></li><li><p><code>countByKey()</code></p><p>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数</p></li><li><p><code>foreach(func)</code></p><p>在数据集的每一个元素上，运行函数func进行更新</p></li></ul><h4 id="常用算子实例"><a href="#常用算子实例" class="headerlink" title="常用算子实例"></a>常用算子实例</h4><p>Transformation算子：</p><ul><li><p><code>map(func)</code>:  将输入的每个元素重写组合成一个元组</p><p>通过sc.parallelize创建RDD：</p><p><code>val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))</code></p><p>返回一个(K,*)格式的元组：</p><p><code>val rdd2 = rdd1.map((_,&quot;*&quot;))</code></p><p>每个元素乘以10：</p><p><code>val rdd2 = rdd1.map(_ * 10)</code></p><p>每个元素加上10：</p><p><code>val rdd2 = rdd1.map((x:Int) =&gt; x + 10)</code></p></li><li><p><code>filter(func)</code>:  返回一个新的RDD，该RDD是经过func运算后返回true的元素</p><p><code>val rdd3 = rdd1.filter(_ &gt; 5)</code></p></li><li><p><code>flatMap(func)</code>:  压平操作</p><p><code>val books = sc.parallelize(List(&quot;Hadoop&quot;,&quot;Hive&quot;,&quot;HDFS&quot;))</code></p><p><code>books.flatMap(_.toList).collect</code></p><p>结果：</p><p><code>res12: Array[Char] = Array(H, a, d, o, o, p, H, i, v, e, H, D, F, S)</code></p></li><li><p><code>union(otherDataset)</code>:  并集运算，类型要一致</p><p><code>val rdd4 = sc.parallelize(List(4,5,6,4,7))</code></p><p><code>val rdd5 = sc.parallelize(List(1,2,3,4))</code></p><p><code>val rdd6 = rdd4.union(rdd5)</code></p></li><li><p><code>intersection(otherDataset)</code>:  交集</p><p><code>val rdd7 = rdd5.intersection(rdd4)</code></p></li><li><p><code>distinct([numTasks])</code>: 去掉重复数据</p><p><code>val rdd8 = sc.parallelize(List(5,6,7,5,5,5))</code></p><p><code>rdd8.distinct.collect</code></p><p>结果：</p><p><code>res15: Array[Int] = Array(6, 7, 5)</code></p></li><li><p><code>groupByKey([numTasks])</code>:  对于一个&lt;k,v&gt;的RDD，按照key进行分组</p><p><code>val rdd = sc.parallelize(Array((&quot;I&quot;,1),(&quot;love&quot;,2),(&quot;I&quot;,3)))</code></p><p><code>rdd.groupByKey.collect</code></p><p>结果：</p><p><code>res16: Array[(String, Iterable[Int])] = Array((love,CompactBuffer(2)), (I,CompactBuffer(1, 3)))</code></p></li><li><p><code>flatMap</code> + <code>groupByKey</code></p><p><code>val strings = sc.parallelize(List(&quot;I love Beijing&quot;,&quot;I love China&quot;,&quot;Beijing is the capital of China&quot;))</code></p><p><code>strings.flatMap(_.split(&quot; &quot;)).map((_,1)).groupByKey.collect</code></p><p>结果：</p><p><code>res17: Array[(String, Iterable[Int])] = Array((is,CompactBuffer(1)), (love,CompactBuffer(1, 1)), (capital,CompactBuffer(1)), (Beijing,CompactBuffer(1, 1)), (China,CompactBuffer(1, 1)), (I,CompactBuffer(1, 1)), (of,CompactBuffer(1)), (the,CompactBuffer(1)))</code></p></li><li><p>reduceByKey(func, [numTasks]):  类似于groupByKey，区别是reduceByKey会有一个combiner的过程，对每个分区上的数据先做一次合并，所以效率更高</p></li><li><p><code>cartesian</code>:  笛卡尔积</p><p><code>val rdd1 = sc.parallelize(List(&quot;tom&quot;, &quot;jerry&quot;))</code></p><p><code>val rdd2 = sc.parallelize(List(&quot;tom&quot;, &quot;kitty&quot;, &quot;shuke&quot;))</code></p><p><code>val rdd3 = rdd1.cartesian(rdd2)</code></p></li></ul><p>Action算子：</p><p><code>val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)</code></p><ul><li><p><code>collect</code></p><p><code>rdd1.collect</code></p></li><li><p><code>reduce</code></p><p><code>val rdd2 = rdd1.reduce(_+_)</code></p></li><li><p><code>count</code></p><p><code>rdd1.count</code></p></li><li><p><code>top</code></p><p><code>rdd1.top(2)</code></p></li><li><p><code>take</code></p><p><code>rdd1.take(2)</code></p></li><li><p><code>first:  similar to take(1)</code></p><p><code>rdd1.first</code></p></li><li><p><code>takeOrdered</code></p><p><code>rdd1.takeOrdered(3)</code></p></li></ul><h3 id="RDD的缓存机制"><a href="#RDD的缓存机制" class="headerlink" title="RDD的缓存机制"></a>RDD的缓存机制</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><p><img src="https://vinxikk.github.io/img/spark/rdd-persist-cache.png" alt="RDD的persist和cache"></p><p>查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在<code>object StorageLevel</code>中定义。</p><p><img src="https://vinxikk.github.io/img/spark/storage-level.png" alt="存储级别"></p><p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。</p><p>通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><p>示例说明：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-cache-demo.png" alt="cache实例应用"></p><p>通过Web UI进行监控：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-cache-demo-ui.png" alt="cache Job的UI界面"></p><h3 id="RDD的Checkpoint容错机制"><a href="#RDD的Checkpoint容错机制" class="headerlink" title="RDD的Checkpoint容错机制"></a>RDD的Checkpoint容错机制</h3><p>检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage（血统）做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。</p><p>设置checkpoint的目录，可以是本地的文件夹、也可以是HDFS。一般是在具有容错能力，高可靠的文件系统上(比如HDFS, S3等)设置一个检查点路径，用于保存检查点数据。</p><p>本地目录（这种模式，需要将spark-shell运行在本地模式上）：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-checkpoint-local.png" alt="基于本地目录的checkpoint"></p><p>HDFS的目录（这种模式，需要将spark-shell运行在集群模式上）：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-checkpoint-hdfs.png" alt="基于HDFS的checkpoint"></p><p>checkpoint源码：</p><p><img src="https://vinxikk.github.io/img/spark/rdd-checkpoint-source.png" alt="checkpoint源码"></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive SQL的底层执行原理</title>
      <link href="/2018/05/17/hive/hive-ql-principle/"/>
      <url>/2018/05/17/hive/hive-ql-principle/</url>
      
        <content type="html"><![CDATA[<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><p><code>select * from tb_name where col_1 = &#39;2018&#39; and col_2 = &#39;hive&#39;;</code></p><p>执行select语句时，只有map阶段，没有shuffle和reduce。</p><p>map：根据split个数开启几个map，每个map task会接收到一个split文件，每个map函数会逐行对输入的文件进行检测，筛选出col_1为2018、col_2为hive的数据，保存到本地。</p><p>map的个数可以这样理解：如果文件为256M（hadoop2.x每个块128M），会开启2个map；289M就会开启3个map。</p><p>这些map都是并行执行的，当数据量特别大时开启的map会越多，理论上还会并行执行。</p><p>select数据时一定要加上where条件，避免盲目查询。</p><h3 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h3><p><code>select col_1,count(*) from tb_name where col_2 = &quot;hive&quot; group by col_1;</code></p><p>map：分完片的文件会产生相应数量的map，每个map会逐行检测col_2是否为hive，如果是hive它会生成键值对<code>&lt;col_1,1&gt;</code>。</p><p>combine：该操作发生在对应的文件中，即map分了几个，就有几个combine，它会把map端产生的键值对相同的key对进行累加，如：<code>&lt;col_1,3&gt;</code>。</p><p>shuffle：该操作分为partition, sort, spill, copy, merge。最重要的是分区和合并的过程。</p><p>map生成的task会通过对每个键取hash值，使map task按照相同的键均匀分配到reduce上，这个过程就是分区，分配到同一个reduce上的task会经过合并过程，生成键值对<code>&lt;col_1,{3,1}&gt;</code>，做完reduce task输入。</p><p>reduce：调用函数累加，3+1=4</p><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p><code>select t1.col_1,t2.col_2 from (select col_1,col_3 from tb_1) t1 join (select col_2,col_3 from tb_2) t2 on t1.col_3 = t2.col_3;</code></p><p>会开启3个MR任务。</p><p>第一个执行<code>select col_1,col_3 from tb_1</code>的操作，第二个执行<code>select col_2,col_3 from tb_2</code>的操作，第三个会将第一个和第二个的结果进行关联合并，然后输出。</p><p>split：join的时候会把第一个MR和第二个MR任务的输出文件输入该次任务，首先会对前两个任务进行分片，hadoop大于128M分一个片。</p><p>map：hadoop集群根据split出来的结果开启相应个数的map task。</p><p>shuffle：join时主要是分区操作，join的列进行数据的重分布和分发过程，分区的类为col_3，于是所有的map task都根据col_3进行发布。相同join就会发生到同一个reducer上。</p><p>reduce：shuffle发送的col_1, col_3; col_2, col_3会根据col_3发送到reduce task上。这个时候根据col_3将他们的值变成一行，保留到本地输出文件中。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的压缩格式</title>
      <link href="/2018/05/16/hive/hive-compress/"/>
      <url>/2018/05/16/hive/hive-compress/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive的压缩格式"><a href="#Hive的压缩格式" class="headerlink" title="Hive的压缩格式"></a>Hive的压缩格式</h3><p>test</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-Compress </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的数据倾斜及性能调优</title>
      <link href="/2018/05/15/hive/hive-skewindata/"/>
      <url>/2018/05/15/hive/hive-skewindata/</url>
      
        <content type="html"><![CDATA[<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><p>数据倾斜是指，MR程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长。这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。</p><p>表现：</p><p>任务进度长时间维持在99%（或者100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。</p><h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><table><thead><tr><th>类型</th><th>keyword</th><th>情形</th><th>后果</th><th>解决方法</th></tr></thead><tbody><tr><td>数据倾斜</td><td>join</td><td>其中一个表是小表（1000条以下记录或1G容量以下）</td><td>分发到某几个reduce上的数据远高于平均值</td><td>SMB Join,  Map Join, SMB Map Join</td></tr><tr><td>数据倾斜</td><td></td><td>大表与大表join，但是关联值0或null过多</td><td>空值会由一个reduce处理，非常慢</td><td>SMB Join, 空值特殊处理, 调参(skewjoin)</td></tr><tr><td>数据倾斜</td><td></td><td>小表不小不大</td><td>-</td><td>嵌套Map Join</td></tr><tr><td>数据倾斜</td><td></td><td>不同数据类型关联</td><td>默认的Hash操作会按int型的id来进行分配，所有string类型id的记录都分配到一个reducer中</td><td>类型转换</td></tr><tr><td>数据倾斜</td><td>group by</td><td>某些维度值数据过多</td><td>处理某值的reduce耗时</td><td>调参(skewindata)</td></tr><tr><td>数据倾斜</td><td>count distinct</td><td>某些值过多</td><td>处理某值的reduce耗时</td><td>count distinct优化</td></tr><tr><td>数据量大</td><td>on … where …</td><td>过滤在where条件</td><td>大数据量join</td><td>where优化</td></tr><tr><td>Job数多</td><td>union all</td><td>union属于嵌套查询</td><td>生成过多的job</td><td>union all优化</td></tr></tbody></table><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h4><p>对于group by引起的倾斜，设置下面的参数：</p><p><code>set hive.map.aggr = true</code></p><p><code>set hive.groupby.skewindata=true</code></p><p>有数据倾斜的时候进行负载均衡，当选项设定为true，生成的查询计划会有两个MR Job。</p><p>第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的。</p><p>第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><p>skew join优化：</p><p><code>set hive.optimize.skewjoin = true;</code></p><p>skew join，其原理是把这种user_id=0的特殊值先不在Reduce端计算掉，而是先写入hdfs，然后启动一轮Map join专门做这个特殊值的计算，期望能提高计算这部分值的处理速度。还有要告诉Hive如何判断特殊值，根据<code>hive.skewjoin.key</code>设置的数量Hive可以知道，比如默认值是100000，那么超过100000条记录的值就是特殊值。</p><h4 id="Sort-Merge-Bucket-Join-SMB-Join"><a href="#Sort-Merge-Bucket-Join-SMB-Join" class="headerlink" title="Sort Merge Bucket Join (SMB Join)"></a>Sort Merge Bucket Join (SMB Join)</h4><p>Hive桶：</p><p>对于每一个表（table）或者分区，Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p><p>SMB Join:</p><ol><li>解决大表与小表间的join问题。小表的number_buckets必须是大表的倍数。</li><li>解决大表与大表间的join问题。这一优化方法并不一定要求两个表必须桶的个数相同，两个表的桶个数是倍数关系也可以。</li></ol><p><img src="https://vinxikk.github.io/img/hive/hive-smb-join.png" alt="SMB Join"></p><p>例子：</p><p><img src="https://vinxikk.github.io/img/hive/hive-smb-join-example.png" alt="SMB Join例子"></p><h4 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h4><p>对于大小表关联查询产生的数据倾斜，一般的做法是使用Map Join将其中做连接的小表（全量数据）分发到所有map task端进行join，从而避免reduce task，前提要求是内存足以装下该全量数据。</p><p>以大表a和小表b为例，所有的map task节点都装载小表b的所有数据，然后大表a的一个数据块数据比如a1去跟b全量数据做连接，就省去了reduce做汇总的过程。</p><p>所以相对来说，在内存允许的条件下使用map join比直接使用MapReduce效率还高些，当然这只限于做join查询的时候。</p><p>所有的工作都在Map端进行计算。首先小表的Map阶段它会将自己转化成MapReduce Local Task，然后从HDFS取小表的所有数据，将自己转化成Hashtable file并压缩打包放入DistributedCache里面。</p><p><code>hive.auto.convert.join=true ;</code>  设置Map Join优化自动开启 </p><p><code>hive.smalltable.filesize=25000000L;</code>  参数控制（默认25M），当小表超过这个大小，hive会默认转化成common join。另一种是手动判断<code>/*+mapjoin(map_table)*/</code>。</p><p>认为开启Map Join：</p><p><code>select /* +mapjoin(b) */ a.id aid,name,age from a join b on a.id = b.id;</code></p><p>因为加了<code>/* +mapjoin(b) */</code>这一段代码，执行的时候就会将b表读入内存中，但是要求b表必须是小表，数据量不能太大。</p><h4 id="SMB-Map-Join"><a href="#SMB-Map-Join" class="headerlink" title="SMB Map Join"></a>SMB Map Join</h4><p>两个表关联键为imei，需要按imei分桶并且排序，小表（lxw_test）分桶数是大表（lxw_test1）的倍数。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> lxw_test(imei <span class="keyword">string</span>,sndaid <span class="keyword">string</span>,data_time <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">CLUSTERED <span class="keyword">BY</span>(imei) SORTED <span class="keyword">BY</span>(imei) <span class="keyword">INTO</span> <span class="number">10</span> BUCKETS;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> lxw_test1(imei <span class="keyword">string</span>,sndaid <span class="keyword">string</span>,data_time <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">CLUSTERED <span class="keyword">BY</span>(imei) SORTED <span class="keyword">BY</span>(imei) <span class="keyword">INTO</span> <span class="number">5</span> BUCKETS;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--插入数据前需要打开该选项</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--join时需要打开的参数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+ mapjoin(b) */</span> <span class="keyword">count</span>(<span class="number">1</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxw_test1 a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> lxw_test b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> a.imei = b.imei</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--包括insert数据，差不多10分钟左右；</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--如果这两个表做普通的join, 耗时1个多小时，没跑完，kill掉了。</span></span></pre></td></tr></table></figure><h4 id="嵌套Map-Join"><a href="#嵌套Map-Join" class="headerlink" title="嵌套Map Join"></a>嵌套Map Join</h4><p>小表不大不小，怎么用map join解决倾斜问题。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/*+mapjoin(x)*/</span>* <span class="keyword">from</span> <span class="keyword">log</span> a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span>  <span class="comment">/*+mapjoin(c)*/</span>d.*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">from</span> ( <span class="keyword">select</span> <span class="keyword">distinct</span> user_id <span class="keyword">from</span> <span class="keyword">log</span> ) c</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">join</span> <span class="keyword">users</span> d</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">on</span> c.user_id = d.user_id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    ) x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">on</span> a.user_id = b.user_id;</span></pre></td></tr></table></figure><h4 id="where优化"><a href="#where优化" class="headerlink" title="where优化"></a>where优化</h4><p>WHERE条件放置在ON条件中，以达到两表做join的时候，数据量相对变小的效果。</p><h4 id="union-all优化"><a href="#union-all优化" class="headerlink" title="union all优化"></a>union all优化</h4><p>union all优化还不完善，嵌套查询时会生成过多的job。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--Before</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> *</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> t1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">union</span> <span class="keyword">all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">select</span> * <span class="keyword">from</span> t4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">union</span> <span class="keyword">all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">select</span> * <span class="keyword">from</span> t2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">join</span> t3</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">on</span> t2.id = t3.id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">     ) x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> c1,c2;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--After</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">--先join生成临时表，后union all。原来4个jobs，改进后变成2个jobs。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> t5</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">join</span> t3</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">     <span class="keyword">on</span> t2.id = t3.id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (t1 <span class="keyword">union</span> <span class="keyword">all</span> t4 <span class="keyword">union</span> <span class="keyword">all</span> t5);</span></pre></td></tr></table></figure><h4 id="count-distinct优化"><a href="#count-distinct优化" class="headerlink" title="count distinct优化"></a>count distinct优化</h4><p>在Hive中应小心使用count distinct，因为很可能出现性能问题。</p><p>因为要去重，hive会把map阶段的输出全部分配到一个reduce task上，此时很容易发生性能问题，我们可以先group by，再count，减少distinct的使用。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/*改写前*/</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">count</span>(<span class="keyword">distinct</span> course) <span class="keyword">as</span> course_count</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/*改写后*/</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id,<span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">as</span> course_count</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span> <span class="keyword">distinct</span> <span class="keyword">id</span>,course </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">from</span> student</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">) a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 或者</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id,<span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">as</span> course_count </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> student </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>,course</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">) a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.id;</span></pre></td></tr></table></figure><h4 id="空值特殊处理"><a href="#空值特殊处理" class="headerlink" title="空值特殊处理"></a>空值特殊处理</h4><p>在日志中，常会有字段值丢失的问题，比如日志中的user_id，如果取其中的user_id和用户表中的user_id相关联，就会碰到数据倾斜的问题。</p><p>方法1，user_id为空的不参与关联：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> <span class="keyword">user</span> b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> a.user_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> a.user_id = b.user_id </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> <span class="keyword">all</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> c </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> c.user_id <span class="keyword">is</span> <span class="literal">null</span>;</span></pre></td></tr></table></figure><p>方法2，把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">user</span> b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> a.user_id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">'null_'</span>,<span class="keyword">rand</span>()) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> a.user_id <span class="keyword">end</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">= b.user_id;</span></pre></td></tr></table></figure><p>方法2比方法1效率更好，不但IO少了，而且作业数也少了。</p><p>方法1中，log表读了两次，job数肯定是2，而方法2的job数是1.</p><p>方法2使本身为null的所有记录不会拥挤在同一个reduce task中，加上随机字符串值，会分散到过个reduce task中，由于null值关联不上，处理后并不影响最终结果。</p><h4 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h4><p>场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的join操作时，默认的hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reduce中。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">users</span> a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">logs</span> b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">on</span> a.usr_id = <span class="keyword">cast</span>(b.user_id <span class="keyword">as</span> <span class="keyword">string</span>)</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据倾斜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的数据分桶及使用场景</title>
      <link href="/2018/05/14/hive/hive-bucket/"/>
      <url>/2018/05/14/hive/hive-bucket/</url>
      
        <content type="html"><![CDATA[<h3 id="数据分桶引入"><a href="#数据分桶引入" class="headerlink" title="数据分桶引入"></a>数据分桶引入</h3><p>分区提供了一个隔离数据和优化查询的便利方式，不过并非所有的数据都可以形成合理的分区，尤其是需要确定合适大小的分区划分时（不合理的分区划分方式可能导致有的分区数据过多，而某些分区没有多少数据的情况）。</p><p>分桶是将数据集分解为若干部分的一种技术。</p><h3 id="分桶的原理"><a href="#分桶的原理" class="headerlink" title="分桶的原理"></a>分桶的原理</h3><p>跟MR中的HashPartitioner的原理一样。</p><p>MR中，按照key的hash值去模除以reduceTask的个数。</p><p>Hive中，按照分桶字段的hash值去模除以分桶的个数。</p><p>Hive也是针对某一列进行桶的组织，Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p><h3 id="分桶的作用"><a href="#分桶的作用" class="headerlink" title="分桶的作用"></a>分桶的作用</h3><ul><li><p>方便抽样</p><p>使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，能够在数据集的一小部分数据上试运行查询。</p></li><li><p>提高join查询效率</p><p>获得更高的查询处理效率。桶为表加上了额外的结构，Hive在处理有些查询时能利用这个结构。具体而言，连接两个在相同列（包含连接列的）上划分了桶的表，可以使用Map端连接（Map-side join）高效地实现。比如join操作，对于join操作两个表有一个相同的列，如果对这两个表都进行了桶操作，那么将保存相同列值的桶进行join操作就可以，可以大大减小join的数据量。</p></li></ul><h3 id="分桶表的创建"><a href="#分桶表的创建" class="headerlink" title="分桶表的创建"></a>分桶表的创建</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> clickcube;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> <span class="string">`clickcube_mid`</span>(             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`logtype`</span> <span class="built_in">bigint</span>,                                </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`date`</span> <span class="keyword">string</span>,                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`hour`</span> <span class="built_in">bigint</span>,                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`projectid`</span> <span class="built_in">bigint</span>,                              </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`campaignid`</span> <span class="built_in">bigint</span>,                             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`templateid`</span> <span class="built_in">bigint</span>,                             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`mediaid`</span> <span class="built_in">bigint</span>,                                </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`slotid`</span> <span class="built_in">bigint</span>,                                 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`channeltype`</span> <span class="built_in">bigint</span>,                            </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`regioncode`</span> <span class="keyword">string</span>,                             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`campclick`</span> <span class="built_in">bigint</span>,                              </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`campimp`</span> <span class="built_in">bigint</span>,                                </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`mediaclick`</span> <span class="built_in">bigint</span>,                             </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`mediaimp`</span> <span class="built_in">bigint</span>,                               </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`templateimp`</span> <span class="built_in">bigint</span>,                            </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`templatecampimp`</span> <span class="built_in">bigint</span>,                        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`mediaclickcost`</span> <span class="keyword">double</span>,                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`campclickcost`</span> <span class="keyword">double</span>)                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  PARTITIONED <span class="keyword">BY</span> (                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">   <span class="string">`day`</span> <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">  CLUSTERED <span class="keyword">BY</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="string">`campaignid`</span>,  <span class="string">`mediaid`</span> ) <span class="keyword">INTO</span> <span class="number">100</span> BUCKETS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">STORED</span> <span class="keyword">AS</span> INPUTFORMAT</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">  OUTPUTFORMAT</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">  TBLPROPERTIES (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'last_modified_by'</span>=<span class="string">'cloudera-scm'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'last_modified_time'</span>=<span class="string">'1530676367'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'transient_lastDdlTime'</span>=<span class="string">'1530676367'</span>)</span></pre></td></tr></table></figure><h3 id="将数据插入分桶表"><a href="#将数据插入分桶表" class="headerlink" title="将数据插入分桶表"></a>将数据插入分桶表</h3><p>步骤：</p><ol><li>从HDFS或本地磁盘中load数据，导入中间表</li><li>通过从中间表查询的方式完成数据导入</li></ol><p>分桶的本质就是对分桶的字段做了hash，然后存放到对应的文件中，所以说如果原有数据没有按key hash，需要在插入分桶的时候hash，也就是说向分桶表中插入数据的时候必然要执行一次MapReduce，分桶表的数据基本只能通过从结果集查询插入的方式进行导入。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> clickcube;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> clickcube_mid_bucket </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">PARTITION</span>( <span class="keyword">day</span> = <span class="string">'2018-07-03'</span> )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.logtype,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.<span class="string">`date`</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.<span class="string">`hour`</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.projectid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campaignid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templateid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.slotid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.channeltype,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.regioncode,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campclick,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaclick,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templateimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templatecampimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaclickcost,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campclickcost </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> clickcube_mid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> <span class="keyword">day</span> = <span class="string">'2018-07-03'</span></span></pre></td></tr></table></figure><p>我们需要确保reduce的数量与表中的bucket数量一致，为此有两种做法：</p><ol><li><p>让hive强制分桶，自动按照分桶表的bucket进行分桶（推荐）</p><p><code>set hive.enforce.bucketing=true;</code></p></li><li><p>手动指定reduce数量</p><p><code>set mapreduce.job.reduces=num;</code></p><p><code>set mapreduce.reduce.tasks=num;</code></p><p>并在SELECT后增加CLUSTER BY语句</p></li></ol><p>整体的数据导入脚本：</p><p>insert_into_bucket.hql   数据导入HQL</p><p>insert_into_bucket.init   设置初始环境</p><p>insert_into_bucket.sh     主体执行脚本</p><p>insert_into_bucket.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> -o errexit</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">ROOT_PATH=$(dirname $(readlink -f <span class="variable">$0</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$ROOT_PATH</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">date_pattern_old=<span class="string">'^[0-9]&#123;4&#125;-[0-9]&#123;1,2&#125;-[0-9]&#123;1,2&#125;$'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">date_pattern=<span class="string">'^[0-9]&#123;4&#125;-((0([1-9]&#123;1&#125;))|(1[1|2]))-(([0-2]([0-9]&#123;1&#125;))|(3[0|1]))$'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#参数数量</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">argsnum=<span class="variable">$#</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#一些默认值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">curDate=`date +%Y%m%d`</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">partitionDate=`date -d <span class="string">'-1 day'</span> +%Y-%m-%d`</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">fileLocDate=`date -d <span class="string">'-1 day'</span> +%Y-%m-%d`</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#日志存放位置</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">logdir=insert_bucket_logs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">tips</span></span>() &#123; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Usage : insert_into_bucket.sh [date]"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Args :"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"date"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"date use this format yyyy-MM-dd , ex : 2018-06-02"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">echo</span> <span class="string">"============================================================"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Example :"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"example1 : sh insert_into_bucket.sh"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"example2 : sh insert_into_bucket.sh 2018-06-02"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="variable">$argsnum</span> -eq 0 ] ; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"No argument, use default value"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> [ <span class="variable">$argsnum</span> -eq 1 ] ; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"One argument, check date pattern"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">arg1=<span class="variable">$1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ! [[ <span class="string">"<span class="variable">$arg1</span>"</span> =~ <span class="variable">$date_pattern</span> ]] ; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">       <span class="built_in">echo</span> -e <span class="string">"\033[31m Please specify valid date in format like 2018-06-02"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">       <span class="built_in">echo</span> -e <span class="string">"\033[0m"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">       tips</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">        <span class="built_in">exit</span> 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fi</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">dateArr=($(<span class="built_in">echo</span> <span class="variable">$arg1</span> |tr <span class="string">"-"</span> <span class="string">" "</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"dateArr length is "</span><span class="variable">$&#123;#dateArr[@]&#125;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">partitionDate=<span class="variable">$&#123;dateArr[0]&#125;</span>-<span class="variable">$&#123;dateArr[1]&#125;</span>-<span class="variable">$&#123;dateArr[2]&#125;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">"\033[31m Not valid num of arguments"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">"\033[0m"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">tips</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span> 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fi</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ ! -d <span class="string">"<span class="variable">$logdir</span>"</span> ]; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">    mkdir -p <span class="variable">$logdir</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fi</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$ROOT_PATH</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#nohup hive -hivevar p_date=$&#123;partitionDate&#125; -hivevar f_date=$&#123;fileLocDate&#125; -f  hdfs_add_partition_dmp_clearlog.hql  &gt;&gt; $logdir/load_$&#123;curDate&#125;.log</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">nohup beeline -u jdbc:hive2://master:10000 -n root --color=<span class="literal">true</span> --silent=<span class="literal">false</span>  --hivevar p_date=<span class="variable">$&#123;partitionDate&#125;</span> -i insert_into_bucket.init -f insert_into_bucket.hql  &gt;&gt; <span class="variable">$logdir</span>/insert_bucket_<span class="variable">$&#123;curDate&#125;</span>.<span class="built_in">log</span></span></pre></td></tr></table></figure><p>insert_into_bucket.init：</p><p><code>set hive.enforce.bucketing = true;</code></p><p>insert_into_bucket.hql：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> clickcube;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> clickcube_mid_bucket </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">PARTITION</span>( <span class="keyword">day</span> = <span class="string">'$&#123;hivevar:p_date&#125;'</span> )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.logtype,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.<span class="string">`date`</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.<span class="string">`hour`</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.projectid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campaignid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templateid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.slotid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.channeltype,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.regioncode,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campclick,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaclick,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templateimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.templatecampimp,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.mediaclickcost,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">clickcube_mid.campclickcost  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> clickcube_mid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> <span class="keyword">day</span> = <span class="string">'$&#123;hivevar:p_date&#125;'</span></span></pre></td></tr></table></figure><h3 id="针对于分桶表的数据抽样"><a href="#针对于分桶表的数据抽样" class="headerlink" title="针对于分桶表的数据抽样"></a>针对于分桶表的数据抽样</h3><p>分桶的一个主要优势就是数据抽样，主要有两种方式：</p><ol><li>基于桶抽样</li><li>基于百分比抽样</li></ol><h4 id="基于桶抽样"><a href="#基于桶抽样" class="headerlink" title="基于桶抽样"></a>基于桶抽样</h4><p><code>select * from bucketed_users tablesample(bucket 1 out of 4 on id);</code></p><p>桶的个数从1开始计数，因此，前面的查询从4个桶的第一个中获取所有的用户。对于一个大规模的、均匀分布的数据集，这会返回表中约四分之一的数据行。我们也可以用其他比例对若干个桶进行取样（因为取样并不是一个精确的操作，因此这个比例不一定要是桶数的整数倍）。</p><p><strong>说法一：</strong></p><p><code>注：</code>tablesample是抽样语句，语法：<code>tablesample(bucket x out of y)</code></p><p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了64份，y=32时，抽取（64/32=）2个bucket的数据，当y=128时，抽取（64/128=）1/2个bucket的数据。</p><p>x表示从哪个bucket开始抽取。例如，table总bucket数为32，<code>tablesample(bucket 3 out of 16)</code>，表示总共抽（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</p><p><strong>说法二：</strong></p><p>分桶语句中的分母表示的是数据将会被散列的桶的个数，分子表示将会选择的桶的个数。</p><p><strong>实例：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> clickcube_mid_bucket </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">10</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">100</span> <span class="keyword">on</span> <span class="keyword">rand</span>()) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> <span class="keyword">day</span> = <span class="string">'2018-07-03'</span>;</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/hive/hive-bucket-sample.png" alt="基于桶的抽样"></p><h4 id="基于百分比抽样"><a href="#基于百分比抽样" class="headerlink" title="基于百分比抽样"></a>基于百分比抽样</h4><p>hive另外一种按照抽样百分比进行抽样的方式，该种方式基于行数，按照输入路径下的数据块的百分比进行抽样。这种抽样的最小单元是一个HDFS数据块，如果表的数据大小小于普通块大小128M，将返回所有行。</p><p>基于百分比的抽样方式提供了一个变量，用于控制基于数据块的调优种子信息：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.sample.seednumber<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>A number userd for percentage sampling. By changing this number, user will change the subsets of data sampled.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><h3 id="数据分桶的缺点"><a href="#数据分桶的缺点" class="headerlink" title="数据分桶的缺点"></a>数据分桶的缺点</h3><p>如果通过数据文件load到分桶表中，会存在额外的MR负担。</p><p>实际生产中分桶策略使用频率较低，更常见的还是使用数据分区。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive分桶 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么kafka这么快，又能保证消息不丢失？</title>
      <link href="/2018/05/13/kafka/kafka-3/"/>
      <url>/2018/05/13/kafka/kafka-3/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么Kafka这么快？"><a href="#为什么Kafka这么快？" class="headerlink" title="为什么Kafka这么快？"></a>为什么Kafka这么快？</h3><h4 id="顺序写磁盘"><a href="#顺序写磁盘" class="headerlink" title="顺序写磁盘"></a>顺序写磁盘</h4><p>顺序写磁盘的性能是随机写入的性能的6000倍的提升，媲美内存随机访问的性能，磁盘不再是瓶颈点。</p><h4 id="Page-Cache"><a href="#Page-Cache" class="headerlink" title="Page Cache"></a>Page Cache</h4><p>为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。</p><h4 id="零拷贝技术"><a href="#零拷贝技术" class="headerlink" title="零拷贝技术"></a>零拷贝技术</h4><p>零拷贝技术，可以有效的减少上下文切换和拷贝次数。</p><h3 id="如何做到消息不丢失"><a href="#如何做到消息不丢失" class="headerlink" title="如何做到消息不丢失"></a>如何做到消息不丢失</h3><h4 id="ACK机制"><a href="#ACK机制" class="headerlink" title="ACK机制"></a>ACK机制</h4><p>通过 ACK 机制保证消息送达。Kafka 采用的是至少一次（At least once），消息不会丢，但是可能会重复传输。</p><h4 id="发送消息"><a href="#发送消息" class="headerlink" title="发送消息"></a>发送消息</h4><p>为了得到更好的性能，Kafka 支持在生产者一侧进行本地buffer，也就是累积到一定的条数才发送，如果这里设置不当是会丢消息的。</p><p>生产者端设置 producer.type=async, sync，默认是 sync。</p><p>当设置为 async，会大幅提升性能，因为生产者会在本地缓冲消息，并适时批量发送。</p><p>如果对可靠性要求高，那么这里可以设置为 sync 同步发送。</p><h4 id="消费消息"><a href="#消费消息" class="headerlink" title="消费消息"></a>消费消息</h4><p>如果更注重可靠性，则需要显示提交 Offset，也就是当所有业务都处理完成的时候，再提交 Offset。这样会导致重复消费，需要提供幂等性接口。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka工作流程</title>
      <link href="/2018/05/12/kafka/kafka-2/"/>
      <url>/2018/05/12/kafka/kafka-2/</url>
      
        <content type="html"><![CDATA[<h3 id="Kafka生产过程"><a href="#Kafka生产过程" class="headerlink" title="Kafka生产过程"></a>Kafka生产过程</h3><h4 id="写入方式"><a href="#写入方式" class="headerlink" title="写入方式"></a>写入方式</h4><p>producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。</p><h4 id="分区（Partition）"><a href="#分区（Partition）" class="headerlink" title="分区（Partition）"></a>分区（Partition）</h4><p>Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息。</p><p>Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。</p><p>消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：</p><p><img src="https://vinxikk.github.io/img/kafka/anatomy-of-a-topic.png" alt="Topic的结构"></p><p>上图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响。</p><p><img src="https://vinxikk.github.io/img/kafka/partition-producer-consumer.png" alt="Partition的消息是有序的"></p><p>我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。</p><p>发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。</p><ol><li><p>分区的原因</p><ol><li><p>方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p></li><li><p>可以提高并发，因为可以以Partition为单位读写了。</p><p>传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。</p><p>Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。</p></li></ol></li><li><p>分区的原则</p><ol><li><p>指定了partition，则直接使用；</p></li><li><p>未指定partition但指定key，通过对key的value进行hash出一个partition；</p></li><li><p>partition和key都未指定，使用轮询选出一个partition。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// DefaultPartitioner类</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">            &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">            <span class="comment">// hash the keyBytes to choose a partition</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr></table></figure></li></ol></li></ol><h4 id="副本（Replication）"><a href="#副本（Replication）" class="headerlink" title="副本（Replication）"></a>副本（Replication）</h4><p>同一个partition可能会有多个replication（对应 server.properties 配置中的 <code>default.replication.factor=N</code>）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。</p><h4 id="写入流程"><a href="#写入流程" class="headerlink" title="写入流程"></a>写入流程</h4><p>producer写入消息流程如下：</p><ol><li>producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader</li><li>producer将消息发送给该leader</li><li>leader将消息写入本地log</li><li>followers从leader pull消息，写入本地log后向leader发送ACK</li><li>leader收到所有ISR（In Sync Replicas）中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK</li></ol><h3 id="Broker保存信息"><a href="#Broker保存信息" class="headerlink" title="Broker保存信息"></a>Broker保存信息</h3><h4 id="存储方式"><a href="#存储方式" class="headerlink" title="存储方式"></a>存储方式</h4><p>物理上把topic分成一个或多个patition（对应 server.properties 中的<code>num.partitions=3</code>配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 logs]$ ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">drwxrwxr-x. 2 vinx vinx  4096 8月   6 14:37 first-0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">drwxrwxr-x. 2 vinx vinx  4096 8月   6 14:35 first-1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">drwxrwxr-x. 2 vinx vinx  4096 8月   6 14:37 first-2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 logs]$ <span class="built_in">cd</span> first-0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 first-0]$ ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 vinx vinx 10485760 8月   6 14:33 00000000000000000000.index</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 vinx vinx      219 8月   6 15:07 00000000000000000000.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 vinx vinx 10485756 8月   6 14:33 00000000000000000000.timeindex</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">-rw-rw-r--. 1 vinx vinx        8 8月   6 14:37 leader-epoch-checkpoint</span></pre></td></tr></table></figure><h4 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h4><p>无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：</p><ol><li>基于时间：<code>log.retention.hours=168</code></li><li>基于大小：<code>log.retention.bytes=1073741824</code></li></ol><p>需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。</p><h3 id="Kafka消费过程"><a href="#Kafka消费过程" class="headerlink" title="Kafka消费过程"></a>Kafka消费过程</h3><p>kafka提供了两套consumer API：高级Consumer API和低级API。</p><h4 id="消费模型"><a href="#消费模型" class="headerlink" title="消费模型"></a>消费模型</h4><p>消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）。</p><p>基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的。</p><p>Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。</p><p><img src="https://vinxikk.github.io/img/kafka/consumer-pull.png" alt="consumer的pull拉取模式"></p><p>在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉。</p><h4 id="高级API"><a href="#高级API" class="headerlink" title="高级API"></a>高级API</h4><p>高级API优点：</p><ol><li>写起来简单</li><li>不需要自行去管理offset，系统通过zookeeper自行管理</li><li>不需要管理分区、副本等情况，系统自动管理</li><li>消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset）</li><li>可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）</li></ol><p>高级API缺点：</p><ol><li>不能自行控制offset（对于某些特殊需求来说）</li><li>不能细化控制如分区、副本、zk等</li></ol><h4 id="低级API"><a href="#低级API" class="headerlink" title="低级API"></a>低级API</h4><p>低级API优点：</p><ol><li>能够让开发者自己控制offset，想从哪里读取就从哪里读取</li><li>自行控制连接分区，对分区自定义进行负载均衡</li><li>对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）</li></ol><p>低级API缺点：</p><ol><li>太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等</li></ol><h4 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h4><p>消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。</p><p>在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区。</p><h4 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h4><p>consumer采用pull（拉）模式从broker中读取数据。</p><p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p><p>对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p><p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka Exactly Once语义</title>
      <link href="/2018/05/11/kafka/kafka-exactly-once/"/>
      <url>/2018/05/11/kafka/kafka-exactly-once/</url>
      
        <content type="html"><![CDATA[<h3 id="kafka是什么"><a href="#kafka是什么" class="headerlink" title="kafka是什么"></a>kafka是什么</h3><p>问题引入：Kafka会出现多次消费的情况，Kafka怎么才能确保一次性消费？</p><ol><li>checkpoint（学习可以，生产不建议）</li><li>kafka自身（至少一次消费语义）</li><li>外部存储（精准一次消费语义）</li></ol><p>无论消费多少，比如就算有重复，也要考虑幂等性设计。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka基本概念</title>
      <link href="/2018/05/10/kafka/kafka-1/"/>
      <url>/2018/05/10/kafka/kafka-1/</url>
      
        <content type="html"><![CDATA[<h3 id="kafka是什么"><a href="#kafka是什么" class="headerlink" title="kafka是什么"></a>kafka是什么</h3><p>在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。</p><ol><li>Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。</li><li>Kafka最初是由LinkedIn公司开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。</li><li>Kafka是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)称为broker。</li><li>无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。</li></ol><h4 id="为什么需要消息队列"><a href="#为什么需要消息队列" class="headerlink" title="为什么需要消息队列"></a>为什么需要消息队列</h4><ol><li><p>解耦：</p><p>允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p></li><li><p>冗余：</p><p>消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。</p></li><li><p>扩展性：</p><p>因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。</p></li><li><p>灵活性&amp;峰值处理能力：</p><p>在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。</p></li><li><p>可恢复性：</p><p>系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</p></li><li><p>顺序保证：</p><p>在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）</p></li><li><p>缓冲：</p><p>有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p></li><li><p>异步通信：</p><p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</p></li></ol><h4 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h4><ol><li>Producer: 消息生产者，就是向kafka broker发消息的客户端。</li><li>Consumer: 消息消费者，向kafka broker取消息的客户端。</li><li>Topic: 可以理解为一个队列。</li><li>Consumer Group: kafka提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者或消费者实例（consumer instance），它们共享一个公共的ID，即group ID。组内的所有消费者协调在一起来消费订阅主题（subscribed topics）的所有分区（partition）。当然，每个分区只能由同一个消费者组内的一个consumer来消费。</li><li>Broker: 一台kafka服务器就是一个broker。一个集群由多个broker组成，一个broker可以容纳多个topic。</li><li>Partition: 为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。</li><li>Offset: kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka</li></ol><h4 id="分布式模型"><a href="#分布式模型" class="headerlink" title="分布式模型"></a>分布式模型</h4><p>Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本（Leader），其他节点作为备份副本（Follower，也叫作从副本）。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本出现故障时，备份副本中的一个副本会被选择为新的主副本。因为每个分区的副本中只有主副本接受读写，所以每个服务器端都会作为某些分区的主副本，以及另外一些分区的备份副本，这样Kafka集群的所有服务端整体上对客户端是负载均衡的。</p><p>Kafka的生产者和消费者相对于服务器端而言都是客户端。</p><p>Kafka生产者客户端发布消息到服务端的指定主题，会指定消息所属的分区。生产者发布消息时根据消息是否有键，采用不同的分区策略。消息没有键时，通过轮询方式进行客户端负载均衡；消息有键时，根据分区语义（例如hash）确保相同键的消息总是发送到同一分区。</p><p>Kafka的消费者通过订阅主题来消费消息，并且每个消费者都会设置一个消费组名称。因为生产者发布到主题的每一条消息都只会发送给消费者组的一个消费者。所以，如果要实现传统消息系统的“队列”模型，可以让每个消费者都拥有相同的消费组名称，这样消息就会负责均衡到所有的消费者；如果要实现“发布-订阅”模型，则每个消费者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。</p><p>分区是消费者现场模型的最小并行单位。如下图（图1）所示，生产者发布消息到一台服务器的3个分区时，只有一个消费者消费所有的3个分区。在下图（图2）中，3个分区分布在3台服务器上，同时有3个消费者分别消费不同的分区。假设每个服务器的吞吐量是300MB，在下图（图1）中分摊到每个分区只有100MB，而在下图（图2）中，集群整体的吞吐量有900MB。可以看到，增加服务器节点会提升集群的性能，增加消费者数量会提升处理性能。</p><p><img src="https://vinxikk.github.io/img/kafka/kafka-partition.png" alt="Kafka的分区"></p><p>同一个消费组下多个消费者互相协调消费工作，Kafka会将所有的分区平均地分配给所有的消费者实例，这样每个消费者都可以分配到数量均等的分区。Kafka的消费组管理协议会动态地维护消费组的成员列表，当一个新消费者加入消费者组，或者有消费者离开消费组，都会触发再平衡操作。</p><p>Kafka的消费者消费消息时，只保证在一个分区内的消息的完全有序性，并不保证同一个主题汇中多个分区的消息顺序。而且，消费者读取一个分区消息的顺序和生产者写入到这个分区的顺序是一致的。比如，生产者写入“hello”和“Kafka”两条消息到分区P1，则消费者读取到的顺序也一定是“hello”和“Kafka”。如果业务上需要保证所有消息完全一致，只能通过设置一个分区完成，但这种做法的缺点是最多只能有一个消费者进行消费。一般来说，只需要保证每个分区的有序性，再对消息键(message Key 可以是user id等)来保证相同键的所有消息落入同一分区，就可以满足绝大多数的应用。</p><h3 id="Kafka命令行"><a href="#Kafka命令行" class="headerlink" title="Kafka命令行"></a>Kafka命令行</h3><ol><li><p>查看当前服务器中的所有topic</p><p><code>kafka-topics.sh --zookeeper hadoop001:2182 --list</code></p></li><li><p>创建topic</p><p><code>kafka-topics.sh --zookeeper hadoop001:2181 --create --replication-factor 3 --partitions 1 --topic first</code></p><p>选项说明：</p><p><code>--replication-factor</code> 定义副本数</p><p><code>--partitions</code> 定义分区数</p><p><code>--topic</code> 定义topic名</p></li><li><p>删除topic</p><p><code>kafka-topics.sh --zookeeper hadoop001:2181 --delete --topic first</code></p><p>需要server.properties中设置<code>delete.topic.enable=true</code>，否则只是标记删除或者直接重启。</p></li><li><p>发送消息</p><p><code>kafka-console-producer.sh --broker-list hadoop001:9092 --topic first</code></p></li><li><p>消费消息</p><p><code>kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --from-beginning --topic first</code></p><p><code>--from-beginning:</code> 会把first主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。</p></li><li><p>查看某个topic的详情</p><p>kafka-topics.sh –zookeeper hadoop001:2181 –describe –topic first</p></li></ol><h3 id="Kafka配置信息"><a href="#Kafka配置信息" class="headerlink" title="Kafka配置信息"></a>Kafka配置信息</h3><h4 id="Broker配置信息"><a href="#Broker配置信息" class="headerlink" title="Broker配置信息"></a>Broker配置信息</h4><table><thead><tr><th>属性</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>broker.id</td><td></td><td>broker的唯一标识</td></tr><tr><td>log.dirs</td><td>/tmp/kafka-logs</td><td>kafka数据存放的目录。可以指定多个目录，中间用逗号分隔，当新partition被创建时会被存放到当前存放partition最少的目录</td></tr><tr><td>port</td><td>9092</td><td>BrokerServer接受客户端连接的端口号</td></tr><tr><td>zookeeper.connect</td><td>null</td><td>Zookeeper的连接串，格式为：hostname1:port1,hostname2:port2,hostname3:port3。可以填一个或多个，为了提高可靠性，建议都填上。注意，此配置允许我们指定一个zookeeper路径来存放此kafka集群的所有数据，为了与其他应用集群区分开，建议在此配置中指定本集群存放目录，格式为：hostname1:port1,hostname2:port2,hostname3:port3/chroot/path 。需要注意的是，消费者的参数要和此参数一致。</td></tr><tr><td>delete.topic.enable</td><td>false</td><td>启用delete topic参数，建议设置为true</td></tr></tbody></table><h4 id="Producer配置信息"><a href="#Producer配置信息" class="headerlink" title="Producer配置信息"></a>Producer配置信息</h4><table><thead><tr><th>属性</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>metadata.broker.list</td><td></td><td>启动时producer查询brokers的列表，可以是集群中所有brokers的一个子集。注意，这个参数只是用来获取topic的元信息用，producer会从元信息中挑选合适的broker并与之建立socket连接。格式是：host1:port1,host2:port2。</td></tr><tr><td>request.required.acks</td><td>0</td><td></td></tr><tr><td>request.timeout.ms</td><td>10000</td><td>Broker等待ack的超时时间，若等待时间超过此值，会返回客户端错误信息。</td></tr><tr><td>producer.type</td><td>sync</td><td>同步异步模式。async表示异步，sync表示同步。如果设置成异步模式，可以允许生产者以batch的形式push数据，这样会极大的提高broker性能，推荐设置为异步。</td></tr></tbody></table><h4 id="Consumer配置信息"><a href="#Consumer配置信息" class="headerlink" title="Consumer配置信息"></a>Consumer配置信息</h4><table><thead><tr><th>属性</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>group.id</td><td></td><td>Consumer的组ID，相同goup.id的consumer属于同一个组。</td></tr><tr><td>zookeeper.connect</td><td></td><td>Consumer的zookeeper连接串，要和broker的配置一致。</td></tr><tr><td>consumer.id</td><td>null</td><td>如果不设置会自动生成。</td></tr><tr><td>socket.timeout.ms</td><td>30*1000</td><td>网络请求的socket超时时间。实际超时时间由max.fetch.wait + socket.timeout.ms 确定。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop导入数据ERROR: NoClassDefFoundError: org/json/JSONObject</title>
      <link href="/2018/05/09/sqoop/sqoop-import-error/"/>
      <url>/2018/05/09/sqoop/sqoop-import-error/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL数据导入HDFS-ERROR"><a href="#MySQL数据导入HDFS-ERROR" class="headerlink" title="MySQL数据导入HDFS ERROR"></a>MySQL数据导入HDFS ERROR</h3><p>执行如下sqoop命令出错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password ruozedata \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--table staff \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span></span></pre></td></tr></table></figure><p>抛错截图：</p><p><img src="https://vinxikk.github.io/img/sqoop/sqoop-mysql-to-hdfs-error.png" alt="MySQL导入HDFS抛错"></p><p>错误原因：</p><p>缺少<code>java-json.jar</code></p><p>解决办法：</p><p>添加相应jar包<code>cp java-json.jar /home/vinx/app/sqoop/lib/</code></p>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop-ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中delete时抛错：...update or delete... not support these operations.</title>
      <link href="/2018/05/08/hive/hive-delete-error/"/>
      <url>/2018/05/08/hive/hive-delete-error/</url>
      
        <content type="html"><![CDATA[<h3 id="delete-error"><a href="#delete-error" class="headerlink" title="delete error"></a>delete error</h3><p>在Hive中使用<code>delete</code>删除数据时抛以下错误：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10086&gt; delete from downstream where id in (5,6);</span></pre></td></tr></table></figure><p><em>Error: Error while compiling statement: FAILED: SemanticException [Error 10294]: Attempt to do update or delete using transaction manager that does not support these operations. (state=42000,code=10294)</em></p><p>错误原因：</p><p><code>update</code>和<code>delete</code>都属于事务操作，Hive的行级修改，需要开启事务。</p><p>解决办法：</p><p>在hive-site.xml中，增加如下配置：</p><p><code>hive.support.concurrency – true</code></p><p><code>hive.enforce.bucketing – true</code></p><p><code>hive.exec.dynamic.partition.mode – nonstrict</code></p><p><code>hive.txn.manager –org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</code></p><p><code>hive.compactor.initiator.on – true</code></p><p><code>hive.compactor.worker.threads – 1</code></p><p><code>hive.in.test - true</code></p><p>保存后，重启Hive服务。</p><hr><p>参考：</p><p><a href="https://stackoverflow.com/questions/34198339/attempt-to-do-update-or-delete-using-transaction-manager-that-does-not-support-t" target="_blank" rel="noopener">https://stackoverflow.com/questions/34198339/attempt-to-do-update-or-delete-using-transaction-manager-that-does-not-support-t</a></p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据重刷机制</title>
      <link href="/2018/05/07/dw/dw-data-rebrush/"/>
      <url>/2018/05/07/dw/dw-data-rebrush/</url>
      
        <content type="html"><![CDATA[<h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h3><p>使用Sqoop抽取数据，有时会出现数据的丢失，上下游数据会出现不一致的情况，但在整个过程中并没有抛ERROR。那么针对这个问题，该如何解决呢？</p><p>数据丢失或者不准确，归根结底就是：要么上下游数据量不同，要么数据内容不同。基于此，数据重刷要解决的问题就是验证上下游数据量是否相同，不相同则重刷；验证数据内容是否相同，不相同则重刷。</p><p>基于上述问题，为了确认使用Sqoop做数据迁移时，是否存在数据丢失，需要做数据监控。</p><h3 id="数据重刷"><a href="#数据重刷" class="headerlink" title="数据重刷"></a>数据重刷</h3><p>背景：已经count验证，现在发现上下游的数据不准确</p><p>重刷机制：通过对上下游的两个表full outer join来对比字段的NULL值</p><p>创建表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> upstream(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">in</span> <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> downstream <span class="keyword">like</span> upstream;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/upstream.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> upstream;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/downstream.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> downstream;</span></pre></td></tr></table></figure><p>对上游表和下游表做<code>full outer join</code>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">* </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> upstream up</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> downstream down</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> up.id = down.id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------+----------+------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| up.id  |  up.name  | down.id  | down.name  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------+----------+------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 1      | zhangsan  | 1        | zhangsan   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 2      | lisi      | NULL     | NULL       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 3      | wangwu    | 3        | wangwu     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| NULL   | NULL      | 5        | alice      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| NULL   | NULL      | 6        | tom        |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| 7      | jack      | NULL     | NULL       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------+----------+------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">6 rows selected (72.696 seconds)</span></pre></td></tr></table></figure><p>将全连接的结果保存：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> udstream <span class="keyword">as</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">u.id <span class="keyword">as</span> uid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">u.name <span class="keyword">as</span> uname,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">d.id <span class="keyword">as</span> did,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">d.name <span class="keyword">as</span> dname</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> upstream u</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> downstream d</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> u.id = d.id;</span></pre></td></tr></table></figure><p>以上游表upstream为标准，对全连接后的大表udstream做筛选：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> udstream <span class="keyword">where</span> uid <span class="keyword">is</span> <span class="literal">NULL</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| udstream.uid  | udstream.uname  | udstream.did  | udstream.dname  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| NULL          | NULL            | 5             | alice           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| NULL          | NULL            | 6             | tom             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows selected (0.129 seconds)</span></pre></td></tr></table></figure><p>发现did为5和6的行，uid为NULL，说明下游表downstream数据多了，需要删除did为5和6的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> downstream <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">in</span> (<span class="number">5</span>,<span class="number">6</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 也可以根据上面数据直接删除</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> downstream </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">in</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span> did <span class="keyword">from</span> udstream <span class="keyword">where</span> uid <span class="keyword">is</span> <span class="literal">NULL</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure><p><strong>注意：</strong>Hive中的<code>update</code>和<code>delete</code>都属于事务操作，需要开启事务。</p><p>发现uid为2和7的行，did为NULL，说明下游表downstream数据少了，需要追加uid为2和7的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> udstream <span class="keyword">where</span> did <span class="keyword">is</span> <span class="literal">NULL</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| udstream.uid  | udstream.uname  | udstream.did  | udstream.dname  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2             | lisi            | NULL          | NULL            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 7             | jack            | NULL          | NULL            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------+-----------------+---------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows selected (0.229 seconds)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> downstream <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">'lisi'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> downstream <span class="keyword">values</span>(<span class="number">7</span>,<span class="string">'jack'</span>);</span></pre></td></tr></table></figure><p>经过重新构建，也就是重刷后的数据是：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> downstream;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------+------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| downstream.id  | downstream.name  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------+------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 1              | zhangsan         |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 2              | lisi             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 3              | wangwu           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 7              | jack             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------------+------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4 rows selected (0.133 seconds)</span></pre></td></tr></table></figure><p>数据重刷完成，下游表的数据得到了校正。</p><p>总体流程就是：使用count校验数据量是否相同（也可以进一步校验数据内容是否相同），如果不同，则校验数据内容，并重刷下游表的数据。</p><p><code>full outer join</code>其实就是<code>left join</code> + <code>right join</code>的结果，为NULL的刚好是缺失的或者多出的，而交集是上下游都有的，需要做的就是对下游表做<code>delete</code>或<code>insert</code>。</p>]]></content>
      
      
      <categories>
          
          <category> DataWarehouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据重刷 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban入门</title>
      <link href="/2018/05/06/azkaban/azkaban-basic/"/>
      <url>/2018/05/06/azkaban/azkaban-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="Azkaban基础"><a href="#Azkaban基础" class="headerlink" title="Azkaban基础"></a>Azkaban基础</h3><hr><p>工作流的概念</p>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MongoDB同步数据到Hive</title>
      <link href="/2018/05/02/sqoop/sqoop-mongodb-to-hive/"/>
      <url>/2018/05/02/sqoop/sqoop-mongodb-to-hive/</url>
      
        <content type="html"><![CDATA[<h3 id="MongoDB同步数据到Hive"><a href="#MongoDB同步数据到Hive" class="headerlink" title="MongoDB同步数据到Hive"></a>MongoDB同步数据到Hive</h3><p>Sqoop作为常用的数据同步工具，常用于RDBMS和HDFS的数据迁移，但是不支持NoSql，比如说MongoDB。</p><p>假如业务数据库是MongoDB，现在的需求是把MongoDB的数据同步到Hive中。</p><p><img src="https://vinxikk.github.io/img/sqoop/sqoop-mongodb-to-hive.png" alt="MongoDB to Hive"></p><ol><li><p>MongoDB可以导出数据为csv格式或者json格式的文件，csv是以逗号分隔的，可以直接把这个文件put到HDFS上，然后load到Hive。但是如果数据本身就自带一个或者多个逗号，那么这样做就会造成字段错位的问题。基于此，我们选择生成json格式的文件。</p></li><li><p>MongoDB可以条件导出，这样我们可以每次只导出增量数据。</p></li><li><p>如果使用json格式的文件，那么数据相当于在Hive中只有一列，我们可以每天或者每月导出增量数据，每次同步数据的时候都把这次的数据放到以一个分区中（第一次同步之前需要建立好分区表）。可以以时间为分区字段，然后scp将文件发送到Hive的客户端上，通过调度工具每天或每月定时执行任务，将文件直接load到Hive分区表中。</p></li><li><p>导入到Hive表中后，这个Hive表只有1个主要字段及分区字段，可以通过hive_json_object(name,’$.xxx’)解析，导入一个新表里面，新表的字段就是json中的key。</p><p>比如json格式如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="attr">"name"</span>: <span class="string">"zhangsan"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="attr">"city"</span>: <span class="string">"shenzhen"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="attr">"children"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"girl"</span>: <span class="string">"alice"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>Hive中解析如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">get_json_object(<span class="keyword">name</span>, <span class="string">'$.name'</span>) <span class="keyword">as</span> <span class="keyword">name</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">get_json_object(<span class="keyword">name</span>, <span class="string">'$.city'</span>) <span class="keyword">as</span> city,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">get_json_object(get_json_object(<span class="keyword">name</span>, <span class="string">'$.girl'</span>), <span class="string">'$.children'</span>) <span class="keyword">as</span> girl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">tb_name;</span></pre></td></tr></table></figure></li><li><p>判断MongoDB的数据是否已经导入文件并发送过来</p><p>在调度器上面配置一个检查任务，当到了定时执行同步任务的时候让同步任务依赖一个检查任务，检查任务就是去不断检查scp的文件传输过来没，如果没有过来隔一段时间再去检查，直到检查到数据来了，检查任务就结束，后面的同步任务就开始执行。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的UDF自定义函数</title>
      <link href="/2018/04/28/hive/hive-udf/"/>
      <url>/2018/04/28/hive/hive-udf/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive的内置函数"><a href="#Hive的内置函数" class="headerlink" title="Hive的内置函数"></a>Hive的内置函数</h3><p>Hive为我们提供了一些内置函数，比如截取字段串、大小写转换等。</p><p>测试substr()函数：</p><ol><li><p>建立一个伪表dual：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual(<span class="keyword">id</span> <span class="keyword">string</span>);</span></pre></td></tr></table></figure></li><li><p>准备数据</p><p>在本地创建一个dual.txt文件，内容为一个空格或者空行</p></li><li><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/dual.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dual;</span></pre></td></tr></table></figure></li><li><p>进行测试：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">substr</span>(<span class="string">'spark'</span>,<span class="number">1</span>,<span class="number">3</span>) <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| spa  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.116 seconds)</span></pre></td></tr></table></figure></li><li><p>也可以直接使用：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">substr</span>(<span class="string">'spark'</span>,<span class="number">1</span>,<span class="number">3</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| spa  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.104 seconds)</span></pre></td></tr></table></figure></li></ol><h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><p>添加maven依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-metastore --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-metastore<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-common --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-service --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-service<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></pre></td></tr></table></figure><h4 id="大写转小写"><a href="#大写转小写" class="headerlink" title="大写转小写"></a>大写转小写</h4><ol><li><p>创建<code>UpperToLowerCase</code>类，继承<code>UDF</code>，重写<code>evaluate</code>方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UpperToLowerCase</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">/*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">     * 重载evaluate</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">     * 访问限制必须是public</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">     */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String word)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        String lowerWord = word.toLowerCase();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> lowerWord;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li><li><p>打包上传到hadoop集群上</p></li><li><p>将jar包放到hive的classpath下</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add jar /home/vinx/jars/hive.jar;</code></p></li><li><p>创建临时函数，指定完整类名（包名+类名）</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>create temporary function tolower as &#39;com.ivinx.hive.UpperToLowerCase&#39;;</code></p></li><li><p>使用临时函数</p><p><code>select tolower(&#39;HELLO SPARK&#39;);</code></p></li></ol><h4 id="根据电话号码显示归属地信息"><a href="#根据电话号码显示归属地信息" class="headerlink" title="根据电话号码显示归属地信息"></a>根据电话号码显示归属地信息</h4><ol><li><p>创建工具类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoneNumParse</span> <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">static</span> HashMap&lt;String, String&gt; phoneMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">static</span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        phoneMap.put(<span class="string">"136"</span>, <span class="string">"beijing"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        phoneMap.put(<span class="string">"137"</span>, <span class="string">"shanghai"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        phoneMap.put(<span class="string">"138"</span>, <span class="string">"shenzhen"</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">evaluate</span><span class="params">(<span class="keyword">int</span> phoneNum)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        String num = String.valueOf(phoneNum);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        String province = phoneMap.get(num.substring(<span class="number">0</span>, <span class="number">3</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> province==<span class="keyword">null</span>?<span class="string">"foreign"</span>:province;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//测试</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        String string = evaluate(<span class="number">136666</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        System.out.println(string);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li><li><p>重新打jar包，然后上传到hadoop集群上</p></li><li><p>将jar包放到hive的classpath下</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add jar /home/vinx/jars/hive.jar;</code></p></li><li><p>创建临时函数，指定完整类名</p><p>create temporary function getprovince as ‘com.ivinx.hive.PhoneNumParse’;</p></li><li><p>创建本地数据</p><p>创建flow.txt，并追加以下数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ vi flow.txt</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">1367788,1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">1367788,10</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">1377788,80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">1377788,97</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">1387788,98</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1387788,99</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">1387788,100</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">1555118,99</span></pre></td></tr></table></figure></li><li><p>创建表，然后加载数据</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>create table flow(phonenum int,flow int) row format delimited fields terminated by &#39;,&#39;;</code></p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>load data local inpath &#39;/home/vinx/data/flow.txt&#39; into table flow;</code></p></li><li><p>查询结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10086&gt; select phonenum,getprovince(phonenum),flow from flow;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------+-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| phonenum  |    _c1    | flow  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------+-------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 1367788   | beijing   | 1     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 1367788   | beijing   | 10    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 1377788   | shanghai  | 80    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 1377788   | shanghai  | 97    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 1387788   | shenzhen  | 98    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 1387788   | shenzhen  | 99    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 1387788   | shenzhen  | 100   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 1555118   | foreign   | 99    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------+-----------+-------+--+</span></span></pre></td></tr></table></figure></li></ol><h4 id="Json数据解析"><a href="#Json数据解析" class="headerlink" title="Json数据解析"></a>Json数据解析</h4><ol><li><p>创建数据源文件</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"1193"</span>,<span class="attr">"rate"</span>:<span class="string">"5"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978300760"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"661"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978302109"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"914"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978301968"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"3408"</span>,<span class="attr">"rate"</span>:<span class="string">"4"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978300275"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"2355"</span>,<span class="attr">"rate"</span>:<span class="string">"5"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978824291"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"movie"</span>:<span class="string">"1197"</span>,<span class="attr">"rate"</span>:<span class="string">"3"</span>,<span class="attr">"timeStamp"</span>:<span class="string">"978302268"</span>,<span class="attr">"uid"</span>:<span class="string">"1"</span>&#125;</span></pre></td></tr></table></figure></li><li><p>创建表，然后上传数据</p><p><code>create table json(line string);</code></p><p><code>load data local inpath ‘/home/vinx/data/json.txt’ into table json;</code></p></li><li><p>与json数据对应的javabean</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MovieRateBean</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String movie;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String rate;<span class="comment">//评分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String timeStamp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">private</span> String uid;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="meta">@Override</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span>  <span class="keyword">this</span>.movie+<span class="string">"\t"</span>+<span class="keyword">this</span>.rate+<span class="string">"\t"</span>+<span class="keyword">this</span>.timeStamp+<span class="string">"\t"</span>+<span class="keyword">this</span>.uid;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//  get、set方法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li><li><p>java工具类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.ivinx.hive;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.TypeReference;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonParse</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String jsonStr)</span></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        MovieRateBean movieRateBean=JSON.parseObject(jsonStr,<span class="keyword">new</span> TypeReference&lt;MovieRateBean&gt;()&#123;&#125;);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        returnmovieRateBean.toString();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure></li><li><p>打jar包然后上传到hadoop集群</p></li><li><p>将jar包添加到hive下的classpath</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add jar /home/vinx/jars/hive.jar;</code></p></li><li><p>将fastjson的jar包添加到hive下的classpath</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add jar /home/vinx/jars/fastjson-1.1.41.jar;</code></p></li><li><p>创建临时函数</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>create temporary function parsejson as &#39;com.ivinx.hive.JsonParse&#39;;</code></p></li><li><p>执行查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10086&gt; select parsejson(line) from json limit 10;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|         _c0         |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 1193  5       978300760       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 661   3       978302109       1   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 914   3       978301968       1   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 3408  4       978300275       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 2355  5       978824291       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 1197  3       978302268       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 1287  5       978302039       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 2804  5       978300719       1  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| 594   4       978302268       1   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| 919   4       978301368       1   |</span></pre></td></tr></table></figure></li><li><p>显示字段名</p><p>从上面的结果可以看出来，数据虽然分开了，但是没有字段名，现在我们通过建表来实现显示字段名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie <span class="keyword">as</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">split</span>(parsejson(line), <span class="string">'\t'</span>)[<span class="number">0</span>] <span class="keyword">as</span> movieid,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">split</span>(parsejson(line), <span class="string">'\t'</span>)[<span class="number">1</span>] <span class="keyword">as</span> rate,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">split</span>(parsejson(line), <span class="string">'\t'</span>)[<span class="number">2</span>] <span class="keyword">as</span> timestring,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">split</span>(parsejson(line), <span class="string">'\t'</span>)[<span class="number">3</span>] <span class="keyword">as</span> uid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> <span class="keyword">json</span>;</span></pre></td></tr></table></figure><p>再次执行查询，查看结果：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10086&gt; select * from movie;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+----------------+----------------------+---------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| t_rating.movieid  | t_rating.rate  | t_rating.timestring  | t_rating.uid  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+----------------+----------------------+---------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 919               | 4              | 978301368            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 594               | 4              | 978302268            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 2804              | 5              | 978300719            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 1287              | 5              | 978302039            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 1197              | 3              | 978302268            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 2355              | 5              | 978824291            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 3408              | 4              | 978300275            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 914               | 3              | 978301968            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| 661               | 3              | 978302109            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| 1193              | 5              | 978300760            | 1             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+----------------+----------------------+---------------+--+</span></span></pre></td></tr></table></figure></li></ol><h3 id="transform关键字的使用"><a href="#transform关键字的使用" class="headerlink" title="transform关键字的使用"></a>transform关键字的使用</h3><p>将某一个字段时间戳改为输出周几，可以不用实现UDF。</p><p>现在直接使用上面创建好的表，将第三个字段改为时间。</p><h4 id="编写python脚本"><a href="#编写python脚本" class="headerlink" title="编写python脚本"></a>编写python脚本</h4><p>在本地创建一个python脚本trans.py：</p><p><code>vi trans.py</code></p><p>脚本代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  line = line.strip()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  movieid, rating, unixtime,userid = line.split(<span class="string">'\t'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">print</span> <span class="string">'\t'</span>.join([movieid, rating, str(weekday),userid])</span></pre></td></tr></table></figure><h4 id="使用该脚本"><a href="#使用该脚本" class="headerlink" title="使用该脚本"></a>使用该脚本</h4><ol><li><p>添加脚本到hive</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>add FILE /home/vinx/scripts/trans.py;</code></p></li><li><p>使用该脚本</p><p>0: jdbc:hive2://hadoop001:10086&gt; <code>select transform(movieid,rate,timestring,uid) using &#39;python trans.py&#39; as (mov,rat,tim,uid) from movie;</code></p><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">INFO  : Number of reduce tasks is set to 0 since there&#39;s no reduce operator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">INFO  : number of splits:1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">INFO  : Submitting tokens for job: job_1546821616463_0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">INFO  : The url to track the job: http:&#x2F;&#x2F;hadoop001:8088&#x2F;proxy&#x2F;application_1546821616463_0002&#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">INFO  : Starting Job &#x3D; job_1546821616463_0002, Tracking URL &#x3D; http:&#x2F;&#x2F;hadoop001:8088&#x2F;proxy&#x2F;application_1546821616463_0002&#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">INFO  : Kill Command &#x3D; &#x2F;home&#x2F;vinx&#x2F;app&#x2F;hadoop&#x2F;bin&#x2F;hadoop job  -kill job_1546821616463_0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">INFO  : 2018-01-05 00:35:37,847 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">INFO  : 2018-01-05 00:35:56,316 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 1.55 sec</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">INFO  : MapReduce Total cumulative CPU time: 1 seconds 550 msec</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">INFO  : Ended Job &#x3D; job_1546821616463_0002</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+-------+------+------+------+--+</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|  mov  | rat  | tim  | uid  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+-------+------+------+------+--+</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">| 1197  | 3    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">| 2355  | 5    | 7    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">| 3408  | 4    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">| 914   | 3    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">| 661   | 3    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">| 1193  | 5    | 1    | 1    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">+-------+------+------+------+--+</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">6 rows selected (36.201 seconds)</span></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的dual伪表</title>
      <link href="/2018/04/27/hive/hive-dual/"/>
      <url>/2018/04/27/hive/hive-dual/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive的伪表"><a href="#Hive的伪表" class="headerlink" title="Hive的伪表"></a>Hive的伪表</h3><p>Hive中的伪表，也叫虚表，和Oracle中的dual相似，dual是Oracle提供的最小的工作表，只有一行一列，具有某些特殊的功能。</p><p>下面我们在Hive中构建一个类似于Oracle的dual虚表。</p><p>创建dual.txt，将X重定向到文件中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 data]$ touch dual.txt</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 data]$ <span class="built_in">echo</span> <span class="string">'X'</span> &gt; dual.txt</span></pre></td></tr></table></figure><p>在hive中创建dual表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dual (dummy <span class="keyword">string</span>);</span></pre></td></tr></table></figure><p>加载dual.txt数据到dual表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/dual.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dual;</span></pre></td></tr></table></figure><p>查看dual虚表中的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| dual.dummy  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| X           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.142 seconds)</span></pre></td></tr></table></figure><p>测试dual虚表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"> <span class="keyword">select</span> <span class="number">1</span>+<span class="number">1</span> <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> +<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| _c0  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 2    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.142 seconds)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="string">'hello hive'</span> <span class="keyword">from</span> dual;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|     _c0     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| hello hive  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.159 seconds)</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-dual </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的调优（二）</title>
      <link href="/2018/04/26/hive/hive-optimize-2/"/>
      <url>/2018/04/26/hive/hive-optimize-2/</url>
      
        <content type="html"><![CDATA[<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><h4 id="合理设置Map数"><a href="#合理设置Map数" class="headerlink" title="合理设置Map数"></a>合理设置Map数</h4><ol><li><p>通常情况下，job会通过input的目录产生一个或者多个map任务。</p><p>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</p></li><li><p>是不是map数越多越好？</p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小128M），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p></li><li><p>是不是保证每个map处理接近128M的文件块，就高枕无忧了？</p><p>答案是不一定。比如有一个127M的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却又几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p></li></ol><p>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数。</p><h4 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h4><p>在map执行前合并小文件，减少map。CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式），HiveInputFormat没有对小文件的合并功能。</p><p><code>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</code></p><h4 id="复杂文件增加Map数"><a href="#复杂文件增加Map数" class="headerlink" title="复杂文件增加Map数"></a>复杂文件增加Map数</h4><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减小，从而提高任务的执行效率。</p><p>增加map的方法：</p><p>根据<code>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))</code>=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><p>案例剖析：</p><ol><li><p>执行查询</p><p>hive (default)&gt; <code>select count(*) from emp;</code></p><p>Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</p></li><li><p>设置最大切片值为100个字节</p><p>hive (default)&gt; <code>set mapreduce.input.fileinputformat.split.maxsize=100;</code></p><p>hive (default)&gt; <code>select count(*) from emp;</code></p><p>Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1</p></li></ol><h4 id="合理设置Reduce数"><a href="#合理设置Reduce数" class="headerlink" title="合理设置Reduce数"></a>合理设置Reduce数</h4><h5 id="调整reduce个数方法一"><a href="#调整reduce个数方法一" class="headerlink" title="调整reduce个数方法一"></a>调整reduce个数方法一</h5><ol><li><p>每个Reduce处理的数据量默认是256MB</p><p><code>set hive.exec.reducers.bytes.per.reducer=256000000</code></p></li><li><p>每个任务最大的reduce数，默认为1009</p><p><code>set hive.exec.reducers.max=1009</code></p></li><li><p>计算reducer数的公式</p><p>N=min(参数2=1009，总输入数据量/参数1=？)</p></li></ol><h5 id="调整reduce个数方法二"><a href="#调整reduce个数方法二" class="headerlink" title="调整reduce个数方法二"></a>调整reduce个数方法二</h5><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数：</p><p><code>set mapreduce.job.reduces=5;</code></p><h5 id="reduce个数并不是越多越好"><a href="#reduce个数并不是越多越好" class="headerlink" title="reduce个数并不是越多越好"></a>reduce个数并不是越多越好</h5><ol><li>过多的启动和初始化reduce也会消耗时间和资源；</li><li>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li></ol><p>在设置reduce个数的时候也需要考虑这两个原则：</p><p>处理大数据量利用合适的reduce数；单个reduce任务处理数据量大小要合适。</p><h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数<code>hive.exec.parallel</code>值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><p>打开任务并行执行（默认false）：</p><p><code>set hive.exec.parallel=true;</code></p><p>同一个sql允许最大并行度（默认为8）：</p><p><code>set hive.exec.parallel.thread.number=16;</code>  </p><p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p><h3 id="严格模式"><a href="#严格模式" class="headerlink" title="严格模式"></a>严格模式</h3><p>Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。</p><p>开启严格模式需要修改<code>hive.mapred.mode</code>值为strict，开启严格模式可以禁止3种类型的查询：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>strict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    The mode in which the Hive operations are being performed. </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    In strict mode, some risky queries are not allowed to run. They include:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      Cartesian Product.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      No partition being picked up for a query.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      Comparing bigints and strings.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      Comparing bigints and doubles.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">      Orderby without limit.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><ol><li>对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</li><li>对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</li><li>限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</li></ol><h3 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h3><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  no limit. </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h3 id="推测执行"><a href="#推测执行" class="headerlink" title="推测执行"></a>推测执行</h3><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p><p>设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">               may be executed in parallel.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">               may be executed in parallel.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>不过hive本身也提供了配置项来控制reduce-side的推测执行：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.mapred.reduce.tasks.speculative.execution<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether speculative execution for reducers should be turned on. </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p><h3 id="执行计划（Explain）"><a href="#执行计划（Explain）" class="headerlink" title="执行计划（Explain）"></a>执行计划（Explain）</h3><p>基本语法：</p><p><code>EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</code></p><p>案例：</p><ol><li><p>查看下面这条语句的执行计划</p><p>hive (default)&gt; <code>explain select * from emp;</code></p><p>hive (default)&gt; <code>explain select deptno, avg(sal) avg_sal from emp group by deptno;</code></p></li><li><p>查看详细执行计划</p><p>hive (default)&gt; <code>explain extended select * from emp;</code></p><p>hive (default)&gt; <code>explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</code></p></li></ol><p>以下是在MySQL中的显示：</p><p><img src="https://vinxikk.github.io/img/dw/hive-explain-mysql.png" alt="三种模式的关系"></p><p>EXPLAIN字段：</p><ol><li><p>table: 显示这一行的数据是关于哪张表的</p></li><li><p>possible_keys: 显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句。</p></li><li><p>key: 实际使用的索引。如果为NULL，则没有使用索引。MYSQL很少会选择优化不足的索引，此时可以在SELECT语句中使用USE INDEX（index）来强制使用一个索引或者用IGNORE INDEX（index）来强制忽略索引。</p></li><li><p>key_len: 使用的索引的长度。在不损失精确性的情况下，长度越短越好。</p></li><li><p>ref: 显示索引的哪一列被使用了，如果可能的话，是一个常数</p></li><li><p>rows: MySQL认为必须检索的用来返回请求数据的行数</p></li><li><p>type: 这是最重要的字段之一，显示查询使用了何种类型。从最好到最差的连接类型为system、const、eq_reg、ref、range、index和ALL</p></li><li><p>system/const: 可以将查询的变量转为常量.  如id=1; id为 主键或唯一键</p></li><li><p>eq_ref: 访问索引,返回某单一行的数据.(通常在联接时出现，查询使用的索引为主键或惟一键)</p></li><li><p>range: 这个连接类型使用索引返回一个范围中的行，比如使用&gt;或&lt;查找东西，并且该字段上建有索引时发生的情况(注:不一定好于index)</p></li><li><p>index: 以索引的顺序进行全表扫描，优点是不用排序,缺点是还要全表扫描</p></li><li><p>ALL: 全表扫描，应该尽量避免</p></li><li><p>Extra: 关于MYSQL如何解析查询的额外信息，主要有以下几种</p><ol><li>using index：只用到索引,可以避免访问表. </li><li>using where：使用到where来过虑数据. 不是所有的where clause都要显示using where. 如以=方式访问索引.</li><li>using tmporary：用到临时表</li><li>using filesort：用到额外的排序. (当使用order by v1,而没用到索引时,就会使用额外的排序)</li></ol></li><li><p>range checked for eache record(index map:N): 没有好的索引</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库</title>
      <link href="/2018/04/25/dw/dw-1/"/>
      <url>/2018/04/25/dw/dw-1/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h3><p>数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。</p><p>数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。</p><p>它出于分析性报告和决策支持目的而创建，为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。</p><h3 id="数据库与数据仓库的区别"><a href="#数据库与数据仓库的区别" class="headerlink" title="数据库与数据仓库的区别"></a>数据库与数据仓库的区别</h3><p><strong>数据库：</strong></p><p>是一种逻辑概念，用来存放数据的仓库，通过数据库软件来实现。数据库由很多表组成，表是二维的，一张表里可以有很多字段，对应的数据就一行一行写入表中。数据库的表，在于能够用二维表现多维关系。目前市面上流行的数据库都是二维数据库，如：Oracle、DB2、MySQL、Sybase、MS SQL Server等。</p><p><strong>数据仓库：</strong></p><p>是数据库概念的升级。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现的存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大得多。数据仓库主要用于数据挖掘和数据分析，辅助业务决策。数据仓库的表结构是依照分析需求，分析维度，分析指标进行设计的。</p><p>数据库与数据仓库的区别实际讲的是OLTP与OLAP的区别。</p><p><strong>操作型处理</strong>，叫联机事务处理OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。</p><p><strong>分析型处理</strong>，叫联机分析处理OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。</p><table><thead><tr><th>操作型处理</th><th>分析型处理</th></tr></thead><tbody><tr><td>细节的</td><td>综合的或提炼的</td></tr><tr><td>实体-关系（E-R）模型</td><td>星型模型或雪花模型</td></tr><tr><td>存取瞬间数据</td><td>存储历史数据，不包含最近的数据</td></tr><tr><td>可更新的</td><td>只读、只追加</td></tr><tr><td>一次操作一个单元</td><td>一次操作一个集合</td></tr><tr><td>性能要求高，响应时间短</td><td>性能要求宽松</td></tr><tr><td>面向事务</td><td>面向分析</td></tr><tr><td>一次操作数据量小</td><td>一次操作数据量大</td></tr><tr><td>支持日常操作</td><td>支持决策需求</td></tr><tr><td>数据量小</td><td>数据量大</td></tr><tr><td>客户订单、库存水平和银行账户查询等</td><td>客户收益分析、市场细分等</td></tr></tbody></table><h3 id="数据仓库架构分层"><a href="#数据仓库架构分层" class="headerlink" title="数据仓库架构分层"></a>数据仓库架构分层</h3><h4 id="数据仓库架构"><a href="#数据仓库架构" class="headerlink" title="数据仓库架构"></a>数据仓库架构</h4><p>数据仓库标准上可以分为四层：</p><p>ODS(operation data store)：原始数据层，存放原始数据，不做任何的处理。</p><p>DWD/DWI(data warehouse detail)：结构和粒度与ODS层是一致的，只做对ODS层数据进行清洗(脏数据/空值)。拉链表  脱敏 手机号 银行卡 密码(mysql加密)。</p><p>DWS(data warehouse service)：以DWD层为基础，进行轻度汇总，宽表(join)。</p><p>ADS/APP/domain域表(application data store)：按主题提供统计数据(group计算，少量可能做join计算取决于宽表没有你想要的字段)。</p><ol><li><p>ODS层：</p><p>为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。一般来说ODS层的数据和源系统的数据是同构的，主要目的是简化后续数据加工处理的工作。从数据粒度上来说ODS层的数据粒度是最细的。ODS层的表通常包括两类，一个用于存储当前需要加载的数据，一个用于存储处理完后的历史数据。历史数据一般保存3-6个月后需要清除，以节省空间。但不同的项目要区别对待，如果源系统的数据量不大，可以保留更长的时间，甚至全量保存。</p></li><li><p>DWD层：</p><p>为数据仓库明细层，DWD层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了脏数据/空值等）后的数据。这一层的数据一般是遵循数据库第三范式的，其数据粒度通常和ODS的粒度相同。在DWD层会保存BI系统中所有的历史数据，例如保存10年的数据。</p></li><li><p>DWS层：</p><p>为数据仓库服务层，这层数据是面向主题来组织数据的，通常是星形或雪花结构的数据。从数据粒度来说，这层的数据是轻度汇总级的数据，已经不存在明细数据了。从数据的时间跨度来说，通常是DWD层的一部分，主要的目的是为了满足用户分析的需求，而从分析的角度来说，用户通常只需要分析近几年（如近三年的数据）的即可。从数据的广度来说，任然覆盖了所有的业务数据。</p></li><li><p>ADS层：</p><p>为应用数据存储层，这层数据是完全为了满足具体的分析需求而构建的数据，也是星形或雪花结构的数据。从数据粒度来说是高度汇总的数据。从数据的广度来说，则并不一定会覆盖所有业务数据，而是DWS层数据的一个真子集，从某种意义上来说是DWS层数的一个重复。从极端情况来说，可以为每一张报表在ADS层构建一个模型来支持，达到以空间换时间的目的。</p><p>数据仓库的标准分层只是一个建议性质的标准，实际实施时需要根据实际情况确定数据仓库的分层，不同类型的数据也可能采取不同的分层方法。</p></li></ol><h4 id="为什么要对数据仓库分层"><a href="#为什么要对数据仓库分层" class="headerlink" title="为什么要对数据仓库分层"></a>为什么要对数据仓库分层</h4><ol><li>用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据。</li><li>如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗的过程，工作量巨大。</li><li>通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</li></ol><p>数据仓库的分层不是越多越好，合理的层次设计，以及计算成本和人力成本的平衡，是一个好的数仓架构的表现。</p><h4 id="库表命名原则"><a href="#库表命名原则" class="headerlink" title="库表命名原则"></a>库表命名原则</h4><p>数仓建设的第一步就是要有一个良好的命名规则。</p><p>比如：<code>dwd_whct_xmxx_m</code></p><ol><li>数仓分层：可能取值为ods, dwd, dws, ads等。</li><li>业务领域：可能为whct(文化传统)，whcp(文化产品)等。</li><li>自定义标签：比如项目信息为xmxx，用户可以自定义业务、项目和产品标签。</li><li>时间标签：比如d为天，m为月，y为年，di为增量表，df为全量表。</li></ol><p>ods: 源头数据原封不动的抽取一遍，准备层。</p><p>dw: dwd(数据仓库明细层), dws(数据仓库汇总层)，ods层数据经过etl清洗、转换、加载生成。</p><p>ads: 应用层，各个业务方或部门基于dwd和dws建立的数据集市（DM），原则上ads层数据是基于dw层的，不能直接访问ods层，该层只包含部门或业务方自己关心的dwd或dws。</p><h3 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h3><h4 id="元数据的定义"><a href="#元数据的定义" class="headerlink" title="元数据的定义"></a>元数据的定义</h4><p>数据仓库的元数据是关于数据仓库中数据的数据。它的作用类似于数据库管理系统的数据字典，保存了逻辑数据结构、文件、地址和索引等信息。广义上讲，在数据仓库中，元数据描述了数据仓库内数据的结构和建立方法的数据。</p><p>元数据是数据仓库管理系统的重要组成部分，元数据管理器是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p><ol><li>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</li><li>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</li><li>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</li></ol><p>元数据可分为技术元数据和业务元数据。</p><p>技术元数据为开发和管理数据仓库的IT人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。</p><p>而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。</p><p>由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体，如图所示：</p><p><img src="https://vinxikk.github.io/img/dw/dw-metadata.png" alt="元数据与各个组件的关系"></p><h4 id="元数据的存储方式"><a href="#元数据的存储方式" class="headerlink" title="元数据的存储方式"></a>元数据的存储方式</h4><p>元数据有两种常见存储方式：</p><p>一种是以数据集为基础，每一个数据集有对应的元数据文件，每一个元数据文件包含对应数据集的元数据内容；另一种存储方式是以数据库为基础，即元数据库。其中元数据文件由若干项组成，每一项表示元数据的一个要素，每条记录为数据集的元数据内容。</p><p>上述存储方式各有优缺点，第一种存储方式的优点是调用数据时相应的元数据也作为一个独立的文件被传输，相对数据库有较强的独立性，在对元数据进行检索时可以利用数据库的功能实现，也可以把元数据文件调到其他数据库系统中操作；不足是如果每一数据集都对应一个元数据文档，在规模巨大的数据库中则会有大量的元数据文件，管理不方便。第二种存储方式下，元数据库中只有一个元数据文件，管理比较方便，添加或删除数据集，只要在该文件中添加或删除相应的记录项即可。在获取某数据集的元数据时，因为实际得到的只是关系表格数据的一条记录，所以要求用户系统可以接受这种特定形式的数据。</p><p>因此推荐使用元数据库的方式。</p><p>元数据库用于存储元数据，因此元数据库最好选用主流的关系数据库管理系统。元数据库还包含用于操作和查询元数据的机制。建立元数据库的主要好处是提供统一的数据结构和业务规则，易于把企业内部的多个数据集市有机地集成起来。目前，一些企业倾向建立多个数据集市，而不是一个集中的数据仓库，这时可以考虑在建立数据仓库（或数据集市）之前，先建立一个用于描述数据、服务应用集成的元数据库，做好数据仓库实施的初期支持工作，对后续开发和维护有很大的帮助。元数据库保证了数据仓库数据的一致性和准确性，为企业进行数据质量管理提供基础。</p><h4 id="元数据的作用"><a href="#元数据的作用" class="headerlink" title="元数据的作用"></a>元数据的作用</h4><p>在数据仓库中，元数据的主要作用如下：</p><ol><li>描述哪些数据在数据仓库中，帮助决策分析者对数据仓库的内容定位。</li><li>定义数据进入数据仓库的方式，作为数据汇总、映射和清洗的指南。</li><li>记录业务事件发生而随之进行的数据抽取工作时间安排。</li><li>记录并检测系统数据一致性的要求和执行情况。</li><li>评估数据质量。</li></ol>]]></content>
      
      
      <categories>
          
          <category> DataWarehouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中lateral view与explode函数的使用</title>
      <link href="/2018/04/24/hive/hive-lateral-view-and-explode/"/>
      <url>/2018/04/24/hive/hive-lateral-view-and-explode/</url>
      
        <content type="html"><![CDATA[<h3 id="explode函数"><a href="#explode函数" class="headerlink" title="explode函数"></a>explode函数</h3><p>查看explode的函数说明：</p><p>hive (default)&gt; <code>desc function explode;</code></p><p>官方解释：</p><p><code>explode(a)</code> - separates the elements of array a into multiple rows, or the elements of a map into multiple rows and columns.</p><p><code>explode</code>函数可以将一个array或者map展开。</p><p>其中<code>explode(array)</code>将array中的元素分割为多行，也就是将array列表中的每一个元素作为一行。</p><p><code>explode(map)</code>将map中的元素分割为多个行和列，具体就是将map里的每一对元素作为一行，key为一列，value为一列。</p><h4 id="explode的使用"><a href="#explode的使用" class="headerlink" title="explode的使用"></a>explode的使用</h4><p>数据如下：</p><p>001,allen,Java|Scala|Python,90|80|82<br>002,kobe,Java|SQL|Python,82|95|91</p><p>建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> message(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    courses <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    scores <span class="built_in">array</span>&lt;<span class="built_in">int</span>&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'|'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/message.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> message;</span></pre></td></tr></table></figure><p>查询全表数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> message;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+---------------+----------------------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| message.id  | message.name  |      message.courses       | message.scores  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+---------------+----------------------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 001         | allen         | ["Java","Scala","Python"]  | [90,80,82]      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 002         | kobe          | ["Java","SQL","Python"]    | [82,95,91]      |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------+---------------+----------------------------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows selected (0.127 seconds)</span></pre></td></tr></table></figure><p>查询指定下标的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,courses[<span class="number">1</span>] <span class="keyword">from</span> message;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--------+--------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|  id  |  name  |  _c2   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--------+--------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 001  | allen  | Scala  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 002  | kobe   | SQL    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+--------+--------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">2 rows selected (0.115 seconds)</span></pre></td></tr></table></figure><p>使用explode查询array中的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">explode</span>(courses) <span class="keyword">from</span> message;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|   col   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| Java    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| Scala   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| Python  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| Java    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| SQL     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| Python  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">6 rows selected (0.118 seconds)</span></pre></td></tr></table></figure><p>UDTF不能出现在SELECT子句的外面，下面的SQL语句会出错：</p><p><code>select name,explode(courses) from message;</code></p><p>Error: Error while compiling statement: FAILED: SemanticException [Error 10081]: UDTF’s are not supported outside the SELECT clause, nor nested in expressions (state=42000,code=10081)</p><h4 id="lateral-view"><a href="#lateral-view" class="headerlink" title="lateral view"></a>lateral view</h4><p>lateral view: 侧视图，可以配合UDTF函数来使用，把某一行数据拆分为多行数据。</p><p>不加lateral view的UDTF只能提取单个字段拆分,并不能塞会原来数据表中，加上lateral view就可以将拆分的单个字段数据与原始表数据关联上。</p><p>使用lateral view的时候需要指定视图别名和生成的新列别名：</p><p><code>tableA lateral view UDTF(xxx) 视图别名 as a,b,c</code></p><p>lateral view explode相当于一个拆分courses字段的虚表，然后与原表进行关联。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- subview为视图别名，course为指定新列别名</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> subview.* <span class="keyword">from</span> message <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(courses) subview <span class="keyword">as</span> course;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| subview.course  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| Java            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| Scala           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| Python          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| Java            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| SQL             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| Python          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">6 rows selected (0.098 seconds)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,subview.* <span class="keyword">from</span> message <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(courses) subview <span class="keyword">as</span> course;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|  name  | subview.course  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">| allen  | Java            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">| allen  | Scala           |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">| allen  | Python          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">| kobe   | Java            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">| kobe   | SQL             |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">| kobe   | Python          |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+-----------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">6 rows selected (0.123 seconds)</span></pre></td></tr></table></figure><h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>表tmp两个字段：user, profile</p><p>abc    key1:value1,key2:value2<br>def    key1:value1,key2:value2,key3:value3,key4:value4<br>xyz    key1:value1</p><p>需要转换成下面的表结构：</p><p><img src="https://vinxikk.github.io/img/hive/hive-tmp-result.png" alt="转换后的表"></p><p>总体思路：</p><p>先按照“,”拆分，再按照”:”拆分</p><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>拆分”,”，并保存视图：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> tmp_profile_single <span class="keyword">as</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,profile_single</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tmp a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(a.profile,<span class="string">','</span>)) b <span class="keyword">as</span> profile_single;</span></pre></td></tr></table></figure><p>拆分”:”：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.user <span class="keyword">as</span> <span class="keyword">user</span>,t.arr[<span class="number">0</span>] <span class="keyword">as</span> profile_key,t.arr[<span class="number">1</span>] <span class="keyword">as</span> profile_value</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">split</span>(profile_single, <span class="string">':'</span>) <span class="keyword">as</span> arr <span class="keyword">from</span> tmp_profile_single</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> t;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> tmp_profile_single;</span></pre></td></tr></table></figure><h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">t1.user <span class="keyword">as</span> <span class="keyword">user</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">t1.arr[<span class="number">0</span>] <span class="keyword">as</span> profile_key,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">t1.arr[<span class="number">1</span>] <span class="keyword">as</span> profile_value</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">split</span>(profile_item, <span class="string">':'</span>) <span class="keyword">as</span> arr <span class="keyword">from</span> tmp <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">outer</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(profile, <span class="string">','</span>)) t <span class="keyword">as</span> profile_item</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> t1;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lateral view </tag>
            
            <tag> explode </tag>
            
            <tag> 列转行 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的复杂数据类型&amp;json_tuple的使用</title>
      <link href="/2018/04/24/hive/hive-type-json/"/>
      <url>/2018/04/24/hive/hive-type-json/</url>
      
        <content type="html"><![CDATA[<h3 id="复杂数据类型"><a href="#复杂数据类型" class="headerlink" title="复杂数据类型"></a>复杂数据类型</h3><p>Hive有三种复杂数据类型ARRAY、MAP和STRUCT，复杂数据类型允许任意层次的嵌套。</p><p>表info中有如下一行，我们用JSON格式来表示其数据结构：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"name"</span>:<span class="string">"zhangsan"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"friends"</span>:[<span class="string">"lisi"</span>,<span class="string">"wangwu"</span>], <span class="comment">//Array</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"children"</span>:&#123;                 <span class="comment">//Map</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"alice"</span>:<span class="number">18</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"tom"</span>:<span class="number">19</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"address"</span>:&#123;                 <span class="comment">//Struct</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"street"</span>:<span class="string">"W Jefferson Blvd"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"city"</span>:<span class="string">"Los Angeles"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>基于上述的数据结构，我们在Hive中创建对应的表，并导入数据。</p><p>数据文件info.txt</p><p>zhangsan,lisi_wangwu,alice:18_tom:19,Jefferson Boulevard_Los Angeles<br>jason,curry_kobe,daniel:20_sally:21,McClintock Ave_Los Angeles</p><p>创建表info: </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> info(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">children <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="built_in">int</span>&gt;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;</span></pre></td></tr></table></figure><p>字段解释：</p><ul><li><code>row format delimited fields terminated by &#39;,&#39;</code> - 字段分隔符</li><li><code>collection items terminated by &#39;_&#39;</code> - Array/Map/Struct的分隔符（数据分割符号）</li><li><code>map keys terminated by &#39;:&#39;</code> - Map中的key与value的分隔符</li><li><code>lines terminated by &#39;\n&#39;</code> - 行分隔符</li></ul><p>导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/info.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> info;</span></pre></td></tr></table></figure><p>访问三种集合列里的数据（分别是Array, Map, Struct的访问方式）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> friends[<span class="number">1</span>],children[<span class="string">'sally'</span>],address.city <span class="keyword">from</span> info <span class="keyword">where</span> <span class="keyword">name</span>=<span class="string">'jason'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">|  _c0  | _c1  |     city     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| kobe  | 21   | Los Angeles  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------+------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">1 row selected (0.43 seconds)</span></pre></td></tr></table></figure><h3 id="json-tuple的使用"><a href="#json-tuple的使用" class="headerlink" title="json_tuple的使用"></a>json_tuple的使用</h3><p><code>json_tuple()</code>函数也是UDTF函数，因为一个json字符串对应解析出n个字段，与原表数据关联的时候需要使用<code>lateral view</code>。</p><p>语法格式：</p><p><code>select id from table lateral view json_tuple(property, &#39;tag_id&#39;, &#39;tag_type&#39;);</code></p><p>表sku_info_tbl，表中有两个字段id, sku_info，表中有一条数据如下：</p><p>id: 101</p><p>sku_info: </p><p>[{“skuId”:”1017570”,”num”:”2”,”price”:5.8,”jd_price”:23.9,”sale_price”:5.8},{“skuId”:”1329431”,”num”:”1”,”price”:38.38,”jd_price”:59,”sale_price”:36.84},{“skuId”:”1381473”,”num”:”1”,”price”:8.5,”jd_price”:39.8,”sale_price”:8.5}]</p><p>建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> sku_info_tbl(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sku_info <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sku_info_tbl <span class="keyword">values</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="string">'101'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="string">'[&#123;"skuId":"1017570","num":"2","price":5.8,"jd_price":23.9,"sale_price":5.8&#125;,&#123;"skuId":"1329431","num":"1","price":38.38,"jd_price":59,"sale_price":36.84&#125;,&#123;"skuId":"1381473","num":"1","price":8.5,"jd_price":39.8,"sale_price":8.5&#125;]'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure><p>首先将sku_info_tbl表中的list列表拆成三行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span>,sku_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sku_info_tbl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="keyword">substr</span>(sku_infos,<span class="number">2</span>,<span class="keyword">length</span>(sku_infos)<span class="number">-2</span>),<span class="string">'&#125;,'</span>)) subview <span class="keyword">as</span> sku_info;</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/hive/hive-json-1.png" alt="拆分sku_infos"></p><p>补全每行记录：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">t.id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">substr</span>(t.sku_info,<span class="keyword">length</span>(t.sku_info),<span class="number">1</span>) = <span class="string">'&#125;'</span> <span class="keyword">then</span> t.sku_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">concat</span>(t.sku_info,<span class="string">'&#125;'</span>) <span class="keyword">end</span> <span class="keyword">as</span> sku_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,sku_info <span class="keyword">from</span> sku_info_tbl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="keyword">substr</span>(sku_infos,<span class="number">2</span>,<span class="keyword">length</span>(sku_infos)<span class="number">-2</span>),<span class="string">'&#125;,'</span>)) subview <span class="keyword">as</span> sku_info</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">) t;</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/hive/hive-json-2.png" alt="补全&quot;}&quot;号"></p><p>将每一行的json数据拆分成对应字段：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">m.id,skuid,<span class="keyword">num</span>,price,jd_price,sale_price</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">t.id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> <span class="keyword">substr</span>(t.sku_info,<span class="keyword">length</span>(t.sku_info),<span class="number">1</span>) = <span class="string">'&#125;'</span> <span class="keyword">then</span> t.sku_info</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">concat</span>(t.sku_info,<span class="string">'&#125;'</span>) <span class="keyword">end</span> <span class="keyword">as</span> sku_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,sku_info <span class="keyword">from</span> sku_info_tbl </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="keyword">substr</span>(sku_infos,<span class="number">2</span>,<span class="keyword">length</span>(sku_infos)<span class="number">-2</span>),<span class="string">'&#125;,'</span>)) subview <span class="keyword">as</span> sku_info</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">) t</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">) m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> json_tuple(m.sku_info,<span class="string">'skuId'</span>,<span class="string">'num'</span>,<span class="string">'price'</span>,<span class="string">'jd_price'</span>,<span class="string">'sale_price'</span>) subview </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">as</span> skuid,<span class="keyword">num</span>,price,jd_price,sale_price;</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/hive/hive-json-3.png" alt="拆解完成"></p><p>当然也可以将每一步生成的结果表注册成视图，简化和拆解SQL语句的书写。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Array/Map/Struct </tag>
            
            <tag> json_tuple </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive静态分区与动态分区的业务场景应用</title>
      <link href="/2018/04/23/hive/hive-partition/"/>
      <url>/2018/04/23/hive/hive-partition/</url>
      
        <content type="html"><![CDATA[<h3 id="静态分区的应用"><a href="#静态分区的应用" class="headerlink" title="静态分区的应用"></a>静态分区的应用</h3><p>业务场景：</p><p>统计每天的销售额，日期是确定的，需要把每天统计好的销售额数据插入到指定的日期分区中。</p><p>hive脚本：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive -e "</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hive job任务队列</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.queue.name=pms;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建销售额数据汇总表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> pms.rpt_rcmd_gmv(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">page_name <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">section_name <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">order_count <span class="built_in">bigint</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">order_amount <span class="keyword">double</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span>(ds <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;"</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">date=`date +"%Y-%m-%d"`</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">hive -e "</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hive job任务队列</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.queue.name=pms;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 将数据插入到指定的日期分区中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> pms.rpt_rcmd_gmv <span class="keyword">partition</span>(ds=<span class="string">'$date'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">page_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">section_name,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">order_count,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">order_amount</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xxx;"</span></pre></td></tr></table></figure><h3 id="动态分区的应用"><a href="#动态分区的应用" class="headerlink" title="动态分区的应用"></a>动态分区的应用</h3><p>业务场景：</p><p>一个二级类目对应着多个产品，现在想要按二级类目进行分区，问题在于输入数据中包含不止一个二级类目，所以分区的值是非确定的，需要根据输入数据来确定。</p><p>参数设置：</p><ol><li><code>set hive.exec.dynamic.partition=true;</code>（默认true，表示开启动态分区）</li><li><code>set hive.exec.dynamic.partition.mode=nonstrict;</code>（strict表示至少需要指定一个静态分区，nonstrict表示不需要指定静态分区）</li><li>动态分区的字段，需要写在<code>select</code>语句中所有字段的最后</li></ol><p>hive脚本：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建立hive分区表，以二级类目id作为分区，字段之间以\t分隔，行之间以\n分隔</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> exist pms.dynamic_groupon;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> pms.dynamic_groupon(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    product_id <span class="built_in">bigint</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    area_id <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">type</span> <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    original_price <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    price <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    start_time <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    end_time <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    groupon_num <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    ds <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (category_lvl2_id <span class="built_in">bigint</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 根据查询的结果，动态将数据插入到对应的二级类目分区中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max=<span class="number">32</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">32</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.name=[HQL]dynamic_groupon;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> pms.dynamic_groupon <span class="keyword">partition</span>(category_lvl2_id)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.productid <span class="keyword">as</span> <span class="built_in">bigint</span>) <span class="keyword">as</span> product_id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">  a.areaid <span class="keyword">as</span> area_id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.type <span class="keyword">as</span> <span class="built_in">int</span>) <span class="keyword">as</span> <span class="keyword">type</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.originalprice <span class="keyword">as</span> <span class="keyword">double</span>) <span class="keyword">as</span> original_price,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.price <span class="keyword">as</span> <span class="keyword">double</span>) <span class="keyword">as</span> price,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">  a.start_time,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">  a.end_time,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">cast</span>(a.grouponnum <span class="keyword">as</span> <span class="built_in">int</span>) <span class="keyword">as</span> groupon_num,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">  a.ds,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">  c.categ_lvl2_id <span class="keyword">as</span> category_lvl2_id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pms.pms_all_groupon_items a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> product b <span class="keyword">on</span> (<span class="keyword">cast</span>(a.productid <span class="keyword">as</span> <span class="built_in">bigint</span>) = b.id)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">select</span> <span class="keyword">distinct</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">    categ_lvl_id,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">    categ_lvl2_id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">from</span> dw.hier_categ_by_orig</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">) c <span class="keyword">on</span> (b.category_id = c.categ_lvl_id)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> a.ds=<span class="string">'2018-04-06'</span>;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分区表 </tag>
            
            <tag> 业务场景 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop的常规使用</title>
      <link href="/2018/04/22/dw/sqoop-1/"/>
      <url>/2018/04/22/dw/sqoop-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Sqoop概述"><a href="#Sqoop概述" class="headerlink" title="Sqoop概述"></a>Sqoop概述</h3><p>Apache Sqoop(TM)是一种旨在有效地在Apache Hadoop和诸如关系数据库等结构化数据存储之间传输大量数据的工具。</p><p>Sqoop于2012年3月孵化出来，现在是一个顶级的Apache项目。</p><p>请注意，1.99.7与1.4.6不兼容，且没有特征不完整，它并不打算用于生产部署。</p><p>Sqoop原理：</p><p>将导入或导出命令翻译成MapReduce程序来实现。</p><p>在翻译出的MapReduce中主要是对inputformat和outputformat进行定制。</p><h3 id="Sqoop部署"><a href="#Sqoop部署" class="headerlink" title="Sqoop部署"></a>Sqoop部署</h3><p>下载（这里选择CDH5.16.2）：</p><p><code>wget http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz</code></p><p>解压：</p><p><code>tar -zxvf sqoop-1.4.6-cdh5.16.2.tar.gz -C ~/app/</code></p><p>设置软连接：</p><p><code>ln -s sqoop-1.4.6-cdh5.16.2.tar.gz sqoop</code></p><p>配置环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加以下内容</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SQOOP_HOME=/home/hadoop/app/sqoop-1.4.6-cdh5.16.2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$SQOOP_HOME</span>/bin:<span class="variable">$PATH</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存退出后source，使环境变量生效</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看环境变量是否生效</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">which</span> sqoop</span></pre></td></tr></table></figure><p>修改配置文件：</p><p><code>cp sqoop-env-template.sh sqoop-env.sh</code></p><p><code>vi sqoop-env.sh</code></p><p>修改以下内容：</p><p><code>export HADOOP_COMMON_HOME=/home/hadoop/app/hadoop</code></p><p><code>export HADOOP_MAPRED_HOME=/home/hadoop/app/hadoop</code></p><p><code>export HIVE_HOME=/home/hadoop/app/hive</code></p><p>添加驱动包：</p><p><code>cp mysql-connector-java-5.1.27-bin.jar $SQOOP_HOME/lib/</code></p><p>至此，Sqoop部署完成，可以使用<code>sqoop help</code>来验证配置是否正确。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ sqoop <span class="built_in">help</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Warning信息...</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Available commands:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  codegen            Generate code to interact with database records</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  create-hive-table  Import a table definition into Hive</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="built_in">eval</span>               Evaluate a SQL statement and display the results</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  <span class="built_in">export</span>             Export an HDFS directory to a database table</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="built_in">help</span>               List available commands</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  import             Import a table from a database to HDFS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  import-all-tables  Import tables from a database to HDFS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  import-mainframe   Import datasets from a mainframe server to HDFS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  job                Work with saved <span class="built_in">jobs</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  list-databases     List available databases on a server</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">  list-tables        List available tables <span class="keyword">in</span> a database</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">  merge              Merge results of incremental imports</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  metastore          Run a standalone Sqoop metastore</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">  version            Display version information</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">See <span class="string">'sqoop help COMMAND'</span> <span class="keyword">for</span> information on a specific <span class="built_in">command</span>.</span></pre></td></tr></table></figure><h3 id="查看数据库和表"><a href="#查看数据库和表" class="headerlink" title="查看数据库和表"></a>查看数据库和表</h3><p>查看数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop list-databases \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 出现如下数据库信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">information_schema</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">metadata_hive</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">performance_schema</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">test</span></span></pre></td></tr></table></figure><p>查看test数据库中的所有表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop list-tables \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/<span class="built_in">test</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password ruozedata</span></pre></td></tr></table></figure><h3 id="Sqoop导入数据"><a href="#Sqoop导入数据" class="headerlink" title="Sqoop导入数据"></a>Sqoop导入数据</h3><p>在Sqoop中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做导入，即使用import关键字。</p><h4 id="RDBMS到HDFS"><a href="#RDBMS到HDFS" class="headerlink" title="RDBMS到HDFS"></a>RDBMS到HDFS</h4><p>在MySQL中新建一张表并插入一些数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> company;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> company.staff(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">4</span>) primary <span class="keyword">key</span> <span class="keyword">not</span> <span class="literal">null</span> auto_increment, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    sex <span class="built_in">varchar</span>(<span class="number">255</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Thomas'</span>, <span class="string">'Male'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> company.staff(<span class="keyword">name</span>, sex) <span class="keyword">values</span>(<span class="string">'Catalina'</span>, <span class="string">'FeMale'</span>);</span></pre></td></tr></table></figure><p>全部导入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--table staff \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span></span></pre></td></tr></table></figure><p>查看HDFS上的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -ls /user/company/staff1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Found 2 items</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--   1 vinx supergroup          0 2017-12-27 17:06 /user/company/staff1/_SUCCESS</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--   1 vinx supergroup         32 2017-12-27 17:06 /user/company/staff1/part-m-00000</span></pre></td></tr></table></figure><p>查询导入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff2 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--query <span class="string">'select name,sex from staff where id=2 and $CONDITIONS;'</span></span></pre></td></tr></table></figure><p>导入指定列：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff2 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--columns id,name \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">--table staff</span></pre></td></tr></table></figure><p>使用sqoop关键字筛选查询导入数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/company \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--target-dir /user/company/staff2 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--table staff \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">--<span class="built_in">where</span> <span class="string">"id=2"</span></span></pre></td></tr></table></figure><h4 id="RDBMS到Hive"><a href="#RDBMS到Hive" class="headerlink" title="RDBMS到Hive"></a>RDBMS到Hive</h4><p>过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库。</p><p>从MySQL到Hive，本质是从MySQL =&gt; HDFS =&gt; load to Hive</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/<span class="built_in">test</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--table student \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--hive-import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">--hive-overwrite \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">--hive-table student_hive</span></pre></td></tr></table></figure><h3 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h3><p>在Sqoop中，导出是指：从大数据集群（HDFS/Hive/HBase）向非大数据集群（RDBMS）中传输数据，即使用export关键字。</p><h4 id="HIVE-HDFS到RDBMS"><a href="#HIVE-HDFS到RDBMS" class="headerlink" title="HIVE/HDFS到RDBMS"></a>HIVE/HDFS到RDBMS</h4><p>MySQL中创建表：</p><p><code>create table people(id int,name varchar(5));</code></p><p>导出数据到MySQL：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop <span class="built_in">export</span> \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://bigdata111:3306/<span class="built_in">test</span> characterEncoding=utf-8\</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password 123456 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--<span class="built_in">export</span>-dir /user/hive/warehouse/student_hive \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--table student1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--num-mappers 1 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--input-fields-terminated-by <span class="string">"\t"</span></span></pre></td></tr></table></figure><p>MySQL中如果表不存在，不会自动创建，需要手动先创建相应的表。</p><p>重复往MySQL的同一个表导出数据，MySQL的表不能设置主键和自增。</p><p>数据以追加的方式导出到表中。</p><h3 id="脚本打包"><a href="#脚本打包" class="headerlink" title="脚本打包"></a>脚本打包</h3><p>使用opt格式的文件打包sqoop命令，然后执行。</p><ol><li><p>创建一个.opt文件</p><p><code>touch job_HDFS2MySQL.opt</code></p></li><li><p>编写sqoop脚本</p><p><code>vi ./job_HDFS2MySQL.opt</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从student_hive中追加导入到mysql的student表中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--connect</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">jdbc:mysql://hadoop:3306/<span class="built_in">test</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--username</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">root</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--password</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">123456</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">--table</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">student</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">--num-mappers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">--<span class="built_in">export</span>-dir</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">/user/hive/warehouse111/student_hive</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">--input-fields-terminated-by</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="string">"\t"</span></span></pre></td></tr></table></figure></li><li><p>执行该脚本</p><p><code>sqoop --options-file job_HDFS2MySQL.opt</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HiveServer2和Beeline&amp;JOIN&amp;GROUP BY&amp;排序</title>
      <link href="/2018/04/21/dw/hive-3/"/>
      <url>/2018/04/21/dw/hive-3/</url>
      
        <content type="html"><![CDATA[<h3 id="HiveServer2和Beeline"><a href="#HiveServer2和Beeline" class="headerlink" title="HiveServer2和Beeline"></a>HiveServer2和Beeline</h3><p>当我们启动使用<code>hive</code>命令启动Hive时，会出现下面的信息：</p><p>WARNING: Hive CLI is deprecated and migration to Beeline is recommended.</p><p>警告中说：Hive CLI已经被标记为是过时的，并且推荐使用Beeline。</p><p>HS2和beeline的角色：</p><ul><li><p>HiveServer2: Server 默认端口是10000，可以指定端口</p></li><li><p>Beeline: Client</p></li></ul><h4 id="默认端口启动"><a href="#默认端口启动" class="headerlink" title="默认端口启动"></a>默认端口启动</h4><p>提示：需要先配置HIVE_HOME环境变量。</p><p>先启动HS2: </p><p><code>hiveserver2</code></p><p>然后启动Beeline: </p><p><code>beeline -u jdbc:hive2://hadoop001:10000/ -n vinx</code></p><p>退出Beeline: </p><p><code>!q</code></p><h4 id="指定端口启动"><a href="#指定端口启动" class="headerlink" title="指定端口启动"></a>指定端口启动</h4><p>启动HS2: </p><p><code>hiveserver2 --hiveconf hive.server2.thrift.port=10086</code></p><p>启动beeline: </p><p><code>beeline -u jdbc:hive2://hadoop001:10086/ -n vinx</code></p><h3 id="JOIN语句"><a href="#JOIN语句" class="headerlink" title="JOIN语句"></a>JOIN语句</h3><h4 id="常用JOIN"><a href="#常用JOIN" class="headerlink" title="常用JOIN"></a>常用JOIN</h4><p>内连接：只要进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename,d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dept = d.deptno;</span></pre></td></tr></table></figure><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span></pre></td></tr></table></figure><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span></pre></td></tr></table></figure><p>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字符没有符合条件的值的话，那么就使用NULL值替代。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno;</span></pre></td></tr></table></figure><h4 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h4><p>连接n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p><p>创建表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> location(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    loc <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    loc_name <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/location.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> location;</span></pre></td></tr></table></figure><p>多表连接查询：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,d.deptno,l.loc_name</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> dept d <span class="keyword">on</span> d.deptno=e.deptno</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> location l <span class="keyword">on</span> d.loc=l.loc;</span></pre></td></tr></table></figure><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l进行连接操作。Hive总是按照从左到右的顺序执行连接操作的。</p><h4 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h4><p>笛卡尔积会在下面的条件下产生：</p><ol><li>省略了连接条件</li><li>连接条件无效</li><li>所有表中的所有行互相连接</li></ol><p>例如：</p><p>hive (default)&gt; <code>select empno, deptno from emp, dept;</code></p><p>FAILED: SemanticException Column deptno Found in more than One Tables/Subqueries</p><h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><h4 id="GROUP-BY语句"><a href="#GROUP-BY语句" class="headerlink" title="GROUP BY语句"></a>GROUP BY语句</h4><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个队列结果进行分组，然后对每个组执行聚合操作。</p><p>计算emp表每个部门的平均工资：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.deptno,<span class="keyword">avg</span>(e.sal) avg_sal <span class="keyword">from</span> emp e <span class="keyword">group</span> <span class="keyword">by</span> e.deptno;</span></pre></td></tr></table></figure><p>计算emp每个部门中每个岗位的最高薪水：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.deptno,e.job,<span class="keyword">max</span>(e.sal) max_sal <span class="keyword">from</span> emp e <span class="keyword">group</span> <span class="keyword">by</span> e.deptno,e.job;</span></pre></td></tr></table></figure><h4 id="HAVING语句"><a href="#HAVING语句" class="headerlink" title="HAVING语句"></a>HAVING语句</h4><p>having与where的不同点：</p><ol><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据</li><li>where后面不能写分组函数，而having后面可以使用分组函数</li><li>having只用于group by分组统计语句</li></ol><p>求每个部门的平均工资大于2000的部门：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno,<span class="keyword">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal &gt; <span class="number">2000</span>;</span></pre></td></tr></table></figure><h4 id="TopN实现"><a href="#TopN实现" class="headerlink" title="TopN实现"></a>TopN实现</h4><p>需求：求每个部门每个职业的薪水和最高的2个职位。</p><p>创建视图sal，保存每个部门每个职业的薪水和：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> sal <span class="keyword">as</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno,job,<span class="keyword">sum</span>(sal) sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno,job;</span></pre></td></tr></table></figure><p>求出每个职业的top2：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  a.*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sal a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> sal b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">where</span> a.deptno=b.deptno <span class="keyword">and</span> a.sal &lt; b.sal</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">) = <span class="number">1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 求top1可以将1改为0</span></span></pre></td></tr></table></figure><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><h4 id="ORDER-BY全局排序"><a href="#ORDER-BY全局排序" class="headerlink" title="ORDER BY全局排序"></a>ORDER BY全局排序</h4><p>ORDER BY: 全局排序，一个MapReduce，用在SELECT语句的结尾。</p><p>查询员工信息，并按工资升序排序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal;</span></pre></td></tr></table></figure><p>查询员工信息，并按工资降序排序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><p>按照部门和工资升序排序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ename,deptno,sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno,sal;</span></pre></td></tr></table></figure><h4 id="SORT-BY内部排序"><a href="#SORT-BY内部排序" class="headerlink" title="SORT BY内部排序"></a>SORT BY内部排序</h4><p>SORT BY: 每个MapReduce内部进行排序，分区规则按照key的hash来运算，（区内排序）对全局结果集来说不是排序。</p><p>设置reduce个数：</p><p><code>set mapreduce.job.reduces=3;</code></p><p>查看设置reduce个数：</p><p><code>set mapreduce.job.reduces;</code> </p><p>查看员工信息，并根据部门编号降序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><p>将查询结果导入到文件中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/vinx/data/output/emp.txt'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><h4 id="DISTRIBUTE-BY分区排序"><a href="#DISTRIBUTE-BY分区排序" class="headerlink" title="DISTRIBUTE BY分区排序"></a>DISTRIBUTE BY分区排序</h4><p>DISTRIBUTE BY: 类似MR中partition，进行分区，结合sort by使用。DISTRIBUTE BY语句要写在SORT BY语句之前。</p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>设置reduce个数：</p><p><code>set mapreduce.job.reduces=3;</code></p><p>先按照部门编号分区，再按照员工编号降序排序：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/vinx/data/distribute-result'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><h4 id="CLUSTER-BY"><a href="#CLUSTER-BY" class="headerlink" title="CLUSTER BY"></a>CLUSTER BY</h4><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。</p><p>以下两种写法等价：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/home/vinx/data/cluster-result'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span></pre></td></tr></table></figure><p>按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HiveServer2/Beeline </tag>
            
            <tag> JOIN </tag>
            
            <tag> GROUP BY </tag>
            
            <tag> 排序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的UDF/UDAF/UDTF</title>
      <link href="/2018/04/20/hive/hive-function/"/>
      <url>/2018/04/20/hive/hive-function/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive的内置函数"><a href="#Hive的内置函数" class="headerlink" title="Hive的内置函数"></a>Hive的内置函数</h3><p>内置函数：</p><ul><li><code>show functions;</code> - 查看内置函数</li><li><code>desc function upper;</code> - 查看upper的用法</li><li><code>desc function extended upper;</code> - 查看upper的详细用法（带示例）</li></ul><p>常用内置函数：</p><ul><li><p>日期函数</p><p>date_add, datediff, to_date, from_unixtime, unix_timestamp</p></li><li><p>字符串函数</p><p>substr, concat, concat_ws, split, regexp_replace, get_json_object, trim, length</p></li><li><p>聚合函数</p><p>abs, ceil, floor ,round, rand, pow</p></li><li><p>数学函数</p><p>count, max, min, avg, count distinct, sum, group_concat</p></li><li><p>窗口函数</p><p>row_number, lead, lag, rank</p></li><li><p>其他函数</p><p>coalesce, cast, decode explode</p></li></ul><h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><p>用户自定义函数分为以下3种：</p><ol><li><p>UDF(User-Defined-Function)</p><p>一进一出</p></li><li><p>UDAF(User-Defined Aggregation Function)</p><p>聚合函数，多进一出</p><p>类似于：count/max/min</p></li><li><p>UDTF(User-Defined Table-Generating Funcion)</p><p>一进多出</p><p>如：lateral view explode()</p></li></ol><p>编程步骤：</p><ol><li><p>继承<code>org.apache.hadoop.hive.sql.UDF</code></p></li><li><p>实现<code>evaluate</code>函数，<code>evaluate</code>函数支持重载</p></li><li><p>在hive的命令行窗口创建函数</p><ol><li><p>添加jar</p><p><code>add jar linux_jar_path</code></p></li><li><p>创建function</p><p><code>create [temporary] function [dbname.]function_name AS class_name;</code></p></li></ol></li><li><p>在hive的命令行窗口删除函数</p><p><code>drop [temporary] function [if exists] [dbname.]function_name;</code></p></li></ol><p>UDF必须要有返回类型，可以返回null，但是返回类型不能为null。</p><h4 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h4><p>User-Defined-Function用户自定义函数，一进一出。</p><p>in:out=1:1，输入一条记录数据，同时返回一条处理结果。</p><p>类似于cos, sin, substring, indexof…</p><p>实现步骤（Java创建自定义UDF类）：</p><ul><li>自定义一个java类</li><li>继承UDF类</li><li>重写evaluate方法</li><li>打包类所在项目成一个all-in-one的jar包，并上传到hive所在机器</li><li>在hive中执行add jar操作，将jar加载到classpath中</li><li>在hive中创建模板函数，使得后边可以使用该函数名称调用实际的UDF函数</li><li>hive sql中像调用系统函数一样使用UDF函数</li></ul><p>代码实现：</p><ul><li>功能要求：实现当输入字符串超过2个字符的时候，多余的字符以”…”来表示。</li><li>如”12”则返回”12”，如”123”返回”12…”</li><li>自定义类、继承UDF、重写evaluate方法已在代码中体现</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 功能：实现当输入字符串超过2个字符的时候，多余的字符以"..."来表示。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> * 输入/输出：* 如“12”则返回“12”，如“123”返回“12..."</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"> */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ValueMaskUDF</span> <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">       <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String input,<span class="keyword">int</span> maxSaveStringLength,String replaceSign)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">             <span class="keyword">if</span>(input.length()&lt;=maxSaveStringLength)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">                    <span class="keyword">return</span> input;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">             &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">             <span class="keyword">return</span> input.substring(<span class="number">0</span>,maxSaveStringLength)+replaceSign;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">       &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">             System.out.println(<span class="keyword">new</span> ValueMaskUDF().evaluate(<span class="string">"河北省"</span>,<span class="number">2</span>,<span class="string">"..."</span>));;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">       &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h4 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h4><p>in:out=n:1，即接受N条记录数据，同时返回一条处理结果</p><p>类似于：count, sum, avg, max</p><p>实现步骤：</p><ul><li>自定义一个java类</li><li>继承UDAF类</li><li>内部定义一个静态类，实现UDAFEvaluator接口</li><li>实现方法init, iterate, terminatePartial, merge, terminate共5个方法</li><li>在hive中执行add jar操作，将jar加载到classpath中</li><li>在hive中创建模板函数，使得后边可以使用该函数名称调用实际的UDF函数</li><li>hive sql中像调用系统函数一样使用UDAF函数</li></ul><p><img src="https://vinxikk.github.io/img/hive/hive-udaf.png" alt="UDAF的实现步骤"></p><p>建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_score(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    course <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    score <span class="built_in">int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/hive/student_score.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> student_score;</span></pre></td></tr></table></figure><p>查询数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive (hive)&gt; select * from student_score;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">student_score.idstudent_score.namestudent_score.coursestudent_score.score</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">001zhangsanJava90</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">001zhangsanPython80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">001zhangsanScala85</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">002lisiJava95</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">002lisiPython90</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">002lisiScala93</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">003wangwuJava90</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">003wangwuPython80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">003wangwuScala70</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">003wangwuEnglish80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">004aliceScala80</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">Time taken: 2.482 seconds, Fetched: 11 row(s)</span></pre></td></tr></table></figure><p>UDAF代码实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Map;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Set;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDAF;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDAFEvaluator;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">* 实现多条数据合并成一条数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">*/</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 主类继承UDAF</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudentScoreAggUDAF</span> <span class="keyword">extends</span> <span class="title">UDAF</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// 日志对象初始化</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Logger logger = Logger.getLogger(StudentScoreAggUDAF<span class="class">.<span class="keyword">class</span>)</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// 静态类实现UDAFEvaluator</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Evaluator</span> <span class="keyword">implements</span> <span class="title">UDAFEvaluator</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">// 设置成员变量，存储每个统计范围内的总记录数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">private</span> Map&lt;String, String&gt; courseScoreMap;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//初始化函数,map和reduce均会执行该函数,起到初始化所需要的变量的作用</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Evaluator</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">            init();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">// 初始化函数间传递的中间变量</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">            courseScoreMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">         <span class="comment">//map阶段，返回值为boolean类型，当为true则程序继续执行，当为false则程序退出  </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">iterate</span><span class="params">(String course, String score)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> (course == <span class="keyword">null</span> || score == <span class="keyword">null</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">            courseScoreMap.put(course, score);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">         <span class="comment">/**</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"><span class="comment">         * 类似于combiner,在map范围内做部分聚合，将结果传给merge函数中的形参mapOutput  </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"><span class="comment">         * 如果需要聚合，则对iterator返回的结果处理，否则直接返回iterator的结果即可</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="comment">         */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> Map&lt;String, String&gt; <span class="title">terminatePartial</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> courseScoreMap;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">         <span class="comment">// reduce 阶段，用于逐个迭代处理map当中每个不同key对应的 terminatePartial的结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">merge</span><span class="params">(Map&lt;String, String&gt; mapOutput)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">this</span>.courseScoreMap.putAll(mapOutput);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">// 处理merge计算完成后的结果，即对merge完成后的结果做最后的业务处理</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">terminate</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">return</span> courseScoreMap.toString();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><p>添加jar包到Hive的classpath：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">add jar /home/vinx/jars/hiveudaf.jar;</span></pre></td></tr></table></figure><p>注册临时函数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">function</span> score_agg <span class="keyword">as</span> <span class="string">'com.ivinx.hiveudaf.StudentScoreAggUDAF'</span>;</span></pre></td></tr></table></figure><p>或者永久添加函数：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 永久注册，hive-site.xml --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.aux.jars.path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/vinx/jars/hive.jar<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>测试sql语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,score_agg(course,score) <span class="keyword">from</span> student_score <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>,<span class="keyword">name</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------------------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">|  id  |   name    |                     _c2                     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------------------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 001  | zhangsan  | &#123;Java=90, Scala=85, Python=80&#125;              |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 002  | lisi      | &#123;Java=95, Scala=93, Python=90&#125;              |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 003  | wangwu    | &#123;Java=90, English=80, Scala=70, Python=80&#125;  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 004  | alice     | &#123;Scala=80&#125;                                  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------------------------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4 rows selected (69.974 seconds)</span></pre></td></tr></table></figure><h3 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h3><p>User-Defined Table-Generating Functions</p><p>一行输入，多行输出</p><p>用udtf解决一行输入多行输出的不多，往往被lateral view explode+udf等替代实现，比直接用udtf会更简单、直接一些</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive-UDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的调优</title>
      <link href="/2018/04/18/hive/hive-optimize/"/>
      <url>/2018/04/18/hive/hive-optimize/</url>
      
        <content type="html"><![CDATA[<h3 id="Fetch抓取"><a href="#Fetch抓取" class="headerlink" title="Fetch抓取"></a>Fetch抓取</h3><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：<code>select * from emp</code>，Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果。</p><p>在hive-default.xml.template文件中<code>hive.fetch.task.conversion</code>默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p><ol><li><p>把<code>hive.fetch.task.conversion</code>设置成none，然后执行查询语句，都会执行mapreduce程序。</p><p>hive (default)&gt; <code>set hive.fetch.task.conversion=none;</code></p><p>hive (default)&gt; <code>select * from emp;</code></p><p>hive (default)&gt; <code>select ename from emp;</code></p><p>hive (default)&gt; <code>select ename from emp limit 3;</code></p></li><li><p>把<code>hive.fetch.task.conversion</code>设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p><p>hive (default)&gt; <code>set hive.fetch.task.conversion=more;</code></p><p>hive (default)&gt; <code>select * from emp;</code></p><p>hive (default)&gt; <code>select ename from emp;</code></p><p>hive (default)&gt; <code>select ename from emp limit 3;</code></p></li></ol><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>大多数分Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多得多。对于大数据这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p><p>用户可以通过设置<code>hive.exec.mode.local.auto</code>的值为true，来让Hive在适当的时候自动启动这个优化。</p><p>开启本地mr：</p><p><code>set hive.exec.mode.local.auto=true;</code></p><p>设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128M：</p><p><code>set hive.exec.mode.local.auto.inputbytes.max=50000000;</code></p><p>设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4：</p><p><code>set hive.exec.mode.local.auto.input.files.max=10;</code></p><p>案例：</p><ol><li><p>开启本地模式，并执行查询语句(注意重启Hive)</p><p>hive (default)&gt; <code>set hive.exec.mode.local.auto=true;</code><br>hive (default)&gt; <code>select * from emp cluster by deptno;</code><br>Time taken: 1.328 seconds, Fetched: 14 row(s)</p></li><li><p>关闭本地模式，并执行查询语句</p><p>hive (default)&gt; <code>set hive.exec.mode.local.auto=false;</code><br>hive (default)&gt; <code>select * from emp cluster by deptno;</code><br>Time taken: 20.09 seconds, Fetched: 14 row(s)</p></li></ol><h3 id="表的优化"><a href="#表的优化" class="headerlink" title="表的优化"></a>表的优化</h3><h4 id="小表、大表JOIN"><a href="#小表、大表JOIN" class="headerlink" title="小表、大表JOIN"></a>小表、大表JOIN</h4><p>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用group变小的维度表（1000条以下的记录条数）先进内存，在map端完成reduce（预聚合）。</p><p>新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</p><h4 id="大表JOIN大表"><a href="#大表JOIN大表" class="headerlink" title="大表JOIN大表"></a>大表JOIN大表</h4><ol><li><p>空key过滤</p><p>有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。很多情况下，这些key对应的数据是异常数据，需要在SQL语句中进行过滤，例如key对应的字段为空。</p><p>过滤空id：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> nullidtable <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">) n</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> oritable o <span class="keyword">on</span> n.id=o.id;</span></pre></td></tr></table></figure></li><li><p>空key转换</p><p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以给表中key为空的字段附一个随机的值，使得数据随机均匀地分布到不同的reducer上。</p><p>设置5个reduce个数：</p><p><code>set mapreduce.job.reduces=5;</code></p><p>JOIN两张表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> oritable o</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> n.id=o.id;</span></pre></td></tr></table></figure><p>可以看出来，出现了数据倾斜，某些reducer的资源消耗远大于其他reducer。</p><p>随机分布空null值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">full</span> <span class="keyword">join</span> oritable o</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> <span class="keyword">case</span> <span class="keyword">when</span> n.id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">'hive'</span>,<span class="keyword">rand</span>()) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> n.id <span class="keyword">end</span> = o.id;</span></pre></td></tr></table></figure><p>随机分布null值可以一定程度消除数据倾斜，负载均衡reducer的资源消耗。</p></li></ol><h4 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h4><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join，容易发生数据倾斜。可以用MapJoin把小表全部加载进内存，在map端进行join，避免reducer处理。</p><p>开启MapJoin参数设置：</p><ol><li><p>设置自动选择MapJoin（默认为true）：</p><p><code>set hive.auto.convert.join=true;</code></p></li><li><p>大表小表的阈值设置（默认25M以下认为是小表）：</p><p><code>set hive.mapjoin.smalltable.filesize=25000000;</code></p><p>小表JOIN大表：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> smalltable s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> bigtable  b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> s.id = b.id;</span></pre></td></tr></table></figure><p>大表JOIN小表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bigtable  b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> smalltable  s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> s.id = b.id;</span></pre></td></tr></table></figure><h4 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h4><p>默认情况下，Map阶段同一个key的数据分发给一个reduce，当一个key数据过大时就会发生倾斜。</p><p>并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p><p>开启Map端聚合参数设置：</p><ol><li><p>是否在Map端进行聚合（默认为true）：</p><p><code>set hive.map.aggr=true</code></p></li><li><p>在Map端进行聚合操作的数目：</p><p><code>set hive.groupby.mapaggr.checkinterval=100000</code></p></li><li><p>有数据倾斜的时候进行负载均衡（默认是false）：</p><p><code>set hive.groupby.skewindata=true</code></p></li></ol><p>当选项设定为true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><h4 id="Count-Distinct-去重统计"><a href="#Count-Distinct-去重统计" class="headerlink" title="Count(Distinct)去重统计"></a>Count(Distinct)去重统计</h4><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换。</p><p>创建一张大表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/bigtable'</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span></pre></td></tr></table></figure><p>设置5个reduce个数：</p><p><code>set mapreduce.job.reduces=5;</code></p><p>执行去重id查询（此时只有一个Stage）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> <span class="keyword">id</span>) <span class="keyword">from</span> bigtable;</span></pre></td></tr></table></figure><p>采用group by去重id（有两个Stage，其中第一个Stage会启动5个Reduce）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(a.id) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) a;</span></pre></td></tr></table></figure><h4 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h4><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p><h4 id="行列过滤"><a href="#行列过滤" class="headerlink" title="行列过滤"></a>行列过滤</h4><p>列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用<code>SELECT *</code>。</p><p>行处理：在分区裁剪中，当使用外关联时，如果将副表的过滤条件写在where后面，那么就会先全表关联，之后再过滤，总而言之，就是先where还是先join的执行顺序的问题。以下两种，经过SQL优化器，执行效果大体一样：</p><ol><li><p>先关联两张表，再用where条件过滤：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> oritable o</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> o.id = b.id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> o.id &lt;=<span class="number">10</span>;</span></pre></td></tr></table></figure></li><li><p>通过子查询后，再关联表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> oritable <span class="keyword">where</span> <span class="keyword">id</span>&lt;= <span class="number">10</span>) o</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> b.id=o.id;</span></pre></td></tr></table></figure></li></ol><h4 id="动态分区调整"><a href="#动态分区调整" class="headerlink" title="动态分区调整"></a>动态分区调整</h4><p>关系型数据库中，对分区表INSERT数据时，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区（Dynamic Partition）。</p><p>开启动态分区参数设置：</p><ol><li><p>开启动态分区功能（默认true）：</p><p><code>set hive.exec.dynamic.partition=true</code></p></li><li><p>设置为非严格模式（默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区）：</p><p><code>set hive.exec.dynamic.partition.mode=nonstrict</code></p></li><li><p>在所有执行MR的节点上，最大一共可以创建多少个动态分区（默认1000）：</p><p><code>set hive.exec.max.dynamic.partitions=1000</code></p></li><li><p>在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p><p><code>set hive.exec.max.dynamic.partitions.pernode=100</code></p></li><li><p>整个MR Job中，最大可以创建多少个HDFS文件（默认值100000）：</p><p><code>set hive.exec.max.created.files=100000</code></p></li><li><p>当有空分区生成时，是否抛出异常（默认false，一般不需要设置）：</p><p><code>set hive.error.on.empty.partition=false</code></p></li></ol><p>案例需求：将ori中的数据按照时间(如：20111230000008)，插入到目标表ori_partitioned_target的相应分区中。</p><ol><li><p>创建分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partitioned(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (p_time <span class="built_in">bigint</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure></li><li><p>加载数据到分区表中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/datas/ds1'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">into</span> <span class="keyword">table</span> ori_partitioned </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">partition</span>(p_time=<span class="string">'20111230000010'</span>) ;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/datas/ds2'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">into</span> <span class="keyword">table</span> ori_partitioned </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">partition</span>(p_time=<span class="string">'20111230000011'</span>) ;</span></pre></td></tr></table></figure></li><li><p>创建目标分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partitioned_target(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (p_time <span class="keyword">string</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure></li><li><p>设置动态分区</p><p><code>set hive.exec.dynamic.partition = true;</code><br><code>set hive.exec.dynamic.partition.mode = nonstrict;</code><br><code>set hive.exec.max.dynamic.partitions = 1000;</code><br><code>set hive.exec.max.dynamic.partitions.pernode = 100;</code><br><code>set hive.exec.max.created.files = 100000;</code><br><code>set hive.error.on.empty.partition = false;</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> ori_partitioned_target </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">partition</span> (p_time) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span>, <span class="built_in">time</span>, uid, keyword, url_rank, click_num, click_url, p_time </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ori_partitioned;</span></pre></td></tr></table></figure></li><li><p>查看目标分区表的分区情况</p><p><code>show partitions ori_partitioned_target;</code></p></li><li><p>如果不设置非严格模式，报错如下：</p><p>FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict</p></li></ol><h4 id="sort-by代替order-by"><a href="#sort-by代替order-by" class="headerlink" title="sort by代替order by"></a>sort by代替order by</h4><p>order by就是将结果按某字段全局排序，这会导致所有map端数据都进入一个reducer中，数据量大时导致性能下降。</p><p>sort by会视情况启动多个reducer排序，并且保证每个reducer内局部有序。为了控制map端数据分配到reducer的key，往往还要配合distribute by一同使用。如果不加distribute by的话，map端数据就会随机分配到reducer。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 以uid为key，以upload_time倒序、event_type倒序 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> uid,upload_time,event_type,recore_data </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> recore_log </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> rl_date &gt;= <span class="number">20190405</span> <span class="keyword">and</span> rl_date &lt;= <span class="number">20190410</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">distribute</span> <span class="keyword">by</span> uid </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">sort</span> <span class="keyword">by</span> upload_time <span class="keyword">desc</span>,event_type <span class="keyword">desc</span>;</span></pre></td></tr></table></figure><h4 id="group-by代替distinct"><a href="#group-by代替distinct" class="headerlink" title="group by代替distinct"></a>group by代替distinct</h4><p>当要统计某一列的去重数时，如果数据量很大，count(distinct)就会非常慢，原因与order by类似，count(distinct)只会有很少的reducer来处理。这时可以用group by来改写：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">select</span> uid <span class="keyword">from</span> recore_log </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">where</span> rl_date &gt;= <span class="number">20180405</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> uid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">) t;</span></pre></td></tr></table></figure><p>但是这样写会启动两个MR job（单纯distinct只会启动一个），所以要确保数据量大到启动job的overhead远小于计算耗时，才考虑这种方法。当数据集很小或者key的倾斜比较明显时，group by可能比distinct还慢。</p><p>使用group by同时统计多列：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t.a,<span class="keyword">sum</span>(t.b),<span class="keyword">count</span>(t.c),<span class="keyword">count</span>(t.d) <span class="keyword">from</span> (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> <span class="keyword">select</span> a,b,<span class="literal">null</span> c,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">) t;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive窗口函数</title>
      <link href="/2018/04/17/hive/hive-window-function/"/>
      <url>/2018/04/17/hive/hive-window-function/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive窗口函数"><a href="#Hive窗口函数" class="headerlink" title="Hive窗口函数"></a>Hive窗口函数</h3><p>窗口函数可以计算累积和、移动平均值等，可以结合聚合函数sum(), avg()等使用，也可以结合first_value()和last_value()，返回窗口的第一个和最后一个值。</p><p>如果只使用partition by子句，未指定order by的话，聚合是分组内的聚合。</p><p>使用了order by子句，未使用window子句的情况下，默认从起点到当前行。</p><p>window子句：</p><ul><li>preceding: 往前</li><li>following: 往后</li><li>current row: 当前行</li><li>unbounded: 界限，UNBOUNDED PRECEDING表示往前无界，UNBOUNDED FOLLOWING表示往后无界</li></ul><h4 id="计算累积和"><a href="#计算累积和" class="headerlink" title="计算累积和"></a>计算累积和</h4><ol><li><p>计算所有月份的累积和：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法1：默认就是UNBOUNDED PRECEDING到CURRENT ROW，可以不写</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li><li><p>计算前3个月和本月共4个月的累积和：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法1：从前3条记录到本条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法2：默认下界就是CURRENT ROW，可以不写</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="number">3</span> <span class="keyword">PRECEDING</span>) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li><li><p>计算前1月后1月和本月，共3个月的累积和：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,<span class="keyword">sum</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span>) cumulative_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li></ol><h4 id="计算平均值"><a href="#计算平均值" class="headerlink" title="计算平均值"></a>计算平均值</h4><ol><li><p>计算前1月后1月和本月，共3个月总值的平均值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_month,<span class="keyword">avg</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span>) average_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li><li><p>计算前3个月和本月，共4个月总值的平均值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_month,<span class="keyword">avg</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">3</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) average_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li></ol><h4 id="计算首尾值"><a href="#计算首尾值" class="headerlink" title="计算首尾值"></a>计算首尾值</h4><ol><li><p>计算窗体第一条和最后一条的值：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> pt_month,<span class="keyword">sum</span>(amount) pay_amount,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">first_value</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span>) first_amount,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">last_value</span>(<span class="keyword">sum</span>(amount)) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> pt_month <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span>) last_amount </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> data_sell_pay_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> pt_month <span class="keyword">between</span> <span class="string">'2017-01'</span> <span class="keyword">and</span> <span class="string">'2017-11'</span> <span class="keyword">and</span> state = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> pt_month;</span></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive窗口函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的管理表和外部表&amp;分区表&amp;分桶表&amp;行转列和列转行</title>
      <link href="/2018/04/15/dw/hive-2/"/>
      <url>/2018/04/15/dw/hive-2/</url>
      
        <content type="html"><![CDATA[<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><p>建表语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[<span class="keyword">COMMENT</span> table_comment] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">[LOCATION hdfs_path]</span></pre></td></tr></table></figure><p>字段说明：</p><ol><li><p>CREATE TABLE创建一个指定名字的表，如果表已经存在，则抛出异常；用户可以用IF NOT EXISTS选项来忽略这个异常。</p></li><li><p>EXTERNAL关键字可以创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）。Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p></li><li><p>COMMENT: 为表和列添加注释。</p></li><li><p>PARTITIONED BY: 创建分区表</p></li><li><p>CLUSTERED BY: 创建分桶表</p></li><li><p>ROW FORMAT</p><p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p><p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe，如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。建表时用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p></li><li><p>STORED AS: 指定存储文件类型</p><p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）。</p><p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p></li><li><p>LOCATION: 指定表在HDFS上的存储位置。</p></li><li><p>LIKE允许用户复制现有的表结构，但是不复制数据。</p></li></ol><h3 id="内部表和外部表"><a href="#内部表和外部表" class="headerlink" title="内部表和外部表"></a>内部表和外部表</h3><p>内部表（管理表）：删除表时，元数据（META）删除，业务数据（HDFS）也删除。</p><p>外部表：删除表时，元数据删除，HDFS中业务数据不删除。</p><p>管理表和外部表的使用场景：</p><p>每天将收集到的网站日志定期流入HDFS文本文件，在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p><h4 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h4><p>默认创建的表都是所谓的管理表，也被称为内部表，因为这种表，Hive会（或多或少）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在<code>hive.metastore.warehouse.dir</code>(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive也会删除这个表中的数据。管理表不适合和其他工具共享数据。</p><p>创建管理表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">empno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">ename <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">job <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">mgr <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">hiredate <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">sal <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">comm <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">deptno <span class="built_in">int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 从Linux本地加载</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 从HDFS加载</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/data/emp.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span></pre></td></tr></table></figure><p>查看表结构：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 简单查看</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">desc emp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 格式化查看详细信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">desc formatted emp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Table Type:             MANAGED_TABLE</span></pre></td></tr></table></figure><h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4><p>因为是外部表，所以Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</p><p>创建外部表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建外部表，并指定在HDFS中存储的路径</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp_external(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">empno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ename <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">job <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">mgr <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">hiredate <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">sal <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">comm <span class="keyword">double</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">deptno <span class="built_in">int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">location <span class="string">'/hive_data/hive_external_table/emp/'</span>;</span></pre></td></tr></table></figure><p>使用HDFS命令加载表数据：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hdfs dfs -put emp.txt /hive_data/hive_external_table/emp/</span></pre></td></tr></table></figure><p>查看创建的表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show tables;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">tab_name</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">emp</span></pre></td></tr></table></figure><p>查询表数据：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from emp;</span></pre></td></tr></table></figure><p>查看格式化表结构：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Table Type:             EXTERNAL_TABLE</span></pre></td></tr></table></figure><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>引入分区表（需要根据日期对日志进行管理）：</p><p><code>/user/hive/warehouse/log_partition/20180402/20180402.log</code></p><p><code>/user/hive/warehouse/log_partition/20180403/20180403.log</code></p><p><code>/user/hive/warehouse/log_partition/20180404/20180404.log</code></p><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>创建分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_partition(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">lid <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">lrecord <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ltime <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据到分区表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/logs/log.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> log_partition <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201804'</span>);</span></pre></td></tr></table></figure><p>查询分区表中的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 单分区查询</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201804'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 多分区查询</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201803'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201804'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201805'</span></span></pre></td></tr></table></figure><p>增加分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建单个分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> log_partiton <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201806'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建多个分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> log_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201806'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201807'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 增加多个分区之间用空格" "隔开，删除多个分区用","隔开</span></span></pre></td></tr></table></figure><p>删除分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除单个分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> log_partition <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201804'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除多个分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> log_partition <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201803'</span>),<span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201805'</span>);</span></pre></td></tr></table></figure><p>查看分区表有多少分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> log_partition;</span></pre></td></tr></table></figure><p>查看分区表结构：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">desc formatted log_partition;</span></pre></td></tr></table></figure><h4 id="多级分区表"><a href="#多级分区表" class="headerlink" title="多级分区表"></a>多级分区表</h4><p>创建二级分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_partition2(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">lid <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">lrecord <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ltime <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据到二级分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/logs/log.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> log_partition2 <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201804'</span>, <span class="keyword">day</span>=<span class="string">'13'</span>);</span></pre></td></tr></table></figure><p>查询分区数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> log_partition2 <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201804'</span> <span class="keyword">and</span> <span class="keyword">day</span>=<span class="string">'13'</span>;</span></pre></td></tr></table></figure><h4 id="数据关联和修复"><a href="#数据关联和修复" class="headerlink" title="数据关联和修复"></a>数据关联和修复</h4><p>把数据上传到分区目录上，让分区表和数据产生关联的三种方式：</p><ol><li><p>上传数据后修复</p><p>上传数据：</p><p>hive (hive_2)&gt; <code>dfs -mkdir -p /user/hive/warehouse/log_partition2/month=201804/day=13;</code></p><p>hive (hive_2)&gt; <code>dfs -put /home/vinx/logs/log.txt /user/hive/warehouse/log_partition2/month=201804/day=13;</code></p><p>查询数据（查询不到刚才上传的数据）：</p><p>hive (hive_2)&gt; <code>select * from log_partition2 where month=&#39;201804&#39; and day=&#39;13&#39;;</code></p><p>执行修复命令：</p><p>hive (hive_2)&gt; <code>msck repair table log_partition2;</code></p><p>再次查询数据：</p><p>hive (hive_2)&gt; <code>select * from log_partition2 where month=&#39;201804&#39; and day=&#39;13&#39;;</code></p></li><li><p>上传数据后添加分区</p><p>上传数据：</p><p>hive (hive_2)&gt; <code>dfs -mkdir -p /user/hive/warehouse/log_partition2/month=201804/day=14;</code></p><p>hive (hive_2)&gt; <code>dfs -put /home/vinx/logs/log.txt /user/hive/warehouse/log_partition2/month=201804/day=14;</code></p><p>添加分区：</p><p>hive (hive_2)&gt; <code>alter table log_partition2 add partition(month=&#39;201804&#39;, day=&#39;14&#39;);</code></p><p>查询数据：</p><p>hive (hive_2)&gt; <code>select * from log_partition2 where month=&#39;201804&#39; and day=&#39;14&#39;;</code></p></li><li><p>上传数据后load数据到分区：</p><p>创建目录：</p><p>hive (hive_2)&gt; <code>dfs -mkdir -p /user/hive/warehouse/log_partition2/month=201804/day=15;</code></p><p>上传数据：</p><p>hive (hive_2)&gt; <code>load data local inpath &#39;/home/vinx/logs/log.txt&#39; into table log_partition2 partition(month=&#39;201804&#39;,day=&#39;15&#39;);</code></p><p>查询数据：</p><p>hive (hive_2)&gt; <code>select * from log_partition2 where month=&#39;201804&#39; and day=&#39;15&#39;;</code></p></li></ol><h3 id="静态分区和动态分区"><a href="#静态分区和动态分区" class="headerlink" title="静态分区和动态分区"></a>静态分区和动态分区</h3><p>分区是Hive存放数据的一种形式，查询数据时使用分区列进行过滤，只需根据列值直接扫描对应目录下的数据。</p><p>Hive的一个分区名对应一个目录名，子分区名就是子目录名，并不是一个实际的字段。</p><p>插入数据的时候指定分区，其实就是新建一个目录或者子目录，或者在原有的目录上添加数据文件。</p><h4 id="静态分区"><a href="#静态分区" class="headerlink" title="静态分区"></a>静态分区</h4><p>若分区的值是确定的，那么称为静态分区。新增分区或者是加载分区数据时，已经指定分区名。</p><p>创建静态分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> order_partition(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    order_number <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    event_time <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (event_month <span class="keyword">string</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据到分区表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法1：通过load方式加载</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/order.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> order_partition (event_month=<span class="string">'2017-09'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法2：查询并导入</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> order_partition <span class="keyword">partition</span>(event_month=<span class="string">'2017-09'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> order_number,event_time <span class="keyword">from</span> order_partition <span class="keyword">where</span> event_month=<span class="string">'2017-08'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 方法3：手动创建HDFS文件夹，并上传文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 静态分区表如果手动创建对应的HDFS目录，并上传文件，分区表中无法查到该分区信息，则需要刷新修复</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> order_partition <span class="keyword">where</span> event_month=<span class="string">'2017-09'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 此时是查询不到分区数据的，需要修复表信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">msck <span class="keyword">repair</span> <span class="keyword">table</span> order_partiton;</span></pre></td></tr></table></figure><p>查询指定分区数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> order_partiton <span class="keyword">where</span> event_month=<span class="string">'2017-09'</span>;</span></pre></td></tr></table></figure><h4 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h4><p>动态分区指不需要为不同的分区添加不同的插入语句，分区不确定，需要从数据中获取。</p><p>参数设置：</p><ol><li><p>开启动态分区功能（默认true，开启）</p><p><code>set hive.exec.dynamic.partition=true</code></p></li><li><p>设置为非严格模式（默认strict，表示必须指定至少一个分区为静态分区，nonstrict表示允许所有的分区字段都可以使用动态分区）</p><p><code>set hive.exec.dynamic.partiton.mode=nonstrict</code></p></li><li><p>在所有执行MR的节点上，最大一共可以创建多少个动态分区（默认1000）</p><p><code>set hive.exec.max.dynamic.partitions=1000</code></p></li><li><p>在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错</p><p><code>set hive.exec.max.dynamic.partitions.pernode=100</code></p></li><li><p>整个MR job中，最大可以创建多少个HDFS文件（默认值100000）</p><p><code>set hive.exec.max.created.files=100000</code></p></li><li><p>当有空分区生成时，是否抛出异常（默认false，一般不需要设置）</p><p><code>set hive.error.on.empty.partition=false</code></p></li></ol><p>案例需求：</p><p>将ori中的数据按照时间(如：20111230000008)，插入到目标表ori_partition_target的相应分区中。</p><p>创建分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partition(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (p_time <span class="built_in">bigint</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>加载数据到分区表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/datas/ds1'</span> <span class="keyword">into</span> <span class="keyword">table</span> ori_partition <span class="keyword">partition</span>(p_time=<span class="string">'20111230000010'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/datas/ds2'</span> <span class="keyword">into</span> <span class="keyword">table</span> ori_partition <span class="keyword">partition</span>(p_time=<span class="string">'20111230000011'</span>);</span></pre></td></tr></table></figure><p>创建目标分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ori_partition_target(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">time</span> <span class="built_in">bigint</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    uid <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    keyword <span class="keyword">string</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    url_rank <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    click_num <span class="built_in">int</span>, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    click_url <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">partitioned <span class="keyword">by</span> (p_time <span class="keyword">string</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>设置动态分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition = <span class="literal">true</span>;<span class="comment">-- （默认true）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode = nonstrict;<span class="comment">-- (默认strict)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions = <span class="number">1000</span>;<span class="comment">-- (默认1000)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode = <span class="number">100</span>;<span class="comment">-- （默认100）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.max.created.files = <span class="number">100000</span>;<span class="comment">-- (默认值100000)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.error.on.empty.partition = <span class="literal">false</span>;<span class="comment">-- (默认值false)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> ori_partition_target partitoin(p_time) <span class="keyword">select</span> <span class="keyword">id</span>,<span class="built_in">time</span>,uid,keyword,url_rank,click_num,click_url,p_time <span class="keyword">from</span> ori_partition;</span></pre></td></tr></table></figure><p>查看目标分区表的分区情况：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> ori_partition_target;</span></pre></td></tr></table></figure><p>如果不设置为非严格模式，报错如下：</p><p>FAILED: SemanticException [Error<br>10096]: Dynamic partition strict mode requires at least one static partition<br>column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict.</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>尽量不要使用动态分区，因为动态分区的时候，将会为每一个分区分配reducer，当分区数量多的时候，reducer数量将会相应增加。</p><p>静态分区不管有没有数据都会创建该分区，动态分区是有结果集就创建，否则不创建。</p><p>hive.mapred.mode=strict 为了阻止用户不小心提交恶意HQL</p><p>如果设置为strict，将会阻止以下3种查询：</p><ol><li>对分区表查询，where中过滤字段不是分区字段</li><li>笛卡尔积join查询，不带on或where的join查询语句</li><li>order by查询不带limit条件</li></ol><h3 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h3><p>分区针对的是数据的存储路径，分桶针对的是数据文件。</p><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。分桶是将数据集分解成更容易管理的若干部分的一个技术。</p><p>Hive分桶就是将表（或者分区，即HDFS存储在该目录下的真正数据）中文件分成几个文件去存储。在处理大规模数据集时，在开发阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。</p><p>桶中的数据可以根据一个或多个列另外进行排序，这样对每个桶的连接变成了高效的归并排序（merge-sort），可以进一步提升Map端连接的效率。</p><p>声明一个表使用排序桶：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bucketed_user(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sorted <span class="keyword">by</span>(<span class="keyword">id</span> <span class="keyword">asc</span>) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span></pre></td></tr></table></figure><h4 id="创建分桶表，并导入本地数据"><a href="#创建分桶表，并导入本地数据" class="headerlink" title="创建分桶表，并导入本地数据"></a>创建分桶表，并导入本地数据</h4><p>创建分桶表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_bucket(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) <span class="keyword">into</span> <span class="number">4</span> buckets</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>查看表结构：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">desc formatted stu_bucket;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Num Buckets:            4</span></pre></td></tr></table></figure><p>导入数据到分桶表中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_bucket;</span></pre></td></tr></table></figure><p>此时会发现并没有分成4个桶</p><h4 id="创建分桶表，并通过子查询的方式导入数据"><a href="#创建分桶表，并通过子查询的方式导入数据" class="headerlink" title="创建分桶表，并通过子查询的方式导入数据"></a>创建分桶表，并通过子查询的方式导入数据</h4><p>创建一个普通的stu表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr></table></figure><p>向stu表中导入数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> stu;</span></pre></td></tr></table></figure><p>清空stu_bucket表中数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> stu_bucket;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_bucket;</span></pre></td></tr></table></figure><p>导入数据到分桶表，通过子查询的方式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span></pre></td></tr></table></figure><p>设置属性：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">-1</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_bucket <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span></pre></td></tr></table></figure><p>查询分桶的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_bucket;</span></pre></td></tr></table></figure><h4 id="分桶抽样查询"><a href="#分桶抽样查询" class="headerlink" title="分桶抽样查询"></a>分桶抽样查询</h4><p>对于非常大的数据集，有时需要使用的是一个具有代表性的查询结果而不是全部结果，Hive可以通过对表进行抽样来满足这个需求。</p><p>查询stu_bucket中的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_bucket <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- tablesample是抽样语句，语法：tablesample(bucket x out of y)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 不是桶数的倍数或者因子也可以，但是不推荐</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_bucket <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">3</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span></pre></td></tr></table></figure><p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取（4/2=）2个bucket的数据，当y=8时，抽取（4/8=）1/2个bucket的数据。</p><p>x表示从哪个bucket开始抽取。例如，table总bucket数为4，tablesample(bucket 4 out of 4)，表示总共抽取（4/4=）1个bucket的数据，抽取第4个bucket的数据。</p><p>注意：x的值必须小于等于y的值，否则：</p><p>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_bucket.</p><h4 id="数据块抽样"><a href="#数据块抽样" class="headerlink" title="数据块抽样"></a>数据块抽样</h4><p>Hive提供了另外一种按照百分比进行抽样的方式，这种是基于行数的，按照输入路径下的数据块百分比进行的抽样。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu <span class="keyword">tablesample</span>(<span class="number">0.1</span> <span class="keyword">percent</span>);</span></pre></td></tr></table></figure><p>提示：这种抽样方式不一定适用于所有的文件格式。另外，这种抽样的最小抽样单元是一个HDFS数据块。因此，如果表的数据大小小于普通的块的大小128M的话，那么将会返回所有行。</p><h3 id="行转列和列转行"><a href="#行转列和列转行" class="headerlink" title="行转列和列转行"></a>行转列和列转行</h3><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>创建学生表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_info(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">course <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 导入本地数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/student_info.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> student_info;</span></pre></td></tr></table></figure><p>查看表中数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student_info;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------------+--------------------+----------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| student_info.id  | student_info.name  | student_info.course  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------------+--------------------+----------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 103              | zhangsan           | Java                 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 103              | zhangsan           | SQL                  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 105              | lisi               | Python               |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 109              | wangwu             | Scala                |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 109              | wangwu             | Spark                |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------------+--------------------+----------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">5 rows selected (0.742 seconds)</span></pre></td></tr></table></figure><p>将多行转为1行（行转列）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">max</span>(<span class="keyword">id</span>),<span class="keyword">name</span>,<span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_set(course)) <span class="keyword">as</span> courses</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 没有id列</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,<span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_set(course)) <span class="keyword">as</span> courses</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student_info </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">INFO  : OK</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| _c0  |   name    |   courses    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| 105  | lisi      | Python       |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">| 109  | wangwu    | Scala,Spark  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">| 103  | zhangsan  | Java,SQL     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+--------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">3 rows selected (35.758 seconds)</span></pre></td></tr></table></figure><h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>建表语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_info2(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">courses <span class="keyword">string</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 加载数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/vinx/data/student_info2.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> student_info2;</span></pre></td></tr></table></figure><p>查看表数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student_info2;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+---------------------+------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| student_info2.id  | student_info2.name  | student_info2.courses  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+---------------------+------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 103               | zhangsan            | Java,SQL               |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 105               | lisi                | Python                 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 109               | wangwu              | Scala,Spark            |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">-------------------+---------------------+------------------------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">3 rows selected (0.128 seconds)</span></pre></td></tr></table></figure><p>将1行转为多行（列转行）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,course </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student_info2 a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(a.courses, <span class="string">','</span>)) b <span class="keyword">as</span> course;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">|  id  |   name    | course  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 103  | zhangsan  | Java    |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| 103  | zhangsan  | SQL     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| 105  | lisi      | Python  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| 109  | wangwu    | Scala   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| 109  | wangwu    | Spark   |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-----------+---------+--+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">5 rows selected (0.202 seconds)</span></pre></td></tr></table></figure><h4 id="关键函数释义"><a href="#关键函数释义" class="headerlink" title="关键函数释义"></a>关键函数释义</h4><p>使用<code>desc function extended split</code>可以查看split()函数的用法，行列互转的关键函数的官方释义如下：</p><ul><li><p><code>collect_set(x)</code> - returns a set of objects with duplicate elements eliminated.</p><p>返回一个不重合的set集合</p></li><li><p><code>concat_ws(separator, [string | array(string)]+)</code> - returns the concatenation of the strings separated by the separator.</p><p>返回以指定分隔符隔离的拼接起来的字符串</p></li><li><p><code>split(str, regex)</code> - splits str around occurances that match regex.</p><p>以指定分隔符切割字符串，并返回切割后的字符串数组</p></li><li><p><code>explode(a)</code> - separates the elements of array a into multiple rows, or the elements of a map into multiple rows and columns.</p><p>分割数组元素为多行，或分割map的元素为多行和多列</p></li><li><p><code>lateral view</code> - 侧视图配合<code>explode</code>，把单行数据拆解成多行，不加<code>lateral view</code>的UDTF(User-Defined Table-Generating Functions)只能提取单个字段拆分，并不能塞回原来的数据表中，加上<code>lateral view</code>就可以将拆分的单个字段数据与原始表数据关联上</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 管理表和外部表 </tag>
            
            <tag> 分区表 </tag>
            
            <tag> 分桶表 </tag>
            
            <tag> 行列互转 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基本概念&amp;Hive的部署&amp;Hive的基本命令</title>
      <link href="/2018/04/14/dw/hive-1/"/>
      <url>/2018/04/14/dw/hive-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive基本概念"><a href="#Hive基本概念" class="headerlink" title="Hive基本概念"></a>Hive基本概念</h3><p>Hive的官网：</p><p><a href="http://hive.apache.org/" target="_blank" rel="noopener">http://hive.apache.org/</a></p><p>The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.</p><h4 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h4><p>Hive: 由Facebook开源，用于解决海量结构化日志的数据统计问题。</p><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文化映射为一张表，并提供类SQL查询功能。</p><p>本质：将HQL/SQL转化成MapReduce程序</p><ol><li>Hive处理的数据存储在HDFS</li><li>Hive底层执行引擎：MapReduce/Tez/Spark，只需要通过一个参数就能够切换底层的执行引擎</li><li>Hive作业提交到YARN上运行</li></ol><h4 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h4><p><strong>优点：</strong></p><ol><li>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）</li><li>避免了MapReduce编程，减少学习成本</li><li>Hive的执行延迟比较高，因此Hive常用于离线分析，对实时性要求不高的场合</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li></ol><p><strong>缺点：</strong></p><ol><li>Hive的HQL表达能力有限<ol><li>迭代式算法无法表达</li><li>数据挖掘方面不擅长</li></ol></li><li>Hive的效率比较低<ol><li>Hive自动生成的MapReduce作业，通常不够智能化</li><li>Hive调优比较困难，粒度较粗</li></ol></li></ol><h4 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h4><p><img src="https://vinxikk.github.io/img/dw/hive-architecture.png" alt="Hive的架构"></p><p>如上图所示，Hive通过给用户提供的一系列交互接口，接收到用户的指令（SQL），使用自己的Driver，结合元数据（MetaStore），将这些指令翻译成MapReduce，提交到Hadoop中执行，最后将执行返回的结果输出到用户交互接口。</p><ol><li><p>用户接口：Client</p><p>CLI(hive shell)、JDBC/ODBC(java访问hive)、WEBUI(浏览器访问hive)</p></li><li><p>元数据：Meta store</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</p><p>默认存储在自带的derby数据库中，推荐使用MySQL存储Meta store。</p></li><li><p>Hadoop</p><p>使用HDFS进行存储，使用MapReduce进行计算</p></li><li><p>驱动器：Driver</p><ol><li>解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</li><li>编译器（Physical Plan）：将AST编译生成逻辑执行计划。</li><li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li><li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划，对于Hive来说，就是MR/Spark。</li></ol></li></ol><h3 id="Hive的部署"><a href="#Hive的部署" class="headerlink" title="Hive的部署"></a>Hive的部署</h3><p>Hive的使用文档：</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p><p>环境要求：</p><ul><li>JDK8</li><li>Hadoop2.x</li><li>Linux</li><li>MySQL</li></ul><p>Hive版本选择：<code>wget http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.16.2.tar.gz</code></p><h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><p>解压：<code>tar -zxvf hive-1.1.0-cdh5.16.2.tar.gz -C ~/app/</code></p><p>设置软连接：<code>ln -s hive-1.1.0-cdh5.16.2 hive</code></p><p>配置环境变量，建议普通用户的<code>.bashrc</code>文件，追加以下内容：</p><p><code>export HIVE_HOME=/home/hadoop/app/hive</code><br><code>export PATH=$HIVE_HOME/bin:$PATH</code></p><p>生效环境变量文件：<code>source .bashrc</code></p><p>检查是否生效：<code>which hive</code></p><p><code>cd $HIVE_HOME/conf/</code>进入配置文件目录，<code>vi hive-site.xml</code>，编译以下内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop001:3306/metadata_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr></table></figure><p>保存并退出。</p><p>然后，拷贝MySQL驱动包到$HIVE_HOME/lib/下。</p><p>最后，启动Hive（需要先启动HDFS和YARN）：<code>hive</code></p><h3 id="Hive的使用"><a href="#Hive的使用" class="headerlink" title="Hive的使用"></a>Hive的使用</h3><h4 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h4><ul><li><code>show databases;</code>  查看数据库</li><li><code>use default;</code>  打开默认数据库</li><li><code>create database company location &#39;/user/company&#39;;</code>  创建库并指定hdfs路径</li><li><code>alter database company set dbproperties(&#39;creator&#39; = &#39;vinx&#39;);</code>  为数据库添加额外的描述信息</li><li><code>drop database if exists company cascade;</code>  删除不为空的数据库</li><li><code>show tables;</code>  查看所在数据库的所有表</li><li><code>create table student(id int, name string);</code>  创建一张表</li><li><code>desc student;</code>  查看表结构</li><li><code>desc extended student;</code>  查看表结构详细信息</li><li><code>desc formatted student;</code>  查看表结构的格式化信息</li><li><code>insert into student values(1001, &quot;zhangsan&quot;);</code>  向表中插入一条数据（慎用，会跑MR程序）</li><li><code>select * from student;</code>  查询表中数据</li><li><code>!clear;</code>  清空屏幕</li><li><code>quit;</code>  退出hive</li><li><code>bin/hive -e &quot;select * from student;&quot;</code>  不登录hive客户端直接操作hive</li><li><code>bin/hive -f /home/vinx/shell/select_stu.sql</code>  执行HQL文件</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive基本概念 </tag>
            
            <tag> Hive SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch ERROR汇总</title>
      <link href="/2018/04/13/es/es-boot-error/"/>
      <url>/2018/04/13/es/es-boot-error/</url>
      
        <content type="html"><![CDATA[<h4 id="权限错误"><a href="#权限错误" class="headerlink" title="权限错误"></a>权限错误</h4><p>Q: 启动es时，如出现异常：can not run elasticsearch as root.</p><p>问题原因：elasticsearch权限控制严格，不能用root用户启动。</p><p>解决方法：创建一个新账户，并修改es文件夹用户权限。</p><h4 id="启动错误"><a href="#启动错误" class="headerlink" title="启动错误"></a>启动错误</h4><p>Centos6.5启动elasticsearch时出现以下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">ERROR: [4] bootstrap checks failed</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[2]: max number of threads [1024] for user [vinx] is too low, increase to at least [2048]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk</span></pre></td></tr></table></figure><p>解决方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[1]: max file descriptors [4096] <span class="keyword">for</span> elasticsearch process is too low, increase to at least [65536]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">* soft nofile 65536</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">* hard nofile 65536</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">重新登录用户，才会生效</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">[2]: max number of threads [1024] <span class="keyword">for</span> user [vinx] is too low, increase to at least [2048]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.d/90-nproc.conf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">定位到：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">* soft nproc 1024</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">修改为：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">* soft nproc 4096</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误3</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">vi /ect/sysctl.conf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">vm.max_map_count=655360</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">保存后，执行：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">sysctl -p</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误4</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">[4]: system call filters failed to install; check the logs and fix your configuration or <span class="built_in">disable</span> system call filters at your own risk</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误原因</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">因为Centos6不支持SecComp，而ES5.2.1默认bootstrap.system_call_filter为<span class="literal">true</span>进行检测，所以导致检测失败，失败后直接导致ES不能启动。</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">参考：https://github.com/elastic/elasticsearch/issues/22899</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">修改config/elasticsearch.yml，修改或追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">bootstrap.memory_lock: <span class="literal">false</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">bootstrap.system_call_filter: <span class="literal">false</span></span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> ES </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ES错误 </tag>
            
            <tag> ERROR信息 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch入门</title>
      <link href="/2018/04/12/es/es-basic/"/>
      <url>/2018/04/12/es/es-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="ElasticSearch简介"><a href="#ElasticSearch简介" class="headerlink" title="ElasticSearch简介"></a>ElasticSearch简介</h3><hr><h4 id="什么是ElasticSearch"><a href="#什么是ElasticSearch" class="headerlink" title="什么是ElasticSearch"></a>什么是ElasticSearch</h4><ul><li>基于Apache Lucene构建的<strong>开源搜索引擎</strong></li><li>采用Java编写，提供简单易用的RESTFul API</li><li>轻松的横向扩展，可支持PB级的结构化或非结构化数据处理</li></ul><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><ul><li>海量数据分析引擎</li><li>站内搜索引擎</li><li>数据仓库</li></ul><h3 id="ES安装"><a href="#ES安装" class="headerlink" title="ES安装"></a>ES安装</h3><h4 id="ES官网"><a href="#ES官网" class="headerlink" title="ES官网"></a>ES官网</h4><p><a href="https://www.elastic.co" target="_blank" rel="noopener">https://www.elastic.co</a></p><h4 id="单实例安装"><a href="#单实例安装" class="headerlink" title="单实例安装"></a>单实例安装</h4><p>环境要求：</p><ul><li>JDK1.8</li><li>NodeJs(6.0以上)</li></ul><h5 id="Node安装"><a href="#Node安装" class="headerlink" title="Node安装"></a>Node安装</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解压安装好后，查看版本(此时node和npm命令不是全局的，需要设置软连接)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@eshost001 node]$ ./bin/node -v</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">v8.2.1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置软连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">ln -s node-v8.2.1-linux-x64 node</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">ln -s /home/vinx/app/node/bin/node /usr/<span class="built_in">local</span>/bin/node</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">ln -s /home/vinx/app/node/bin/npm /usr/<span class="built_in">local</span>/bin/npm</span></pre></td></tr></table></figure><h5 id="es安装："><a href="#es安装：" class="headerlink" title="es安装："></a>es安装：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gz</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">tar -vxf elasticsearch-5.5.2.tar.gz</span></pre></td></tr></table></figure><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> elasticsearch-5.5.2/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sh ./bin/elasticsearch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[master] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;127.0.0.1:9200&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[master] started</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[master] recovered [0] indices into cluster_state</span></pre></td></tr></table></figure><h4 id="head插件安装"><a href="#head插件安装" class="headerlink" title="head插件安装"></a>head插件安装</h4><h5 id="下载解压"><a href="#下载解压" class="headerlink" title="下载解压"></a>下载解压</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">wget https://github.com/mobz/elasticsearch-head/archive/master.zip</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">unzip master.zip</span></pre></td></tr></table></figure><h5 id="安装运行"><a href="#安装运行" class="headerlink" title="安装运行"></a>安装运行</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> elasticsearch-head-master/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">npm install</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">num run start</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&gt; elasticsearch-head@0.0.0 start /home/vinx/app/elasticsearch-head-master</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&gt; grunt server</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">Running <span class="string">"connect:server"</span> (connect) task</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">Waiting forever...</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Started connect web server on http://localhost:9100</span></pre></td></tr></table></figure><h5 id="修改配置文件："><a href="#修改配置文件：" class="headerlink" title="修改配置文件："></a>修改配置文件：</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改yml配置文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">vi config/elasticsearch.yml</span></pre></td></tr></table></figure><p>在最下面添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">http.cors.enabled: <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">http.cors.allow-origin: <span class="string">"*"</span></span></pre></td></tr></table></figure><h5 id="重新启动"><a href="#重新启动" class="headerlink" title="重新启动"></a>重新启动</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动elasticsearch 后台-d</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">./bin/elasticsearch -d</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动head插件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">npm run start</span></pre></td></tr></table></figure><h5 id="启动界面"><a href="#启动界面" class="headerlink" title="启动界面"></a>启动界面</h5><p><img src="https://vinxikk.github.io/img/es/es-single-green.png" alt="es单实例"></p><h4 id="分布式安装"><a href="#分布式安装" class="headerlink" title="分布式安装"></a>分布式安装</h4><p>内存建议8G。</p><h5 id="master配置文件"><a href="#master配置文件" class="headerlink" title="master配置文件"></a>master配置文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi config/elasticsearch.yml</span></pre></td></tr></table></figure><p>增加如下信息：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="attr">http.cors.enabled:</span> <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">http.cors.allow-origin:</span> <span class="string">"*"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="attr">cluster.name:</span> <span class="string">vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">node.name:</span> <span class="string">master</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="attr">node.master:</span> <span class="literal">true</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="attr">network.host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span></pre></td></tr></table></figure><h5 id="slave配置文件"><a href="#slave配置文件" class="headerlink" title="slave配置文件"></a>slave配置文件</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># slave1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">cluster.name:</span> <span class="string">vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="attr">node.name:</span> <span class="string">slave1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">network.host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="attr">http.port:</span> <span class="number">8200</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="attr">discovery.zen.ping.unicast.hosts:</span> <span class="string">["127.0.0.1"]</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">--------------------------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># slave2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="attr">cluster.name:</span> <span class="string">vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="attr">node.name:</span> <span class="string">slave2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="attr">network.host:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="attr">http.port:</span> <span class="number">8000</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="attr">discovery.zen.ping.unicast.hosts:</span> <span class="string">["127.0.0.1"]</span></span></pre></td></tr></table></figure><h5 id="分布式ES启动"><a href="#分布式ES启动" class="headerlink" title="分布式ES启动"></a>分布式ES启动</h5><p><img src="https://vinxikk.github.io/img/es/es-full-distr.png" alt="es分布式集群"></p><h3 id="ES的用法"><a href="#ES的用法" class="headerlink" title="ES的用法"></a>ES的用法</h3><h4 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h4><ul><li>Cluster-Node集群和节点：集群 = 节点1 + 节点2 + … + 节点N</li></ul><ul><li>Index索引：含有相同属性的文档集合 - Database</li><li>Type类型：索引可以定义一个或多个类型，文档必须属于一个类型 - Table</li><li>Document文档：文档是可以被索引的基本数据单位 - Row</li><li>Field字段：Column</li><li>分片：每个索引都有多个分片，每个分片是一个Lucene索引</li><li>备份：拷贝一份分片就完成了分片的备份</li></ul><h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><h5 id="索引创建"><a href="#索引创建" class="headerlink" title="索引创建"></a>索引创建</h5><p><strong>RESTFul API</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">API基本格式  http:&#x2F;&#x2F;&lt;ip&gt;:&lt;port&gt;&#x2F;&lt;索引&gt;&#x2F;&lt;类型&gt;&#x2F;&lt;文档id&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">常用HTTP动词 GET&#x2F;PUT&#x2F;POST&#x2F;DELETE</span></pre></td></tr></table></figure><h5 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h5><ul><li>指定文档id插入</li><li>自动产生文档id插入</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 指定id为1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">PUT eshost001:9200/people/man/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"name"</span>: <span class="string">" vinx"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"country"</span>: <span class="string">"CN"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"age"</span>: <span class="number">30</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"date"</span>: <span class="string">"1987-03-07"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"># 自动产生id</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/people/man/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"name"</span>: <span class="string">" super vinx"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"country"</span>: <span class="string">"CN"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"age"</span>: <span class="number">40</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"date"</span>: <span class="string">"1977-03-07"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h5 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h5><ul><li>直接修改文档</li><li>脚本修改文档</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 直接修改</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/people/man/1/_update</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"doc"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"name"</span>: <span class="string">"who's vinx"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"># 脚本修改</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/people/man/1/_update</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"script"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"lang"</span>: <span class="string">"painless"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"inline"</span>: <span class="string">"ctx._source.age += 1"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"script"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"lang"</span>: <span class="string">"painless"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"inline"</span>: <span class="string">"ctx._source.age = params.age"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"params"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"age"</span>: <span class="number">100</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h5 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h5><ul><li>删除文档</li><li>删除索引</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 删除id为1文档</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">DELETE eshost001:9200/people/man/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"># 删除索引people</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">DELETE eshost001:9200/people</span></pre></td></tr></table></figure><h5 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h5><ul><li>简单查询</li><li>条件查询</li><li>聚合查询</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 预先创建book索引</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"># 增加mapping</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">book/novel/_mappings</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  <span class="attr">"novel"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="attr">"properties"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      <span class="attr">"word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"type"</span>: <span class="string">"integer"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      &#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">      <span class="attr">"author"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"type"</span>: <span class="string">"keyword"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">      &#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">      <span class="attr">"title"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"type"</span>: <span class="string">"text"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      &#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      <span class="attr">"publish_date"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss || yyyy-MM-dd || epoch_millis"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        <span class="attr">"type"</span>: <span class="string">"date"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"># 简单查询：查询id为1的文档</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">GET eshost001:9200/book/novel/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"># 条件查询</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"># 查询所有数据</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/book/_search</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"query"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"match_all"</span>: &#123;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"># 限制查询条数</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"query"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"match_all"</span>: &#123;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"from"</span>: <span class="number">1</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"size"</span>: <span class="number">1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line"># 查询title中含有es的记录</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"query"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"match"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"title"</span>: <span class="string">"es"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line"># 指定排序条件</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"query"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"match"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"title"</span>: <span class="string">"es"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"sort"</span>: [</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"publish_date"</span>: &#123;<span class="attr">"order"</span>: <span class="string">"desc"</span>&#125;&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"># 聚合查询</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line"># 单个分组聚合，根据word_count字段聚合</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">POST eshost001:9200/book/_search</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"aggs"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"group_by_word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"terms"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"word_count"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line"># 多个分组聚合</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"aggs"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"group_by_word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"terms"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"word_count"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line">&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"group_by_publish_date"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"terms"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"publish_date"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line"># 统计函数</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"aggs"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"grades_word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"stats"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"word_count"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">104</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">105</span></pre></td><td class="code"><pre><span class="line"># 最小值</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">106</span></pre></td><td class="code"><pre><span class="line">&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">107</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"aggs"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">108</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"grades_word_count"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">109</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"min"</span>: &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">110</span></pre></td><td class="code"><pre><span class="line"><span class="attr">"field"</span>: <span class="string">"word_count"</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">111</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">112</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">113</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">114</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure><h4 id="高级查询"><a href="#高级查询" class="headerlink" title="高级查询"></a>高级查询</h4><ul><li>子条件查询 - 特定字段查询所指特定值</li><li>复合条件查询 - 以一定的逻辑组合子条件查询</li></ul><p><strong>子条件查询：</strong></p><ul><li><p>Query context</p><p>在查询过程中，除了判断文档是否满足查询条件外，ES还会计算一个<strong>_score</strong>来标识匹配的程度，旨在判断目标文档和查询条件匹配的<strong>有多好</strong>。</p></li><li><p>Filter context</p><p>在查询过程中，只判断该文档是否满足条件，只有Yes或者No。</p></li></ul><p><strong>Query Context常用查询：</strong></p><ul><li>全文本查询 - 针对文本类型数据</li><li>字段级别查询 - 针对结构化数据，如数字、日期等</li></ul><p><strong>复合条件查询：</strong></p><ul><li>固定分数查询</li><li>布尔查询</li></ul>]]></content>
      
      
      <categories>
          
          <category> ES </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ES入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN参数调优&amp;资源调度器</title>
      <link href="/2018/04/11/dw/hadoop-6/"/>
      <url>/2018/04/11/dw/hadoop-6/</url>
      
        <content type="html"><![CDATA[<h3 id="YARN参数调优"><a href="#YARN参数调优" class="headerlink" title="YARN参数调优"></a>YARN参数调优</h3><p>container: </p><p>运行task任务的容器，虚拟化的，维度memory+vcore</p><h4 id="系统和组件预留"><a href="#系统和组件预留" class="headerlink" title="系统和组件预留"></a>系统和组件预留</h4><p>假设128G，16物理core</p><ol><li><p>装完CentOS，消耗内存1G</p></li><li><p>系统预留15-20%内存（包括1），以防全部使用导致系统夯住和OOM-kill机制，或者给将来部署其他组件预留空间。</p><p>128x20%=25.6，取26G</p></li><li><p>假设只有DN、NM节点，DN=2G，NM=4G</p></li><li><p>至此剩下的内存为：128-26-2-4=96G</p></li></ol><h4 id="container内存"><a href="#container内存" class="headerlink" title="container内存"></a>container内存</h4><p>官网配置文件：</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></p><p>相关参数和默认配置如下表：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.memory-mb</td><td>-1</td><td>Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB.</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1024</td><td>The minimum allocation for every container request at the RM in MBs. Memory requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have less memory than this value will be shut down by the resource manager.</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>8192</td><td>The maximum allocation for every container request at the RM in MBs. Memory requests higher than this will throw an InvalidResourceRequestException.</td></tr></tbody></table><p>按照现存的96G内存，一般设置如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.memory-mb</td><td>96G</td><td></td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>1G</td><td>极限情况下，只有96个container，内存1G</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>96G</td><td>极限情况下，只有1个container，内存96G</td></tr></tbody></table><p>container的内存会自动增加，默认1G递增。</p><p>container：1-96个</p><h4 id="container虚拟核"><a href="#container虚拟核" class="headerlink" title="container虚拟核"></a>container虚拟核</h4><p>官网默认参数如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.pcores-vcores-multiplier</td><td>1.0</td><td>Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The number of vcores will be calculated as number of CPUs * multiplier.</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>-1</td><td>Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default.</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td><td>The minimum allocation for every container request at the RM in terms of virtual CPU cores. Requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have fewer virtual cores than this value will be shut down by the resource manager.</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>4</td><td>The maximum allocation for every container request at the RM in terms of virtual CPU cores. Requests higher than this will throw an InvalidResourceRequestException.</td></tr></tbody></table><p>一般设置如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.nodemanager.resource.pcores-vcores-multiplier</td><td>2</td><td>物理核：虚拟核</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>32</td><td>虚拟核总数</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>1</td><td>极限情况下，只有32个container</td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>32</td><td>极限情况下，只有1个container</td></tr></tbody></table><p>container：1-32个</p><p>cloudera公司推荐，一个container的vcore最好不要超过5，我们这里设置4，即：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>4</td><td>极限情况下，只有8个container</td></tr></tbody></table><h4 id="综合memory-vcore"><a href="#综合memory-vcore" class="headerlink" title="综合memory + vcore"></a>综合memory + vcore</h4><p>以vcore为主的话，确定vcore=4，那么container=8，最终memory的参数如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>12G</td><td>极限情况，container有8个</td></tr></tbody></table><p>这是理想情况，当Spark计算时内存不够大，这个参数肯定要调大，那么这种理想化的以vcore为主的设置必然要打破，以memory为主。</p><p>假如该节点还有其他组件，比如：hbase regionserver进程，需要先分配给其他组件，然后再分配container的vcore和memory。</p><p>一般，hbase regionserver=30G</p><h4 id="vcore解读"><a href="#vcore解读" class="headerlink" title="vcore解读"></a>vcore解读</h4><p>YARN自己引入的，设计的初衷是考虑不同节点的CPU的性能和计算能力不一样，比如某个物理CPU是另外一个物理CPU的2倍，这时通过设置第一个物理CPU的虚拟core来弥补这种差异。</p><p>第一台机器 强悍 pcore:vcore=1:2</p><p>第二台机器 不强悍 pcore:vcore=1:1</p><h3 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h3><p>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。</p><p>Hadoop默认的资源调度器是Capacity Scheduler，CDH默认是Fair Scheduler。</p><p>apache官网默认参数（yarn-site.xml）：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>三种调度器如下：</p><p><img src="https://vinxikk.github.io/img/dw/scheduler.png" alt="YARN的3种资源调度器"></p><h4 id="FIFO（先进先出）"><a href="#FIFO（先进先出）" class="headerlink" title="FIFO（先进先出）"></a>FIFO（先进先出）</h4><p><img src="https://vinxikk.github.io/img/dw/fifo-scheduler.png" alt="FIFO-先进先出调度器"></p><p>优点：调度算法简单，JobTracker工作负担轻。</p><p>缺点：忽略了不同作业的需求差异。例如如果类似对海量数据进行统计分析的作业长期占据计算资源，那么在其后提交的交互型作业有可能迟迟得不到处理，从而影响到用户的体验。</p><h4 id="Capacity-Scheduler（容量调度器）"><a href="#Capacity-Scheduler（容量调度器）" class="headerlink" title="Capacity Scheduler（容量调度器）"></a>Capacity Scheduler（容量调度器）</h4><p><img src="https://vinxikk.github.io/img/dw/capacity-scheduler.png" alt="Capacity Scheduler-容量调度器"></p><p>Capacity Scheduler：</p><ol><li>多队列支持，每个队列采用FIFO</li><li>为了防止同一个用户的作业独占队列中的资源，该调度器会对同一个用户提交多的作业所占资源量进行限定</li><li>首先，计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列</li><li>其次，根据作业的优先级和提交时间顺序，同时考虑用户资源量限制和内存限制对队列内任务排序</li><li>三个队列同时按照任务的先后顺序依次执行，比如：job1, job21和job31分别排在队列最前面，是最先运行，也是同时运行。</li></ol><p>容量调度器默认情况下不支持优先级，但是可以在配置文件中开启此选项，如果支持优先级，调度算法就是带有优先级的FIFO。</p><p>不支持优先级抢占，一旦一个作业开始执行，在执行完之前它的资源不会被高优先级作业所抢占。</p><p>对队列中同一用户提交的作业能够获得的资源百分比进行了限制以使同属于一个用户的作业不能出现独占资源的情况。</p><h4 id="Fair-Scheduler（公平调度器）"><a href="#Fair-Scheduler（公平调度器）" class="headerlink" title="Fair Scheduler（公平调度器）"></a>Fair Scheduler（公平调度器）</h4><p><img src="https://vinxikk.github.io/img/dw/fair-scheduler.png" alt="Fair Scheduler-容量调度器"></p><p>Fair Scheduler：</p><ol><li>支持多队列多用户，每个队列中的资源量可以配置，同一个队列中的作业公平共享队列中所有资源</li><li>比如有三个队列A, B, C。每个队列中的job按照优先级分配资源，优先级越高分配的资源越多，但是每个job都分配到资源以确保公平。在资源有限的情况下，每个job理想情况下，获得的计算资源与实际获得计算资源存在一种差距，这个差距叫缺额。同一个队列，job的资源缺额越大，越先获得的资源优先执行，作业是按照缺额的高低来先后执行的，而且可以看到上图有多个作业同时运行。</li></ol><h3 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h3><p>推测执行（Speculative Execution）是指在集群环境下运行MapReduce，可能是程序bug、负载不均或其他一些问题，导致在一个job的多个task速度不一致，比如有的任务已经完成，但是有些任务可能只跑了10%，根据木桶原理，这些任务将成为整个job的短板。如果集群启动了推测执行，这时为了最大限度的提高短板，Hadoop会为该task启动备份任务，让speculative task与原始task同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果，并且在运行完成后kill掉另外一个任务。</p><ol><li><p>作业完成时间取决于最慢的任务完成时间</p><p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件bug等，某些任务可能运行非常慢。</p><p>典型案例：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p></li><li><p>推测执行机制</p><p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时执行。谁先运行完，则采用谁的结果。</p></li><li><p>执行推测任务的前提条件</p><ol><li><p>每个task只能有一个备份任务</p></li><li><p>当前job已完成的task不小于0.05（5%）</p></li><li><p>开启推测执行参数设置，mapred-site.xml中默认是打开的</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks                may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">               may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure></li><li><p>不能启动推测执行机制的情况：</p><ol><li>任务间存在严重的负载倾斜</li><li>特殊任务，比如任务向数据库中写数据</li></ol></li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YARN调优 </tag>
            
            <tag> 调度器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN架构&amp;InputSplit和MapTask的关系&amp;Shuffle机制&amp;压缩格式</title>
      <link href="/2018/04/08/dw/hadoop-5/"/>
      <url>/2018/04/08/dw/hadoop-5/</url>
      
        <content type="html"><![CDATA[<h3 id="MR-on-YARN流程"><a href="#MR-on-YARN流程" class="headerlink" title="MR on YARN流程"></a>MR on YARN流程</h3><p>YARN是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p><h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><p>官网原文：</p><p>The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system.</p><p>The ResourceManager has two main components: Scheduler and ApplicationsManager.</p><p>RM组成：</p><ul><li>Application Manager - 应用程序管理器</li><li>Resource Scheduler - memory+cpu资源调度器</li></ul><h4 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h4><p>官网原文：</p><p>The NodeManager is responsible for launching and managing containers on a node. Containers execute tasks as specified by the AppMaster.</p><p>NM组成：</p><ul><li>container - 虚拟概念，执行MR、Spark计算任务的最小单元</li></ul><h4 id="MR-on-YARN"><a href="#MR-on-YARN" class="headerlink" title="MR on YARN"></a>MR on YARN</h4><p><img src="https://vinxikk.github.io/img/dw/mr-on-yarn.png" alt="MR on YARN流程"></p><p>描述：</p><ol><li>用户向YARN提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令等。</li><li>RM为该job分配第一个container，运行job的ApplicationMaster。</li><li>App Master向Application Manager注册，这样就可以在RM WEB界面查询这个job的运行状态。</li><li>App Master采用轮询的方式通过RPC协议向RM申请和领取资源。</li><li>一旦App Master拿到资源，就对应的与NM通信，要求启动任务。</li><li>NM为任务设置好运行环境（jar包等），将任务启动命令写在一个脚本里，并通过该脚本启动任务task。</li><li>各个task通过RPC协议向App Master汇报自己的状态和进度，以此让App Master随时掌握各个task的运行状态，从而在task运行失败后重启任务。</li><li>App Master向Application Manager注销且关闭自己。</li></ol><p>总的来说就是两步：</p><ul><li>启动App Master，申请资源；</li><li>运行任务，直到任务运行完成。</li></ul><h3 id="input-split与map-task的关系"><a href="#input-split与map-task的关系" class="headerlink" title="input split与map task的关系"></a>input split与map task的关系</h3><p>首先，<strong>split数量和map task数量一一对应</strong>。</p><p>下图是wordcount的流程：</p><p><img src="https://vinxikk.github.io/img/dw/wordcount-mr.png" alt="wordcount流程"></p><p>可以看到，一个job的Map阶段map task并行度（个数），由客户端提交job时的切片个数决定。</p><p>假如有以下两个文件，blocksize=128M，则split和map task的关系如下图所示：</p><p><img src="https://vinxikk.github.io/img/dw/split-maptask.png" alt="split-maptask的关系"></p><p>也就是说，有多少个切片，就会启动相应数量的map task进行数据处理。那么，如果需要确定map task的数量，只需要确定切片的实际数量即可。</p><h4 id="切片机制"><a href="#切片机制" class="headerlink" title="切片机制"></a>切片机制</h4><p>默认使用FileInputFormat类处理数据输入，遵循如下的切片机制：</p><ol><li>按照文件的内容长度进行切片</li><li>切片大小，默认等于block大小</li><li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li></ol><p>比如待处理数据有两个文件：</p><p>file01.txt 320M</p><p>file02.txt 10M</p><p>经过FileInputFormat的切片机制运算后，形成的切片信息如下：</p><p>file01.txt.split1–  0~128</p><p>file01.txt.split2–  128~256</p><p>file01.txt.split3–  256~320</p><p>file02.txt.split1–  0~10M</p><p>那么切片的数量是否就是分块的数量+小文件的数据量呢？其实是不一定的，因为源码中还有这样一个参数：private static final double SPLIT_SLOP = 1.1，也就是还有10%的切片裕度，下面结合源码进行说明。</p><h4 id="FileInputFormat源码分析"><a href="#FileInputFormat源码分析" class="headerlink" title="FileInputFormat源码分析"></a>FileInputFormat源码分析</h4><p>版本：hadoop2.6.0-cdh5.15.1</p><p>类路径：package org.apache.hadoop.mapreduce.lib.input.FileInputFormat.java;</p><p>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> SPLIT_SLOP = <span class="number">1.1</span>;   <span class="comment">// 10% slop</span></span></pre></td></tr></table></figure><p>getSplits获取文件切片列表的核心方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * Generate the list of files and make them into FileSplits.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * <span class="doctag">@param</span> job the job context</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * <span class="doctag">@throws</span> IOException</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// generate splits</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;InputSplit&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    List&lt;FileStatus&gt; files = listStatus(job);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      Path path = file.getPath();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">long</span> length = file.getLen();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        BlockLocation[] blkLocations;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">          blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">          FileSystem fs = path.getFileSystem(job.getConfiguration());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">          blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> blockSize = file.getBlockSize();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> bytesRemaining = length;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">          <span class="comment">// while循环判断切完剩下的部分是否大于块的1.1倍，大于的话继续切分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">                        blkLocations[blkIndex].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">                        blkLocations[blkIndex].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">            bytesRemaining -= splitSize;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">          <span class="comment">//剩余部分小于等于1.1倍且不为0，就将剩下的划分为一块</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">                       blkLocations[blkIndex].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">                       blkLocations[blkIndex].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">            <span class="comment">// Log only if the file is big enough to be splitted</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> (length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">              LOG.debug(<span class="string">"File is not splittable so no parallelization "</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">                  + <span class="string">"is possible: "</span> + file.getPath());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">          splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">                      blkLocations[<span class="number">0</span>].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">      &#125; <span class="keyword">else</span> &#123; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//Create empty hosts array for zero length files</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">        splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Save the number of input files for metrics/loadgen</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">    sw.stop();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">      LOG.debug(<span class="string">"Total # of splits generated by getSplits: "</span> + splits.size()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">          + <span class="string">", TimeTaken: "</span> + sw.now(TimeUnit.MILLISECONDS));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> splits;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure><p>整体流程：</p><ol><li>找到数据存储的目录</li><li>遍历处理（规划切片）目录下的每一个文件</li><li>遍历第一个文件<ol><li>获取文件大小</li><li>计算切片大小。默认情况下，切片大小=blocksize</li><li>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分为一块切片</li><li>将切片信息写到一个切片规划文件中</li><li>整个切片的核心过程在getSplit()方法中完成</li><li>数据切片只是在逻辑上对输入数据进行分片，并不会在磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。</li><li>注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分</li></ol></li><li>提交切片规划文件到YARN上，YARN上的AppMaster就可以根据切片规划文件计算开启map task个数。</li></ol><p>总结：所以，切片的数量并不一定等于分块的数量+小文件的数据量，还要考虑大文件切分后剩余的部分是否大于块的1.1倍。</p><h3 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h3><p>MapReduce确保每个reduce的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reduce的过程）就是Shuffle。</p><p><img src="https://vinxikk.github.io/img/dw/shuffle.png" alt="Shuffle"></p><p><img src="https://vinxikk.github.io/img/dw/shuffle-detail.png" alt="Shuffle机制"></p><p><strong>Shuffle过程：</strong></p><p>shuffle一开始就是map阶段做输出操作，map在做输出时会在内存里开启一个环形缓冲区，这个缓冲区是专门用来输出的，默认大小是100MB，并且在配置文件里为这个缓冲区设定了一个阈值，默认是0.80。同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阈值的80%的时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill（溢写）。另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作互不干扰，如果缓存区被撑爆了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作。</p><p>写入磁盘前会有个排序操作，这个是在写入磁盘操作时进行，不是在写入内存时进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。每次spill操作也就是写入磁盘时就会写一个溢出文件，也就是说在做map输出时有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。</p><p>这个过程里还会有一个Partitioner操作，和map阶段的输入分片（Input Split）很像，一个Partitioner对应一个reduce作业，如果我们的MR操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片。</p><p>到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，这个复制过程和map写入磁盘过程类似，也有阈值和内存大小，阈值可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。</p><p>上面也提到，将map的输出作为reduce的输入的过程就是shuffle（洗牌）。</p><p>那么shuffle到底是属于map阶段还是reduce阶段呢？比较公认的说法是，shuffle应该是属于reduce阶段，因为map阶段的主要任务是数据映射，而shuffle的目的是主要是为reduce阶段做准备的。</p><h4 id="MapReduce计算框架的不足之处"><a href="#MapReduce计算框架的不足之处" class="headerlink" title="MapReduce计算框架的不足之处"></a>MapReduce计算框架的不足之处</h4><p>从上图可以看到，shuffle阶段中会有多次写入磁盘的操作，基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。另外，当一些查询（Hive）翻译到MapReduce任务时，往往会产生多个stage（阶段），而这些串联的stage又依赖于底层文件系统（HDFS）来存储每一个stage的输出结果，而I/O的效率往往较低，从而影响了MapReduce的运行速度。相比之下，Spark的运算结果中间不落地，而且兼容HDFS、Hive，实际应用中会以Spark替代MR作为常用的计算引擎。</p><h3 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h3><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。</p><h4 id="压缩的好处和坏处"><a href="#压缩的好处和坏处" class="headerlink" title="压缩的好处和坏处"></a>压缩的好处和坏处</h4><p>好处：</p><ul><li>减少磁盘存储空间</li><li>降低IO（网络的IO和磁盘的IO）</li><li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li></ul><p>坏处：</p><ul><li>由于使用数据时，需要先将数据解压，加重CPU负荷</li></ul><p>基本原则：</p><ol><li>运算密集型的job，少用压缩</li><li>IO密集型的job，多用压缩</li></ol><h4 id="压缩格式-1"><a href="#压缩格式-1" class="headerlink" title="压缩格式"></a>压缩格式</h4><p>Hadoop中的压缩格式：</p><table><thead><tr><th>压缩格式</th><th>工具</th><th>算法</th><th>扩展名</th><th>是否支持分割</th><th>Hadoop编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>N/A</td><td>DEFLATE</td><td>.deflate</td><td>No</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>gzip</td><td>DEFLATE</td><td>.gz</td><td>No</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>Yes</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>Lzop</td><td>LZO</td><td>.lzo</td><td>Yes(if index)</td><td>com.hadoop.compression.lzo.LzoCodec</td></tr><tr><td>LZ4</td><td>N/A</td><td>LZ4</td><td>.lz4</td><td>No</td><td>org.apache.hadoop.io.compress.Lz4Codec</td></tr><tr><td>Snappy</td><td>N/A</td><td>Snappy</td><td>.snappy</td><td>No</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>是否可分割是指，压缩后的文件是否可以再分割。可以分割的格式允许单一文件由多个Mapper程序同时读取，可以做到更好的并行化。</p><p>压缩比：</p><p><img src="https://vinxikk.github.io/img/dw/compress-codec-size.png" alt="压缩之后的size"></p><p>压缩时间：</p><p><img src="https://vinxikk.github.io/img/dw/compress-codec-time.png" alt="压缩时间-两次不同大小的文件"></p><p>可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</p><p>压缩格式的优缺点：</p><table><thead><tr><th>压缩格式</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>gzip</td><td>压缩比在四种压缩方式中较高；Hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分Linux系统都自带gzip命令，使用方便</td><td>不支持split</td></tr><tr><td>lzo</td><td>压缩/解压速度也比较快，合理的压缩率；支持split，是Hadoop中最流行的压缩格式；支持hadoop native库；需要在Linux系统下自行安装lzop命令，使用方便</td><td>压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td></tr><tr><td>snappy</td><td>压缩速度快；支持hadoop native库</td><td>不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td></tr><tr><td>bzip2</td><td>支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td><td>压缩/解压速度慢；不支持native</td></tr></tbody></table><h4 id="压缩的应用场景"><a href="#压缩的应用场景" class="headerlink" title="压缩的应用场景"></a>压缩的应用场景</h4><p>在Hadoop中的应用场景主要在三方面：输入，中间，输出。</p><p>整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce</p><ol><li>Use Compressd Map Input: 从HDFS中读取文件进行MR作业，如果数据很大，可以使用压缩并且选择支持分片的压缩方式（bzip2, LZO），可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式，例如：Sequence Files, RC, ORC等。</li><li>Compress Intermediate Data: Map输出作为Reduce的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少存储文件所占空间，提升数据传输速率。建议使用压缩速度快的压缩方式，例如：Snappy和LZO。</li><li>Compress Reducer Output: 进行归档处理或者链接MR的工作（该作业的输出作为下个作业的输入），压缩可以减少存储文件所占空间，提升数据传输速率，如果作为归档处理，可以采用高的压缩比（Gzip, bzip2），如果作为下个作业的输入，考虑是否要分片进行选择。</li></ol><h4 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h4><p>要在Hadoop中启动压缩，可以配置如下参数：</p><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs <br/>（在core-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td><td>输入压缩</td><td>Hadoop使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compress<br/>（在mapred-site.xml中配置）</td><td>False</td><td>mapper输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.map.output.compress.codec<br/>（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>mapper输出</td><td>使用LZO或snappy编解码器在此阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutputformat.compress<br/>（在mapred-site.xml中配置）</td><td>False</td><td>reducer输出</td><td>这个参数设为true启用压缩</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.codec<br/>（在mapred-site.xml中配置）</td><td>org.apache.hadoop.io.compress. DefaultCodec</td><td>reducer输出</td><td>使用标准工具或者编解码器，如gzip和bzip2</td></tr><tr><td>mapreduce.output.fileoutputformat.compress.type<br/>（在mapred-site.xml中配置）</td><td>RECORD</td><td>reducer输出</td><td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td></tr></tbody></table><hr><p>参考：</p><p><a href="https://www.cnblogs.com/sharpxiajun/p/3151395.html" target="_blank" rel="noopener">https://www.cnblogs.com/sharpxiajun/p/3151395.html</a></p><p><a href="https://blog.csdn.net/yljphp/article/details/89067858" target="_blank" rel="noopener">https://blog.csdn.net/yljphp/article/details/89067858</a></p><p><a href="https://blog.csdn.net/yu0_zhang0/article/details/79524842" target="_blank" rel="noopener">https://blog.csdn.net/yu0_zhang0/article/details/79524842</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YARN </tag>
            
            <tag> Split分片机制 </tag>
            
            <tag> Shuffle机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS副本放置策略&amp;读写流程&amp;PID文件&amp;常用命令&amp;磁盘均衡</title>
      <link href="/2018/04/04/dw/hadoop-4/"/>
      <url>/2018/04/04/dw/hadoop-4/</url>
      
        <content type="html"><![CDATA[<h3 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h3><p>假设副本数（dfs.replication）为3。</p><p>第一个副本：假如上传节点为DN节点，则存放在本节点上（假如Client不在集群范围内，则随机选取一台磁盘不太慢 、CPU不太繁忙的节点放置）。</p><p>第二个副本：存放在与第一个副本不同机架的一个节点上。</p><p>第三个副本：和第二个副本在同一个机架，放在不同的节点上。</p><h3 id="HDFS文件读写流程"><a href="#HDFS文件读写流程" class="headerlink" title="HDFS文件读写流程"></a>HDFS文件读写流程</h3><h4 id="写流程（FSDataOutputStream）"><a href="#写流程（FSDataOutputStream）" class="headerlink" title="写流程（FSDataOutputStream）"></a>写流程（FSDataOutputStream）</h4><p>Client上传文件到HDFS，也就是类似于下面的命令：</p><p>hadoop fs -put test.log /</p><p><img src="https://vinxikk.github.io/img/dw/hdfs-output.png" alt="HDFS文件写流程"></p><p>过程：</p><ol><li>Client 调用FileSystem.create(filePath)方法，与NN进行[RPC]通信，check是否存在及是否有权限创建。假如不OK，就返回错误信息；假如OK，就创建一个新文件，不关联任何的block块，返回一个FSDataOutputStream对象。</li><li>Client调用FSDataOutputStream对象的write()方法，先将第一块的第一个副本写到第一个DN，第一个副本写完就传输给第二个DN，第二个副本写完就传输给第三个DN，直至写完第三个副本。然后返回一个ack packet确认包给第二个DN，第二个DN接收到第三个的ack packet确认包加上自身OK，就返回一个ack packet确认包给第一个DN，第一个DN接收到第二个DN的ack packet确认包加上自身OK，就返回ack packet确认包给FSDataOutputStream对象，标志第一个块的3个副本写完。依次写完余下的块。</li><li>当文件写入数据完成后，Client调用FSDataOutputStream.close()方法，关闭输出流。</li><li>然后调用FileSystem.complete()方法，告诉NN该文件写入成功。</li></ol><h4 id="读流程（FSDataInputStream）"><a href="#读流程（FSDataInputStream）" class="headerlink" title="读流程（FSDataInputStream）"></a>读流程（FSDataInputStream）</h4><p>Client从HDFS下载文件到本地，类似于下面的命令：</p><p>hadoop fs -get /test.log</p><p><img src="https://vinxikk.github.io/img/dw/hdfs-input.png" alt="HDFS文件读流程"></p><p>过程：</p><ol><li><p>Client调用FileSystem.open(filePath)方法，与NN进行[RPC]通信，返回该文件的部分或者全部的block列表，也就是返回FSDataInputStream对象。</p></li><li><p>Client调用FSDataInputStream.read()方法。</p><p>a.与第一个块最近的DN进行read，读取完成后，会check；假如OK，就关闭与当前DN的通信；假如失败，会记录失败块+DN信息，下次不会再读取，会去该块的第二个DN地址读取。</p><p>b.接着去第二个块的最近的DN上通信读取，check后，关闭通信。</p><p>c.假如block列表读取完成后，文件还未结束，FileSystem会再次从NN获取该文件的下一批次的block列表。</p><p>（感觉就是连续的数据流，对于客户端操作是透明无感知的）</p></li><li><p>Client调用FSDataInputStream.close()方法，关闭输入流。</p></li></ol><h3 id="pid文件"><a href="#pid文件" class="headerlink" title="pid文件"></a>pid文件</h3><p>HADOOP和YARN的pid文件默认存储在/tmp目录下。系统会自动清理/tmp文件夹下30天不访问的文件，比如hadoop-env.sh中的配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The directory <span class="built_in">where</span> pid files are stored. /tmp by default.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> NOTE: this should be <span class="built_in">set</span> to a directory that can only be written to by </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">       the user that will run the hadoop daemons.  Otherwise there is the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">       potential <span class="keyword">for</span> a symlink attack.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span></pre></td></tr></table></figure><p>可以看到HADOOP_PID_DIR默认设置为/tmp目录下，所以需要更改pid的存放目录。</p><h4 id="创建用户目录下的tmp文件夹："><a href="#创建用户目录下的tmp文件夹：" class="headerlink" title="创建用户目录下的tmp文件夹："></a>创建用户目录下的tmp文件夹：</h4><p>mkdir /home/vinx/tmp</p><p>chmod -R 777 /home/vinx/tmp</p><h4 id="修改hadoop-env-sh："><a href="#修改hadoop-env-sh：" class="headerlink" title="修改hadoop-env.sh："></a>修改hadoop-env.sh：</h4><p>vi hadoop-env.sh</p><p>export HADOOP_PID_DIR=/home/vinx/tmp</p><h4 id="修改yarn-env-sh"><a href="#修改yarn-env-sh" class="headerlink" title="修改yarn-env.sh"></a>修改yarn-env.sh</h4><p>vi yarn-env.sh</p><p>export YARN_PID_DIR=/home/vinx/tmp</p><p>重新启动HDFS和YARN，可以看到在/home/vinx/tmp下生成了对应的pid文件。</p><h3 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h3><p>hadoop fs == hdfs dfs</p><p>查看hadoop fs命令使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hadoop fs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span></pre></td></tr></table></figure><p>查看hdfs dfs命令使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hdfs dfs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span></pre></td></tr></table></figure><p>可以看到hdfs dfs底层就是调用了hadoop fs。</p><h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><ul><li>hdfs dfs -ls /                     查看hdfs根目录下的文件</li><li>hdfs dfs -put test.log /     将本地test.log文件上传至hdfs根目录</li><li>hdfs dfs -get /test.log ./   将hdfs根目录下的test.log下载至本地当前目录下</li><li>hdfs dfs -rm /test.log       删除hdfs根目录下的test.log，如果设置了回收站，会放入回收站</li><li>hdfs dfs -rm -skipTrash /test.log    直接删除test.log</li><li>hdfs dfs -copyFromLocal test.log /  类似于上面的-put</li><li>hdfs dfs -copyToLocal /test.log ./     类似于上面的-get</li></ul><h4 id="配置回收站"><a href="#配置回收站" class="headerlink" title="配置回收站"></a>配置回收站</h4><p>cd /home/vinx/app/hadoop/etc/hadoop</p><p>vi core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 设置时间为7天，默认单位：minute</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10080<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>start-dfs.sh重新启动hdfs，执行hdfs dfs -rm /test.log，此时test.log文件并没有直接删除，而是会移动到回收站。若想直接删除文件，可以使用hdfs dfs -rm -skipTrash /test.log。CDH默认是开启回收站的。</p><h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4><p>安全模式下只能read，不能write，即只能查看或下载hdfs上的文件，不能修改或者从本地上传文件至hdfs。</p><p>进入安全模式：</p><p>hdfs dfsadmin -safemode enter</p><p>如果NN的log显示进入了safe mode，需要执行以下命令让其离开安全模式：</p><p>hdfs dfsadmin -safemode leave</p><p>注意：尽量在下游HDFS少做安全模式，而是上游来控制数据同步。</p><h4 id="文件系统的检查"><a href="#文件系统的检查" class="headerlink" title="文件系统的检查"></a>文件系统的检查</h4><p>从hdfs根目录开启检查文件系统：</p><p>hdfs fsck /</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hdfs fsck /</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">18/12/08 17:50:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Connecting to namenode via http://hadoop001:50070/fsck?ugi=vinx&amp;path=%2F</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">FSCK started by vinx (auth:SIMPLE) from /192.168.137.130 <span class="keyword">for</span> path / at Sun Dec 08 17:50:14 CST 2018</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">........Status: HEALTHY</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"> Total size:175138 B</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"> Total <span class="built_in">dirs</span>:17</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> Total files:8</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"> Total symlinks:0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"> Total blocks (validated):7 (avg. block size 25019 B)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"> Minimally replicated blocks:7 (100.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"> Over-replicated blocks:0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"> Under-replicated blocks:0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"> Mis-replicated blocks:0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"> Default replication factor:1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"> Average block replication:1.0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"> Corrupt blocks:0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"> Missing replicas:0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"> Number of data-nodes:1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"> Number of racks:1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">FSCK ended at Sun Dec 08 17:50:14 CST 2018 <span class="keyword">in</span> 36 milliseconds</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">The filesystem under path <span class="string">'/'</span> is HEALTHY</span></pre></td></tr></table></figure><p>主要关注两个指标：</p><ul><li>Corrupt blocks    损坏的块</li><li>Missing replicas  丢失的副本</li></ul><h3 id="多节点、单节点的磁盘均衡"><a href="#多节点、单节点的磁盘均衡" class="headerlink" title="多节点、单节点的磁盘均衡"></a>多节点、单节点的磁盘均衡</h3><h4 id="各DN节点的数据均衡"><a href="#各DN节点的数据均衡" class="headerlink" title="各DN节点的数据均衡"></a>各DN节点的数据均衡</h4><p>官网中hdfs-site.xml的balance配置说明：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>dfs.datanode.balance.bandwidthPerSec</td><td>10m</td><td>Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)to specify the size (such as 128k, 512m, 1g, etc.). Or provide complete size in bytes (such as 134217728 for 128 MB).</td></tr></tbody></table><p>参数指定了每个DN节点做数据均衡时，可以利用的最大带宽，单位是bytes/second。</p><p>可以看到dfs.datanode.balance.bandwidthPerSec默认值为10m，生产上一般设为30m。</p><p>默认数据均衡的阈值threshold = 10.0，即所有节点的磁盘used与集群的平均used之差要小于这个阈值。</p><p>指定阈值为10%，并启动数据均衡shell脚本：</p><p>sbin/start-balancer.sh -threshold 10</p><p>每天定时执行sbin/start-balancer.sh，做数据平衡和毛刺修正。</p><p>调度工具可以选择crontab或Azkaban。</p><h4 id="一个DN节点的多个磁盘的数据均衡"><a href="#一个DN节点的多个磁盘的数据均衡" class="headerlink" title="一个DN节点的多个磁盘的数据均衡"></a>一个DN节点的多个磁盘的数据均衡</h4><p>官网HDFS Disk Balancer：</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p><p>其中：dfs.disk.balancer.enabled must be set to true in hdfs-site.xml.即hdfs-site.xml中的dfs.disk.balancer.enabled必须设置为true。</p><p>使用步骤：</p><p>hdfs diskbalancer -plan hadoop001                             生成hadoop001.plan.json<br>hdfs diskbalancer -execute hadoop001.plan.json      执行<br>hdfs diskbalancer -query hadoop001                           查询状态</p><p>什么时候执行diskbalancer：</p><ol><li>新盘加入</li><li>监控磁盘剩余空间，小于阈值10%，发邮件预警，手动执行</li></ol><h4 id="为什么DN在生产上挂载多个物理的磁盘目录"><a href="#为什么DN在生产上挂载多个物理的磁盘目录" class="headerlink" title="为什么DN在生产上挂载多个物理的磁盘目录"></a>为什么DN在生产上挂载多个物理的磁盘目录</h4><p>/data01 disk1<br>/data02 disk2<br>/data03 disk3</p><ol><li>为了高效率读写数据</li><li>提前规划好2-3年存储量 ，避免后期加磁盘维护的工作量</li></ol><p>官网中hdfs-site.xml的相关参数：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>dfs.datanode.data.dir</td><td>file://${hadoop.tmp.dir}/dfs/data</td><td>Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. The default storage type will be DISK if the directory does not have a storage type tagged explicitly. Directories that do not exist will be created if local filesystem permission allows.</td></tr></tbody></table><p>参数指定dfs数据节点应该在本地文件系统上存储其块的位置。</p><p>比如设置dfs.datanode.data.dir = /data01,/data02,/data03,/data04，注意以逗号分隔，comma-delimited</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS架构&amp;NameNode和DataNode工作机制&amp;BLOCK和副本数&amp;小文件问题</title>
      <link href="/2018/04/01/dw/hadoop-3/"/>
      <url>/2018/04/01/dw/hadoop-3/</url>
      
        <content type="html"><![CDATA[<h3 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h3><p>HDFS主要由3个组件构成：NameNode, DataNode, SecondaryNameNode。</p><p>HDFS是以master/slave模式运行的，通常NameNode, SecondaryNameNode运行在master节点，DataNode运行在slave节点。</p><p>HDFS架构图：</p><p><img src="https://vinxikk.github.io/img/dw/hdfs-architecture.png" alt="HDFS主从架构"></p><h4 id="NameNode（名称节点）"><a href="#NameNode（名称节点）" class="headerlink" title="NameNode（名称节点）"></a>NameNode（名称节点）</h4><p>存储：文件元数据信息，包含：</p><ul><li>文件名称</li><li>文件目录结构</li><li>文件的属性（权限，创建时间，副本数）</li><li>文件对应哪些数据块 –&gt; 数据块对应哪些DN节点</li></ul><p>作用：</p><ul><li>管理文件系统命名空间</li><li>维护文件系统树及树中的所有文件和目录</li><li>维护所有这些文件或目录的打开、关闭、移动、重命名等操作</li></ul><h5 id="NameNode工作机制"><a href="#NameNode工作机制" class="headerlink" title="NameNode工作机制"></a>NameNode工作机制</h5><p><img src="https://vinxikk.github.io/img/dw/namenode-process.png" alt="NN工作机制"></p><p>第一阶段：NameNode启动</p><ol><li>第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志（edits）和镜像文件（fsimage）到内存。</li><li>客户端对元数据进行增删改的请求。</li><li>NameNode记录操作日志，更新滚动日志。</li><li>NameNode在内存中对数据进行增删改查。</li></ol><p>第二阶段：SecondaryNameNode工作</p><ol><li>SecondaryNameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。</li><li>SecondaryNameNode请求执行checkpoint。</li><li>NameNode滚动正在写的edits日志。</li><li>将滚动前的编辑日志和镜像文件拷贝到SecondaryNameNode。</li><li>SecondaryNameNode加载编辑日志和镜像文件到内存，并合并。</li><li>生成新的镜像文件fsimage.chkpoint。</li><li>拷贝fsimage.chkpoint到NameNode。</li><li>NameNode将fsimage.chkpoint重新命名成fsimage。</li></ol><p>checkpoint检查时间参数设置：</p><ol><li><p>通常情况下，SecondaryNameNode每隔1小时执行一次</p><p>[hdfs-default.xml]</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure></li><li><p>一分钟检查一次操作次数，当操作次数达到一百万时，SecondaryNameNode执行一次</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure></li></ol><h4 id="DataNode（数据节点）"><a href="#DataNode（数据节点）" class="headerlink" title="DataNode（数据节点）"></a>DataNode（数据节点）</h4><p>存储：数据块，数据块校验和，与NN通信</p><p>作用：</p><ul><li>读写文件的数据块</li><li>接收NN的指示来进行创建、删除、复制等操作</li><li>通过心跳定期向NN发送所存储文件块列表信息</li></ul><h5 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h5><p><img src="https://vinxikk.github.io/img/dw/datanode-detail.png" alt="DN工作机制"></p><ol><li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据，包括数据块的长度、块数据的校验和以及时间戳。</li><li>DataNode启动后向NameNode注册，通过后，周期性（1小时）地向NameNode上报所有的块信息。</li><li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令，如复制块数据到另一台机器或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li><li>集群运行中可以安全加入和退出一些机器。</li></ol><h5 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h5><ol><li>当DataNode读取block的时候，它会计算checksum校验和。</li><li>如果计算后的checksum，与block创建时值不一样，说明block已经损坏。</li><li>client读取其他DataNode上的block。</li><li>DataNode在其文件创建后周期验证checksum校验和。</li></ol><h5 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h5><p>DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：</p><p>timeout  = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval</p><p>而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p><p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span> dfs.heartbeat.interval <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><h4 id="ScondaryNameNode（第二名称节点）"><a href="#ScondaryNameNode（第二名称节点）" class="headerlink" title="ScondaryNameNode（第二名称节点）"></a>ScondaryNameNode（第二名称节点）</h4><p>存储：命名空间镜像文件fsimage+编辑日志editlog</p><p>作用：</p><ul><li>定期合并fsimage+editlog文件为新的fsimage，推送给NN</li><li>监控HDFS状态，每隔一段时间获取HDFS元数据的快照</li></ul><p>为了解决单点故障，设置了SNN的1小时备份机制，虽然能够减轻单点故障，但是还会有风险，即在那1小时中，还是会有发生单点故障的可能，造成数据丢失。为了解决这个问题，需要部署HDFS -HA高可用。</p><h4 id="机架感知（副本放置策略）"><a href="#机架感知（副本放置策略）" class="headerlink" title="机架感知（副本放置策略）"></a>机架感知（副本放置策略）</h4><p>官网地址：</p><p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/RackAwareness.html</a></p><p><a href="http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication</a></p><p>一个hadoop分布式集群会有很多的服务器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，机架内的服务器之间的网络速度通常都会高于跨机架服务器之间的网络速度，并且机架之间服务器的网络通信通常受到上层交换机间网络带宽的限制。</p><p><img src="https://vinxikk.github.io/img/dw/replication-and-rack-awareness.jpg" alt="副本放置策略"></p><p>HDFS对数据文件是分block存储，每个block默认有3个副本（也可以配置大于3），HDFS对副本的存放策略如下：</p><ol><li>第一个副本：放置在Client所在的DN节点上（如果是集群外提交，则随机挑选一台磁盘不太慢、CPU不太忙的DN节点）</li><li>第二个副本：放置在与第一个副本不同的机架的节点上（随机选择）</li><li>第三个副本：与第二个副本相同机架的不同节点上</li><li>如果还有更多的副本，随机放在集群的节点中</li></ol><p>这样的策略主要是为了数据的可靠性和数据访问的性能：</p><ul><li>数据分布在不同的机架上，就算当前机架挂掉，其他机架上还有冗余备份，整个集群依然能对外服务。</li><li>数据分布在不同的机架上，运行MR任务时可以就近获取所需的数据。</li></ul><h3 id="块大小和副本数"><a href="#块大小和副本数" class="headerlink" title="块大小和副本数"></a>块大小和副本数</h3><p>块大小和副本数需要在hdfs-site.xml中配置。</p><p>官网中的相应的默认参数如下：</p><table><thead><tr><th>name</th><th>value</th><th>description</th></tr></thead><tbody><tr><td>dfs.blocksize</td><td>134217728</td><td>The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).</td></tr><tr><td>dfs.replication</td><td>3</td><td>Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.</td></tr></tbody></table><p>可以看到，默认块大小为128M，默认副本数为3.</p><p>这里会有一个常规的面试题：</p><p>问：假如一个文件300M，块128M，副本2。请问实际存储空间多大，多少块？</p><p>答：300 x 2 = 600M，3 x 2 = 6块</p><p>需要注意的是，实际存储空间=文件大小x副本数，并不是块大小，即使44M也占用一个块。</p><h3 id="HDFS小文件问题"><a href="#HDFS小文件问题" class="headerlink" title="HDFS小文件问题"></a>HDFS小文件问题</h3><p>HDFS上每个文件都要在NameNode上建立一个索引，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用NameNode的内存空间，另一方面就是索引文件过大使得索引速度变慢。</p><h4 id="小文件优化"><a href="#小文件优化" class="headerlink" title="小文件优化"></a>小文件优化</h4><p>小文件的优化无非以下几种方式：</p><ol><li>在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS</li><li>在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并</li></ol><p>根据上面的思想，可以考虑采用下面的解决方案：</p><ol><li><p>Hadoop Archive：</p><p>是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样就减少了namenode的内存使用。</p></li><li><p>Sequence file：</p><p>sequence file由一系列的二进制key/value组成，如果key为文件名，value为文件内容，则可以将大批小文件合并成一个大文件。</p></li><li><p>CombineFileInputFormat：</p><p>CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。</p></li><li><p>开启JVM重用：</p><p>对于大量小文件job，可以开启JVM重用，会减少45%运行时间。</p><p>JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他map。</p><p>具体设置：mapreduce.job.jvm.numtasks值在10-20之间。</p></li></ol><p>总的来说，解决小文件问题主要就是将小文件合并成大文件，一般约定：尽量使得合并后的大文件&lt;=blocksize，比如110M（假如块大小128M）。</p><p>一般在生产上会设置一个阈值，比如10M，作为小文件的门槛，并使用shell脚本调用程序进行小文件的定期合并。</p><hr><p>参考：</p><p><a href="https://www.jianshu.com/p/39254e03558d" target="_blank" rel="noopener">https://www.jianshu.com/p/39254e03558d</a></p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS架构 </tag>
            
            <tag> NN/DN工作机制 </tag>
            
            <tag> HDFS小文件问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN伪分布式&amp;WordCount案例&amp;jps命令&amp;OOM-kill机制和/tmp目录定时清理</title>
      <link href="/2018/03/30/dw/hadoop-2/"/>
      <url>/2018/03/30/dw/hadoop-2/</url>
      
        <content type="html"><![CDATA[<h3 id="YARN伪分布式"><a href="#YARN伪分布式" class="headerlink" title="YARN伪分布式"></a>YARN伪分布式</h3><p>官网：</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node</a></p><h4 id="伪分布式部署"><a href="#伪分布式部署" class="headerlink" title="伪分布式部署"></a>伪分布式部署</h4><ol><li><p>修改如下的配置文件：</p><p>etc/hadoop/mapred-site.xml:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>etc/hadoop/yarn-site.xml:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 默认yarn的端口号是8088，可以在此修改 --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure></li><li><p>启动yarn：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ sbin/start-yarn.sh </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">3908 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">3768 ResourceManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">3865 NodeManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ netstat -nlp | grep 3768</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">(Not all processes could be identified, non-owned process info</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"> will not be shown, you would have to be root to see it all.)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::8030                     :::*                        LISTEN      3768/java           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::8031                     :::*                        LISTEN      3768/java           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::8032                     :::*                        LISTEN      3768/java           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::8033                     :::*                        LISTEN      3768/java           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 ::ffff:192.168.xxx.xxx:8088 :::*                        LISTEN      3768/java</span></pre></td></tr></table></figure><p>可以看到YARN已经正常启动。</p></li><li><p>查看web界面：</p><p>ResourceManager - <a href="http://localhost:8088/" target="_blank" rel="noopener">http://localhost:8088/</a></p><p><img src="https://vinxikk.github.io/img/dw/yarn-web.png" alt="YARN的web界面"></p></li><li><p>运行一个MR作业</p></li><li><p>停止yarn：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ sbin/stop-yarn.sh</span></pre></td></tr></table></figure></li></ol><h3 id="MR-wordcount案例"><a href="#MR-wordcount案例" class="headerlink" title="MR wordcount案例"></a>MR wordcount案例</h3><p>wordcount是大数据领域的helloworld。</p><p>下面以Hadoop中的example jar演示wordcount案例。</p><p>数据准备：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ hdfs dfs -cat /wordcount/input/words.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">hello</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">world</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">hello</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">hello spark</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">hello flink</span></pre></td></tr></table></figure><p>当前路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ <span class="built_in">pwd</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">/home/vinx/app/hadoop</span></pre></td></tr></table></figure><p>查找名称中有example的jar包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ find ./ -name <span class="string">'*example*.jar'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">./share/hadoop/mapreduce1/hadoop-examples-2.6.0-mr1-cdh5.16.2.jar</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.16.2-test-sources.jar</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">./share/hadoop/mapreduce2/sources/hadoop-mapreduce-examples-2.6.0-cdh5.16.2-sources.jar</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">./share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar  选用这个jar包</span></pre></td></tr></table></figure><p>启动hdfs和yarn：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ sbin/start-dfs.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ sbin/start-yarn.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">4678 SecondaryNameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">4791 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">3768 ResourceManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">3865 NodeManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4378 NameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">4511 DataNode</span></pre></td></tr></table></figure><p>运行wordcount程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ hadoop \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">&gt; jar /home/vinx/app/hadoop/share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&gt; wordcount \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&gt; /wordcount/input \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">&gt; /wordcount/output02</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">命令解释：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">hadoop \           需要加\换行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">jar /home/vinx/app/hadoop/share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.16.2.jar \        使用的jar包</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">wordcount \            使用的主类</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">/wordcount/input \     输入数据 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">/wordcount/output02    输出路径</span></pre></td></tr></table></figure><p>程序运行成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:29 INFO mapreduce.Job: Running job: job_1575979215182_0001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:43 INFO mapreduce.Job: Job job_1575979215182_0001 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:43 INFO mapreduce.Job:  map 0% reduce 0%</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:50 INFO mapreduce.Job:  map 100% reduce 0%</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:57 INFO mapreduce.Job:  map 100% reduce 100%</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">18/03/10 20:31:58 INFO mapreduce.Job: Job job_1575979215182_0001 completed successfully</span></pre></td></tr></table></figure><p>查看程序计算结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ hadoop fs -cat /wordcount/output02/part-r-00000</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">flink1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">hadoop1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">hello4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">spark1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">world1</span></pre></td></tr></table></figure><h3 id="jps的真正使用"><a href="#jps的真正使用" class="headerlink" title="jps的真正使用"></a>jps的真正使用</h3><p>查看jps的位置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ <span class="built_in">which</span> jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">/usr/java/jdk1.8.0_45/bin/jps</span></pre></td></tr></table></figure><p>查看jps命令帮助：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ jps -<span class="built_in">help</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">usage: jps [-<span class="built_in">help</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">       jps [-q] [-mlvV] [&lt;hostid&gt;]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Definitions:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    &lt;hostid&gt;:      &lt;hostname&gt;[:&lt;port&gt;]</span></pre></td></tr></table></figure><p>jps查看详细进程信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 hadoop]$ jps -l</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">4678 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">3768 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">3865 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">4378 org.apache.hadoop.hdfs.server.namenode.NameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">8077 sun.tools.jps.Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">4511 org.apache.hadoop.hdfs.server.datanode.DataNode</span></pre></td></tr></table></figure><p>查看相应进程的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># pwd</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">/tmp/hsperfdata_vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">total 160</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 3768</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 3865</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 4378</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 4511</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">-rw-------. 1 vinx vinx 32768 Dec 10 20:57 4678</span></pre></td></tr></table></figure><p>root用户下使用jps命令查看当前进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># jps</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">4678 -- process information unavailable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">3768 -- process information unavailable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">3865 -- process information unavailable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">4378 -- process information unavailable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">8124 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">4511 -- process information unavailable</span></pre></td></tr></table></figure><p>可以看到，如果是进程所属的用户vinx去执行jps命令，只会显示当前用户相关的进程信息。</p><p>而root用户可以看所有的进程信息，但是不是root用户启动的进程，会显示process information unavailable。</p><p>当root用户执行jps命令后，显示process information unavailable时，我们如果确定进程是否存在呢？可以使用下面的命令查看相关进程的详细信息，比如查看进程4678的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">会显示当前命令的进程，总共2个进程</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># ps -ef | grep 4678</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">过滤自己</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># ps -ef | grep 4678 | grep -v grep</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">进程数</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hsperfdata_vinx]<span class="comment"># ps -ef | grep 4678 | grep -v grep | wc -l</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">1</span></pre></td></tr></table></figure><h3 id="Linux的两个机制：OOM-Killer和-tmp目录定时清理"><a href="#Linux的两个机制：OOM-Killer和-tmp目录定时清理" class="headerlink" title="Linux的两个机制：OOM-Killer和/tmp目录定时清理"></a>Linux的两个机制：OOM-Killer和/tmp目录定时清理</h3><h4 id="OOM-Killer机制"><a href="#OOM-Killer机制" class="headerlink" title="OOM-Killer机制"></a>OOM-Killer机制</h4><p>OOM Killer的全称为Out of Memory (OOM) killer，它的作用简单点说就是，当系统的内存用光的时候，系统内核会自动的Kill掉一个或者一些进程，以使系统能继续的恢复到正常的运行状态。</p><p>查看机器的内存情况：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">free -m</span></pre></td></tr></table></figure><p>OOM Killer每一次Kill掉进程都会在messages日志里留下记录，检查的命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cat /var/<span class="built_in">log</span>/messages | grep oom</span></pre></td></tr></table></figure><p>部分系统时中日志里的关键字为：Out of memory，对应的检查命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cat /var/<span class="built_in">log</span>/messages | grep -i <span class="string">'Out of memory'</span></span></pre></td></tr></table></figure><p>如果日志里频繁的出现OOM记录，那么就需要考虑调整服务器配置了（软件或者硬件）。</p><p>解决办法：</p><ol><li>限制java进程的max heap，从而降低内存使用</li><li>发现系统没有开启swap，给系统加swap空间（内存数x2）</li></ol><h4 id="tmp自动清理机制"><a href="#tmp自动清理机制" class="headerlink" title="/tmp自动清理机制"></a>/tmp自动清理机制</h4><p>在Linux系统中/tmp文件夹里是存放临时文件的，/tmp默认存储周期 30天，会自动清空不在规则以内的文件。</p><p>/etc/cron.daily/tmpwatch这个文件的作用就是删除/tmp目录下不在规则内且没有访问的文件文件夹，文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# cat &#x2F;etc&#x2F;cron.daily&#x2F;tmpwatch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">#! &#x2F;bin&#x2F;sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">flags&#x3D;-umc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#x2F;usr&#x2F;sbin&#x2F;tmpwatch &quot;$flags&quot; -x &#x2F;tmp&#x2F;.X11-unix -x &#x2F;tmp&#x2F;.XIM-unix \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-x &#x2F;tmp&#x2F;.font-unix -x &#x2F;tmp&#x2F;.ICE-unix -x &#x2F;tmp&#x2F;.Test-unix \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">-X &#39;&#x2F;tmp&#x2F;hsperfdata_*&#39; 10d &#x2F;tmp</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#x2F;usr&#x2F;sbin&#x2F;tmpwatch &quot;$flags&quot; 30d &#x2F;var&#x2F;tmp</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">for d in &#x2F;var&#x2F;&#123;cache&#x2F;man,catman&#125;&#x2F;&#123;cat?,X11R6&#x2F;cat?,local&#x2F;cat?&#125;; do</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    if [ -d &quot;$d&quot; ]; then</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">&#x2F;usr&#x2F;sbin&#x2F;tmpwatch &quot;$flags&quot; -f 30d &quot;$d&quot;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    fi</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">done</span></pre></td></tr></table></figure><p>解释：</p><p>/usr/sbin/tmpwatch “$flags” 30d /var/tmp这一行的30d就决定了30天清理/tmp下不访问的文件。如果你想一天一清理的话，就把这个30d改成1d。</p><p>但有个问题要注意，如果你设置更短的时间来清理的话，比如说30分钟、10秒等，重启系统后你会发现设置无效，这是因为tmpwatch的上层目录是/etc/cron.daily/，而这个目录是一天执行一次计划任务，所以说，你设置了比一天更短的时间，他就不起作用了。</p><p>在Centos6中，系统自动清理/tmp文件夹的默认时限是30天</p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> YARN伪分布式 </tag>
            
            <tag> WordCount案例 </tag>
            
            <tag> jps命令 </tag>
            
            <tag> OOM-kill机制 </tag>
            
            <tag> /tmp定时清理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS伪分布式&amp;HDFS常用命令</title>
      <link href="/2018/03/27/dw/hadoop-1/"/>
      <url>/2018/03/27/dw/hadoop-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Hadoop概述"><a href="#Hadoop概述" class="headerlink" title="Hadoop概述"></a>Hadoop概述</h3><p>apache hadoop软件：</p><p>1.x 基本不用</p><p>2.x 企业主流 ==&gt; CDH5.X系列</p><p>3.x 尝试使用 ==&gt; CDH6.X系列</p><p>CDH的网址：</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a></p><p>Hadoop版本选择hadoop-2.6.0-cdh5.16.2：</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</a></p><p>选择CDH的好处：</p><p>不必考虑版本兼容性</p><h4 id="Hadoop的三大组件"><a href="#Hadoop的三大组件" class="headerlink" title="Hadoop的三大组件"></a>Hadoop的三大组件</h4><p>HDFS - 存储</p><p>MapReduce - 计算</p><p>YARN - 资源（内存、VCORE） + 作业调度</p><h3 id="HDFS伪分布式"><a href="#HDFS伪分布式" class="headerlink" title="HDFS伪分布式"></a>HDFS伪分布式</h3><p>官网：</p><p>Apache版本：</p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation</a></p><p>CDH版本：</p><p><a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/hadoop-project-dist/hadoop-common/SingleCluster.html</a></p><h4 id="伪分布式部署"><a href="#伪分布式部署" class="headerlink" title="伪分布式部署"></a>伪分布式部署</h4><p>Hadoop集群有三种模式，这里我们安装Hadoop的伪分布式模式。</p><ul><li>Local (Standalone) Mode 本地模式</li><li>Pseudo-Distributed Mode 伪分布式模式</li><li>Fully-Distributed Mode  分布式模式、集群模式</li></ul><p>环境要求：</p><ol><li>jdk 这里选择jdk1.8，需要在/etc/profile中全局配置JAVA_HOME</li><li>ssh 系统已默认安装（配置免密登录需要用到）</li></ol><p>使用root创建用户vinx：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">useradd vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">切换至用户vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">su - vinx</span></pre></td></tr></table></figure><p>上传压缩包并修改权限（root用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mv /tmp/hadoop-2.6.0-cdh5.16.2.tar.gz  /home/vinx/software/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">chown vinx:vinx /home/vinx/software/*</span></pre></td></tr></table></figure><p>解压（vinx用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/</span></pre></td></tr></table></figure><p>设置软连接：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ ln -s hadoop-2.6.0-cdh5.16.2 hadoop</span></pre></td></tr></table></figure><p>配置JAVA_HOME进hadoop-env.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi hadoop-env.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">更改：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.8.0_45</span></pre></td></tr></table></figure><p>修改etc/hadoop/core-site.xml - NN：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>修改etc/hadoop/hdfs-site.xml：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure><p>配置ssh免密码登录（vinx用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">$ cp .ssh .ssh_copy</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">$ ssh-keygen</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">然后一路回车</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> .ssh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">修正权限（普通用户）</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">$ chmod 0600 ~/.ssh/authorized_keys</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">验证</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">$ ssh hadoop001 date</span></pre></td></tr></table></figure><p>配置个人环境变量（vinx用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/ruoze/app/hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin:<span class="variable">$&#123;HADOOP_HOME&#125;</span>/sbin:<span class="variable">$PATH</span></span></pre></td></tr></table></figure><p>生效环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ <span class="built_in">source</span> .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ <span class="built_in">which</span> hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">~/app/hadoop/bin/hadoop</span></pre></td></tr></table></figure><p>-format格式化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hdfs namenode -format</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">... has been successfully formatted.</span></pre></td></tr></table></figure><p>配置/etc/hadoop/slaves - DN：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi slaves</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">hadoop001</span></pre></td></tr></table></figure><p>配置/etc/hadoop/hdfs-site.xml - SNN：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi hdfs-site.xml</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    &lt;value&gt;hadoop001:50090&lt;/value&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&lt;/property&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    &lt;value&gt;hadoop001:50091&lt;/value&gt;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">&lt;/property&gt;</span></pre></td></tr></table></figure><p>启动HDFS：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ start-dfs.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">151203 DataNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">151530 Jps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">150875 NameNode</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">151406 SecondaryNameNode</span></pre></td></tr></table></figure><p>查看HDFS的web页面：</p><p>网址：<a href="http://hadoop001:50070" target="_blank" rel="noopener">http://hadoop001:50070</a></p><p><img src="https://vinxikk.github.io/img/dw/hdfs-website.png" alt="HDFS的web界面"></p><p>停止HDFS：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ stop-dfs.sh</span></pre></td></tr></table></figure><h4 id="HDFS主从架构"><a href="#HDFS主从架构" class="headerlink" title="HDFS主从架构"></a>HDFS主从架构</h4><p>NameNode 名称节点，主节点，读写请求先经过它</p><p>DataNode 数据节点，从节点，存储数据，检索数据</p><p>SecondaryNameNode 第二名称节点，h+1 checkpoint检查点机制</p><p>大数据组件基本都是主从架构。</p><p>HBase（读写请求不经过HMaster进程）。</p><h3 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h3><p>hadoop fs == hdfs dfs</p><p>常用命令如下：</p><table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody><tr><td>hadoop fs -mkdir</td><td>创建文件夹</td></tr><tr><td>hadoop fs -put</td><td>上传文件至HDFS</td></tr><tr><td>hadoop fs -get</td><td>下载文件至Client</td></tr><tr><td>hadoop fs -cat</td><td>查看文件内容</td></tr><tr><td>hadoop fs -rm</td><td>删除文件</td></tr><tr><td>hadoop fs -ls</td><td>查看指定目录下的所有文件</td></tr></tbody></table><h3 id="JDK安装-amp-SSH-amp-etc-hosts文件"><a href="#JDK安装-amp-SSH-amp-etc-hosts文件" class="headerlink" title="JDK安装&amp;SSH&amp;/etc/hosts文件"></a>JDK安装&amp;SSH&amp;/etc/hosts文件</h3><h4 id="jdk1-8安装"><a href="#jdk1-8安装" class="headerlink" title="jdk1.8安装"></a>jdk1.8安装</h4><p>使用root用户安装，并全局配置JAVA_HOME（/etc/profile）。</p><p>创建文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mkdir /usr/java</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/java</span></pre></td></tr></table></figure><p>解压：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tar -xzvf jdk-8u45-linux-x64.gz</span></pre></td></tr></table></figure><p>修正权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">chown -R root:root jdk1.8.0_45</span></pre></td></tr></table></figure><p>配置环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">追加以下内容：</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.8.0_45</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span></pre></td></tr></table></figure><p>生效环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 java]<span class="comment"># source /etc/profile</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 java]<span class="comment"># which java</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">/usr/java/jdk1.8.0_45/bin/java</span></pre></td></tr></table></figure><h4 id="ssh配置600权限"><a href="#ssh配置600权限" class="headerlink" title="ssh配置600权限"></a>ssh配置600权限</h4><p>如果是普通用户，配置ssh免密登录时需要修改~/.ssh/authorized_keys的权限为600。</p><p><img src="https://vinxikk.github.io/img/dw/ssh-600.png" alt="SSH 600权限"></p><h4 id="etc-hosts不要删除前两行"><a href="#etc-hosts不要删除前两行" class="headerlink" title="/etc/hosts不要删除前两行"></a>/etc/hosts不要删除前两行</h4><p>/etc/hosts文件配置内网IP和hostname时，不要删除前两行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 java]<span class="comment"># cat /etc/hosts</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">192.168.xxx.xxx hadoop001</span></pre></td></tr></table></figure><p>查看内网IP：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">ifconfig</span></pre></td></tr></table></figure><hr><p>参考：</p><p>故障：<a href="http://blog.itpub.net/30089851/viewspace-2127102/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2127102/</a>  </p><p>ssh多台：<a href="http://blog.itpub.net/30089851/viewspace-1992210/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-1992210/</a>  </p>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS伪分布式 </tag>
            
            <tag> HDFS常用命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL的常用关键字：WHERE/GROUP BY/JOIN&amp;TopN问题</title>
      <link href="/2018/03/24/dw/sql-2/"/>
      <url>/2018/03/24/dw/sql-2/</url>
      
        <content type="html"><![CDATA[<h3 id="SQL-WHERE"><a href="#SQL-WHERE" class="headerlink" title="SQL - WHERE"></a>SQL - WHERE</h3><p>MySQL WHERE 子句的作用是有条件地从表中选取数据，常见于SELECT语句中，另外也可用于DELETE或UPDATE命令。</p><h4 id="WHERE在SELECT语句中"><a href="#WHERE在SELECT语句中" class="headerlink" title="WHERE在SELECT语句中"></a>WHERE在SELECT语句中</h4><p>假设表中有如下数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| wangwu   | product03 |     1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     5 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><ol><li><p>找出customer是zhangsan的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer='zhangsan';</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 搭配in</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer in('zhangsan');</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li><li><p>找出customer不是zhangsan的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer!='zhangsan';</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 搭配not in</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer not in('zhangsan');</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| wangwu   | product03 |     1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     5 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">3 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span></pre></td></tr></table></figure></li><li><p>给出所有购入商品为两种或两种以上的购物人的购物记录：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail` where customer in (select customer from `orders-detail` group by customer having count(*) &gt;= 2);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     5 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL拆分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">* </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="string">`orders-detail`</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">customer <span class="keyword">IN</span> ( <span class="keyword">SELECT</span> customer <span class="keyword">FROM</span> <span class="string">`orders-detail`</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> customer <span class="keyword">HAVING</span> <span class="keyword">count</span>(*) &gt;= <span class="number">2</span> );</span></pre></td></tr></table></figure></li></ol><h4 id="WHERE在UPDATE语句中"><a href="#WHERE在UPDATE语句中" class="headerlink" title="WHERE在UPDATE语句中"></a>WHERE在UPDATE语句中</h4><p>更新customer=lisi，且product=product04的count数量为6：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; update `orders-detail` set count=6 where customer='lisi' and product='product04';</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Query OK, 1 row affected (0.03 sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| wangwu   | product03 |     1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     6 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL拆分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> <span class="string">`orders-detail`</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> <span class="keyword">count</span> = <span class="number">6</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">customer = <span class="string">'lisi'</span> <span class="keyword">AND</span> product = <span class="string">'product04'</span>;</span></pre></td></tr></table></figure><h4 id="WHERE在DELETE语句中"><a href="#WHERE在DELETE语句中" class="headerlink" title="WHERE在DELETE语句中"></a>WHERE在DELETE语句中</h4><ol><li><p>删除customer为wangwu的记录：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; delete from `orders-detail` where customer='wangwu';</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Query OK, 1 row affected (0.02 sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| lisi     | product04 |     6 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 等价于如下SQL，搭配in</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="string">`orders-detail`</span> <span class="keyword">where</span> customer <span class="keyword">in</span> (<span class="string">'wangwu'</span>);</span></pre></td></tr></table></figure></li><li><p>删除count大于4的记录：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; delete from `orders-detail` where count &gt; 4;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Query OK, 1 row affected (0.10 sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `orders-detail`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| customer | product   | count |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product01 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| lisi     | product02 |     4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">| zhangsan | product04 |     2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----------+-----------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">3 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li></ol><h3 id="SQL-GROUP-BY"><a href="#SQL-GROUP-BY" class="headerlink" title="SQL - GROUP BY"></a>SQL - GROUP BY</h3><p>GROUP BY子句根据一个或者多个类对结果集进行分组。</p><p>在分组的列上我们可以使用COUNT, SUM, AVG等聚合函数。</p><p>表数据如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- date: 当日首次登录时间</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- login: 当日登录次数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from `employee_tbl`;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----+--------+---------------------+--------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| id | name   | date                | login  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----+--------+---------------------+--------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">|  1 | 小明   | 2016-04-22 15:25:33 |      1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">|  2 | 小王   | 2016-04-20 15:25:47 |      3 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|  3 | 小丽   | 2016-04-19 15:26:02 |      2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|  4 | 小王   | 2016-04-07 15:26:14 |      4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|  5 | 小明   | 2016-04-11 15:26:40 |      4 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|  6 | 小明   | 2016-04-04 15:26:54 |      2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">----+--------+---------------------+--------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><ol><li><p>按名字分组，并统计每人的记录数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select name,count(*) from `employee_tbl` group by name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">| name   | count(*) |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">| 小明   |        3 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 小王   |        2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 小丽   |        1 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+----------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">3 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li><li><p>按名字分组，并统计每个人登录的次数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- WITH ROLLUP可以实现在分组统计基础上，再进行相同的统计（SUM,AVG,COUNT...）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select name,sum(login) as login_count from `employee_tbl` group by name with rollup;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| name   | login_count  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| 小丽   |            2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| 小明   |            7 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">| 小王   |            7 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| NULL   |           16 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- NULL表示所有人的登录次数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 可以使用 coalesce 来设置一个可以取代NUll的名称，coalesce 语法：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">coalesce</span>(a,b,c);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 说明：如果a==null，则选择b；如果b==null，则选择c；如果a!=null，则选择a；如果abc都为null，则返回null</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 如下：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select coalesce(name, '总数'),sum(login) as login_count from `employee_tbl` group by name with rollup;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------------------------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">| coalesce(name, '总数')   | login_count  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------------------------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">| 小丽                     |            2 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">| 小明                     |            7 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">| 小王                     |            7 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">| 总数                     |           16 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">--------------------------+--------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure></li></ol><h3 id="SQL-JOIN"><a href="#SQL-JOIN" class="headerlink" title="SQL - JOIN"></a>SQL - JOIN</h3><p>MySQL中常用的JOIN有三种：left join, right join, inner join。</p><p>left join: 左连接，以左表为主，显示左表的全部记录；右表是匹配的，匹配不到就以NULL显示。</p><p>right join: 右连接，以右表为主，显示其全部记录；左表是匹配的，匹配不到就以NULL显示，正好与left join相反。</p><p>inner join: 内连接，根据某个字段关联，只显示左右表中都存在的记录。</p><p>具体见我的另一篇博客：</p><p>[MySQL的JOIN] <a href="https://vinxikk.github.io/https:/vinxikk.github.io/2017/05/15/mysql/mysql-join/">https://vinxikk.github.io/https:/vinxikk.github.io/2017/05/15/mysql/mysql-join/</a></p><h3 id="MySQL的TopN问题"><a href="#MySQL的TopN问题" class="headerlink" title="MySQL的TopN问题"></a>MySQL的TopN问题</h3><p>需要创建视图，将查询结果保存为视图，后续在视图中继续执行SQL查询。</p><p>可以参考：</p><p><a href="https://vinxikk.github.io/https:/vinxikk.github.io/2018/01/15/mysql/mysql-example-topn/">https://vinxikk.github.io/https:/vinxikk.github.io/2018/01/15/mysql/mysql-example-topn/</a></p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL-WHERE </tag>
            
            <tag> SQL-GROUP BY </tag>
            
            <tag> SQL-JOIN </tag>
            
            <tag> MySQL案例 </tag>
            
            <tag> MySQL-TopN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL权限</title>
      <link href="/2018/03/21/dw/sql-1/"/>
      <url>/2018/03/21/dw/sql-1/</url>
      
        <content type="html"><![CDATA[<h3 id="赋予用户MySQL权限"><a href="#赋予用户MySQL权限" class="headerlink" title="赋予用户MySQL权限"></a>赋予用户MySQL权限</h3><p>创建数据库：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> testdb;</span></pre></td></tr></table></figure><p>赋予权限：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> vinx@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'123456'</span>;</span></pre></td></tr></table></figure><p>刷新权限：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL权限 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL二进制部署&amp;DBeaver安装</title>
      <link href="/2018/03/20/dw/mysql-install/"/>
      <url>/2018/03/20/dw/mysql-install/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL二进制部署"><a href="#MySQL二进制部署" class="headerlink" title="MySQL二进制部署"></a>MySQL二进制部署</h3><p>生产上，一般：</p><p>mysql服务 ==&gt; mysql用户去维护</p><p>hadoop服务 ==&gt; hadoop用户</p><p>CDH的hdfs服务 ==&gt; hdfs用户</p><p>hbase服务 ==&gt; hbase用户</p><p>安装步骤：</p><ol><li><p>上传安装包（root用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传mysql-5.6.23的安装包至此目录中</span></span></pre></td></tr></table></figure></li><li><p>检查是否已安装了mysql（root用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看mysql进程</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">ps -ef | grep mysqld</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看mysql相关的rpm包</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">rpm -qa | grep -i mysql</span></pre></td></tr></table></figure></li><li><p>解压tar包（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tar xzvf mysql-5.6.23-linux-glibc2.5-x86_64.tar.gz</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置软连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ln -s mysql-5.6.23-linux-glibc2.5-x86_64 mysql</span></pre></td></tr></table></figure></li><li><p>新建用户和用户组（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">groupadd -g 101 dba</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">useradd -u 514 -g dba -G root -d /usr/<span class="built_in">local</span>/mysql mysqladmin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时su - mysqladmin，会出现样式丢失，执行以下命令修复问题：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">cp /etc/skel/.* /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时再次su - mysqladmin，可以看到修复了样式丢失的问题</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">id mysqladmin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改用户密码</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">passwd mysqladmin</span></pre></td></tr></table></figure></li><li><p>配置文件（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cp /etc/my.cnf /etc/my.cnf20180320</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">vi /etc/my.cnf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># gg dG清空文件，并拷贝如下内容：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[client]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">port            = 3306</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">socket          = /usr/<span class="built_in">local</span>/mysql/data/mysql.sock</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">[mysqld]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">port            = 3306</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">socket          = /usr/<span class="built_in">local</span>/mysql/data/mysql.sock</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">skip-external-locking</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">key_buffer_size = 256M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">sort_buffer_size = 2M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">read_buffer_size = 2M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">read_rnd_buffer_size = 4M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">query_cache_size= 32M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">max_allowed_packet = 16M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">myisam_sort_buffer_size=128M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">tmp_table_size=32M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">table_open_cache = 512</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">thread_cache_size = 8</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">wait_timeout = 86400</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">interactive_timeout = 86400</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">max_connections = 600</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Try number of CPU's*2 for thread_concurrency</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">thread_concurrency = 32</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#isolation level and default engine </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">default-storage-engine = INNODB</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">transaction-isolation = READ-COMMITTED</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">server-id  = 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">basedir     = /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">datadir     = /usr/<span class="built_in">local</span>/mysql/data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">pid-file     = /usr/<span class="built_in">local</span>/mysql/data/hostname.pid</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#open performance schema</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">log</span>-warnings</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">sysdate-is-now</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">binlog_format = MIXED</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">log_bin_trust_function_creators=1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">log</span>-error  = /usr/<span class="built_in">local</span>/mysql/data/hostname.err</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">log</span>-bin=/usr/<span class="built_in">local</span>/mysql/arch/mysql-bin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#other logs</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#general_log =1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#general_log_file  = /usr/local/mysql/data/general_log.err</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#slow_query_log=1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#slow_query_log_file=/usr/local/mysql/data/slow_log.err</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#for replication slave</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#log-slave-updates </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#sync_binlog = 1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#for innodb options </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">innodb_data_home_dir = /usr/<span class="built_in">local</span>/mysql/data/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">innodb_data_file_path = ibdata1:500M:autoextend</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">innodb_log_group_home_dir = /usr/<span class="built_in">local</span>/mysql/arch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">innodb_log_files_in_group = 2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">innodb_log_file_size = 200M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#product tuning</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">innodb_buffer_pool_size = 1024M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">innodb_additional_mem_pool_size = 50M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">innodb_log_buffer_size = 16M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">innodb_lock_wait_timeout = 100</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#innodb_thread_concurrency = 0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">innodb_flush_log_at_trx_commit = 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">innodb_locks_unsafe_for_binlog=1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#innodb io features: add for mysql5.5.8</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line">performance_schema</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line">innodb_read_io_threads=4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">innodb-write-io-threads=4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line">innodb-io-capacity=200</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#purge threads change default(0) to 1 for purge</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line">innodb_purge_threads=1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line">innodb_use_native_aio=on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#case-sensitive file names and separate tablespace</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line">innodb_file_per_table = 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line">lower_case_table_names=1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">[mysqldump]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">quick</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">91</span></pre></td><td class="code"><pre><span class="line">max_allowed_packet = 16M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">92</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">93</span></pre></td><td class="code"><pre><span class="line">[mysql]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">94</span></pre></td><td class="code"><pre><span class="line">no-auto-rehash</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">95</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">96</span></pre></td><td class="code"><pre><span class="line">[mysqlhotcopy]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">97</span></pre></td><td class="code"><pre><span class="line">interactive-timeout</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">98</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">99</span></pre></td><td class="code"><pre><span class="line">[myisamchk]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">100</span></pre></td><td class="code"><pre><span class="line">key_buffer_size = 256M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">101</span></pre></td><td class="code"><pre><span class="line">sort_buffer_size = 256M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">102</span></pre></td><td class="code"><pre><span class="line">read_buffer = 2M</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">103</span></pre></td><td class="code"><pre><span class="line">write_buffer = 2M</span></pre></td></tr></table></figure></li><li><p>修改权限（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">chown mysqladmin:dba /etc/my.cnf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">chmon 640 /etc/my.cnf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">chown -R mysqladmin:dba /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">chown -R mysqladmin:dba /usr/<span class="built_in">local</span>/mysql/*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">chown -R mysqladmin:dba /usr/<span class="built_in">local</span>/mysql-5.6.23-linux-glibc2.5-x86_64</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">chmod -R 755 /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">chmod -R 755 /usr/<span class="built_in">local</span>/mysql/*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">chmod -R 755 /usr/<span class="built_in">local</span>/mysql-5.6.23-linux-glibc2.5-x86_64</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">su - mysqladmin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 放binlog归档文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">$ mkdir arch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">$ scripts/mysql_install_db  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">--user=mysqladmin \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">--basedir=/usr/<span class="built_in">local</span>/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">--datadir=/usr/<span class="built_in">local</span>/mysql/data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#缺少libaio.so包</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">Installing MySQL system tables..../bin/mysqld: error <span class="keyword">while</span> loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># root用户安装缺少的包，然后再重新执行命令</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">yum install -y perl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">yum install -y autoconf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">yum install -y libaio</span></pre></td></tr></table></figure></li><li><p>配置mysql service and boot auto start（root）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">cp support-files/mysql.server /etc/rc.d/init.d/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">chmod +x /etc/rc.d/init.d/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">chkconfig --del mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">chkconfig --add mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">chkconfig --level 345 mysql on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">vi /etc/rc.local</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加以下内容：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">su - mysqladmin -c <span class="string">"/etc/init.d/mysql start --federated"</span></span></pre></td></tr></table></figure></li><li><p>启动mysql并查看进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">su - mysqladmin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">$ rm -rf my.cnf</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">$ mysqld_safe &amp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">$ service mysql start</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">$ service mysql status</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">$ mysql -uroot -p</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">$ ps -ef | grep mysqld</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">$ netstat -tulnp | grep mysql</span></pre></td></tr></table></figure></li></ol><h3 id="MySQL设置"><a href="#MySQL设置" class="headerlink" title="MySQL设置"></a>MySQL设置</h3><p>使用mysqladmin用户，mysql -uroot -p登录mysql，然后：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">password</span>,host <span class="keyword">from</span> <span class="keyword">user</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置root用户的密码</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">password</span>(<span class="string">'password'</span>) <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'root'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除空账号</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">''</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span></pre></td></tr></table></figure><p>设置环境变量（mysqladmin用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ vi .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加以下内容：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MYSQL_HOME=/usr/<span class="built_in">local</span>/mysql</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$MYSQL_HOME</span>/bin:<span class="variable">$PATH</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">PS1=`uname -n`<span class="string">":"</span><span class="string">'$USER'</span><span class="string">":"</span><span class="string">'$PWD'</span><span class="string">":&gt;"</span>; <span class="built_in">export</span> PS1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生效环境变量</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看环境变量是否生效</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="variable">$MYSQL_HOME</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">which</span> mysql</span></pre></td></tr></table></figure><h3 id="重新部署"><a href="#重新部署" class="headerlink" title="重新部署"></a>重新部署</h3><p>假如部署出了问题，需要重新部署（mysqladmin用户）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">rm -rf arch/*  <span class="comment">#binlog文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">rm -rf data/*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">scripts/mysql_install_db  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--user=mysqladmin \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--basedir=/usr/<span class="built_in">local</span>/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--datadir=/usr/<span class="built_in">local</span>/mysql/data</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">service mysql start</span></pre></td></tr></table></figure><h3 id="DBeaver部署"><a href="#DBeaver部署" class="headerlink" title="DBeaver部署"></a>DBeaver部署</h3><p>下载社区版：<a href="https://dbeaver.io/download/" target="_blank" rel="noopener">https://dbeaver.io/download/</a></p><p>需要先安装JDK1.8，然后下载社区版安装即可（可能需要安装connector）。</p><p>配置权限：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 赋予root用户所有数据库的权限</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- % 任意ip访问</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> root@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'xxx'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 刷新权限</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span></pre></td></tr></table></figure><hr><p>参考：</p><p><a href="https://github.com/Hackeruncle/mysql" target="_blank" rel="noopener">https://github.com/Hackeruncle/mysql</a>  </p><p><a href="https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt" target="_blank" rel="noopener">https://github.com/Hackeruncle/MySQL/blob/master/MySQL%205.6.23%20Install.txt</a></p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL部署 </tag>
            
            <tag> DBeaver安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux软连接&amp;crontab使用&amp;rundeck部署</title>
      <link href="/2018/03/20/dw/linux-5/"/>
      <url>/2018/03/20/dw/linux-5/</url>
      
        <content type="html"><![CDATA[<h3 id="crontab使用"><a href="#crontab使用" class="headerlink" title="crontab使用"></a>crontab使用</h3><h4 id="shell脚本编写"><a href="#shell脚本编写" class="headerlink" title="shell脚本编写"></a>shell脚本编写</h4><p>新建test.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi test.sh</span></pre></td></tr></table></figure><p>编辑以下内容并保存：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">date</span></pre></td></tr></table></figure><p>此时，如果直接执行，会提示没有权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll ./test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--. 1 root root 18 Dec 14 20:31 ./test.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ./test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-bash: ./test.sh: Permission denied</span></pre></td></tr></table></figure><p>修改权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># chmod 744 test.sh </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll ./test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">-rwxr--r--. 1 root root 18 Dec 14 20:31 ./test.sh</span></pre></td></tr></table></figure><p>执行脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ./test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:37:18 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 或者</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># sh test.sh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:38:21 CST 2017</span></pre></td></tr></table></figure><h4 id="crontab编写"><a href="#crontab编写" class="headerlink" title="crontab编写"></a>crontab编写</h4><p>crontab在线工具：</p><p><a href="https://tool.lu/crontab" target="_blank" rel="noopener">https://tool.lu/crontab</a></p><p>查看crontab命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># crontab --help</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">crontab: invalid option -- <span class="string">'-'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">crontab: usage error: unrecognized option</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">usage:crontab [-u user] file</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">crontab [-u user] [ -e | -l | -r ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(default operation is replace, per 1003.2)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-e(edit user<span class="string">'s crontab)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="string">-l(list user'</span>s crontab)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">-r(delete user<span class="string">'s crontab)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">-i(prompt before deleting user'</span>s crontab)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">-s(selinux context)</span></pre></td></tr></table></figure><p>编辑crontab：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">crontab -e</span></pre></td></tr></table></figure><p>填写如下内容并保存：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">* * * * * &#x2F;root&#x2F;test.sh &gt;&gt; &#x2F;root&#x2F;test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">分 时 日 周 月</span></pre></td></tr></table></figure><p>监控日志文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># tail -F test.log</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:50:02 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:51:01 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:52:01 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 20:53:01 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">...</span></pre></td></tr></table></figure><h4 id="crontab每隔10s执行一次"><a href="#crontab每隔10s执行一次" class="headerlink" title="crontab每隔10s执行一次"></a>crontab每隔10s执行一次</h4><p>新建shell脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi test.sh</span></pre></td></tr></table></figure><p>编写以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">for((i&#x3D;1;i&lt;&#x3D;6;i++));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">do</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        date</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        sleep 10s</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">done</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">exit</span></pre></td></tr></table></figure><p>编辑crontab：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">crontab -e</span></pre></td></tr></table></figure><p>每隔1分钟执行一次：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">* * * * * &#x2F;root&#x2F;test.sh &gt;&gt; &#x2F;root&#x2F;test.log</span></pre></td></tr></table></figure><p>监控日志文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># tail -F test.log </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:05:02 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:01 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:11 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:21 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:31 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">Sat Dec 14 21:06:41 CST 2017</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">...</span></pre></td></tr></table></figure><h4 id="脚本后台执行"><a href="#脚本后台执行" class="headerlink" title="脚本后台执行"></a>脚本后台执行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">./test.sh &amp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">nohup ./test.sh &amp;  手动启动脚本，看日志，开发维护、测试</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">nohup ./test.sh &gt; /root/test.log 2&gt;&amp;1 &amp;  生产上</span></pre></td></tr></table></figure><h3 id="Linux软连接"><a href="#Linux软连接" class="headerlink" title="Linux软连接"></a>Linux软连接</h3><p>应用场景：</p><ol><li>软件版本升级</li><li>日志转移：系统盘 ==&gt; 数据盘</li></ol><h4 id="软件版本升级"><a href="#软件版本升级" class="headerlink" title="软件版本升级"></a>软件版本升级</h4><p>比如原来的mysql5.6需要升级到5.7，如果设置了软连接：</p><p>ln -s mysql5.6 mysql</p><p>此时只要删除mysql5.6，安装上5.7，并且重新设置软连接即可：</p><p>ln -s mysql5.7 mysql</p><h4 id="日志转移"><a href="#日志转移" class="headerlink" title="日志转移"></a>日志转移</h4><p>假设：</p><p>系统盘 / 50G</p><p>数据盘 /data01 2T</p><p>一般CDH log:</p><p>​    /var/log/hbase/xxx01.log 1G</p><p>​    /var/log/hbase/xxx02.log 1G</p><p>​    …</p><p>​    /var/log/hbase/xxx10.log 1G</p><p>此时系统盘中的日志需要转移到数据盘，那么：</p><p>​    mkdir /data01/log/</p><p>​    mv /var/log/hbase /data01/log/</p><p>​    ln -s /data01/log/hbase /var/log/hbase</p><p>但是，需要注意权限的问题。</p><h3 id="rundeck部署"><a href="#rundeck部署" class="headerlink" title="rundeck部署"></a>rundeck部署</h3><p>官网：</p><p><a href="https://www.rundeck.com/" target="_blank" rel="noopener">https://www.rundeck.com/</a></p><p>官网下载页（选择社区版）：</p><p><a href="https://www.rundeck.com/open-source/download" target="_blank" rel="noopener">https://www.rundeck.com/open-source/download</a></p><p>环境要求：</p><p>JDK1.8</p><h4 id="运行步骤"><a href="#运行步骤" class="headerlink" title="运行步骤"></a>运行步骤</h4><p>启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">nohup java -jar rundeck-launcher-2.10.1.jar &amp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">tail -F nohup.out</span></pre></td></tr></table></figure><p>web界面：</p><p><a href="https://hadoop001:4440" target="_blank" rel="noopener">https://hadoop001:4440</a></p><p>默认用户和密码都是admin</p><p>新建项目：</p><p><img src="https://vinxikk.github.io/img/dw/rundeck-create-project.png" alt="rundeck新建项目"></p><p>填写项目名称，其他保持默认，然后点击Create。</p><p><img src="https://vinxikk.github.io/img/dw/rundeck-save.png" alt="rundeck保存项目"></p><p>选择默认配置，点击Save。</p><p>新建Job：</p><p><img src="https://vinxikk.github.io/img/dw/rundeck-new-job.png" alt="rundeck新建Job"></p><p>进入testpro项目，在右上角点击Create Job即可新建Job。</p><p>停止：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">ps -ef | grep rundeck</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 pid</span></pre></td></tr></table></figure><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux软连接 </tag>
            
            <tag> crontab </tag>
            
            <tag> rundeck </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux vi&amp;进程和端口号&amp;连接拒绝&amp;高危命令&amp;安装卸载和压缩解压</title>
      <link href="/2018/03/17/dw/linux-4/"/>
      <url>/2018/03/17/dw/linux-4/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux-vi"><a href="#Linux-vi" class="headerlink" title="Linux vi"></a>Linux vi</h3><p><img src="https://vinxikk.github.io/img/linux/vi.png" alt="Linux vi的三种模式"></p><p>编辑文件的流程：</p><ol><li>vi test.log新建并打开一个文件，此时按i进入编辑模式（或者按o跳到光标下一行，并进入编辑模式）</li><li>编辑文件后，按esc退出编辑模式，然后shift+:进入尾行模式，输入wq后回车，保存退出</li><li>此时可以cat test.log查看编辑后的文件内容</li></ol><h4 id="尾行模式常用命令"><a href="#尾行模式常用命令" class="headerlink" title="尾行模式常用命令"></a>尾行模式常用命令</h4><ul><li>:/ERROR 回车，会定位为ERROR的位置，按N键寻找下一个</li><li>:set nu  显示行号</li><li>:set nonu  不显示行号</li></ul><h4 id="命令模式常用命令"><a href="#命令模式常用命令" class="headerlink" title="命令模式常用命令"></a>命令模式常用命令</h4><ul><li>dd  删除当前行</li><li>dG  删除光标所在行和以下的所有行</li><li>ndd  删除光标所在及以下的n行</li><li>gg  跳转到第一行的第一个字母</li><li>G  跳转到最后一行的第一个字母</li><li>shift + $  行尾</li><li>gg + dG  清空文件内容</li></ul><h4 id="编辑配置文件注意备份"><a href="#编辑配置文件注意备份" class="headerlink" title="编辑配置文件注意备份"></a>编辑配置文件注意备份</h4><p>在编辑配置文件的时候，需要将以前的配置文件备份，再添加新的配置文件，以防不测。</p><p>比如原来的配置文件core-site.xml需要修改其中的参数，可以先备份再编辑：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cp core-site.xml core-site.xml20180317</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">vi core-site.xml</span></pre></td></tr></table></figure><p>这样当想要回退配置文件的时候，可以及时找到。</p><h3 id="查看系统资源"><a href="#查看系统资源" class="headerlink" title="查看系统资源"></a>查看系统资源</h3><p>查看磁盘：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># df -h</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">/dev/sda3        38G  7.6G   28G  22% /</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">tmpfs          1000M   80K 1000M   1% /dev/shm</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">/dev/sda1       194M   34M  151M  19% /boot</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">/dev/sr0        4.2G  4.2G     0 100% /media/CentOS_6.5_Final</span></pre></td></tr></table></figure><p>查看内存和CPU：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># free -m</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">             total       used       free     shared    buffers     cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Mem:          1998       1365        633          0         59        700</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-/+ buffers/cache:        605       1393</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Swap:         2047          0       2047</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># free -g</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">             total       used       free     shared    buffers     cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">Mem:             1          1          0          0          0          0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">-/+ buffers/cache:          0          1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">Swap:            1          0          1</span></pre></td></tr></table></figure><p>查看系统负载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># top</span></span></pre></td></tr></table></figure><h3 id="查看进程和端口号"><a href="#查看进程和端口号" class="headerlink" title="查看进程和端口号"></a>查看进程和端口号</h3><p>查看所有进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ps -ef</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root         1     0  0 20:28 ?        00:00:02 /sbin/init</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root         2     0  0 20:28 ?        00:00:00 [kthreadd]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">root         3     2  0 20:28 ?        00:00:00 [migration/0]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">root         4     2  0 20:28 ?        00:00:00 [ksoftirqd/0]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">root         5     2  0 20:28 ?        00:00:00 [migration/0]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">root         6     2  0 20:28 ?        00:00:00 [watchdog/0]</span></pre></td></tr></table></figure><p>查看ssh相关的进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ps -ef | grep ssh</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root      1455     1  0 20:28 ?        00:00:00 /usr/sbin/sshd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root      3655  1455  0 20:30 ?        00:00:01 sshd: root@pts/0 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root      4558  1455  0 22:06 ?        00:00:04 sshd: root@notty </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">root      4562  4558  0 22:06 ?        00:00:02 /usr/libexec/openssh/sftp-server</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">root      5239  3659  0 23:50 pts/0    00:00:00 grep ssh</span></pre></td></tr></table></figure><p>查看ssh相关进程并过滤当前查找进程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ps -ef | grep ssh | grep -v grep</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root      1455     1  0 20:28 ?        00:00:00 /usr/sbin/sshd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root      3655  1455  0 20:30 ?        00:00:01 sshd: root@pts/0 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root      4558  1455  0 22:06 ?        00:00:04 sshd: root@notty </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">root      4562  4558  0 22:06 ?        00:00:02 /usr/libexec/openssh/sftp-server</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">进程用户 进程pid 父id                             进程的内容（进程所属的目录，<span class="built_in">log</span>, -Xmx, -Xms）</span></pre></td></tr></table></figure><p>查看指定进程的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># netstat -nlp | grep 1455</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      1455/sshd           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::22                       :::*                        LISTEN      1455/sshd</span></pre></td></tr></table></figure><h3 id="Connection-refused问题"><a href="#Connection-refused问题" class="headerlink" title="Connection refused问题"></a>Connection refused问题</h3><p>场景：Centos部署大数据组件时，发生错误：Connection refused。</p><p>解决方法：</p><ol><li><p>ping ip</p></li><li><p>telnet ip port</p></li><li><p>检查防火墙</p></li></ol><p>常见问题：如何登录A服务器，打开xxx软件的web界面？</p><p>解答：寻找ip和端口号</p><p>ps -ef | grep xxx</p><p>netstat -nlp | grep pid ==&gt; port</p><h3 id="高危命令"><a href="#高危命令" class="headerlink" title="高危命令"></a>高危命令</h3><h4 id="删除命令"><a href="#删除命令" class="headerlink" title="删除命令"></a>删除命令</h4><p>在生产中要谨慎使用rm -rf，一不小心就是删库到跑路。</p><h4 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><p>一般先做备份：cp core-site.xml core-site.xml20180317</p><p>然后再编辑：vi core-site.xml</p><h4 id="杀掉进程"><a href="#杀掉进程" class="headerlink" title="杀掉进程"></a>杀掉进程</h4><p>杀进程之前，先ps -ef找到相关的进程，搞清楚哪些是要杀的，不然造成生产事故。</p><p>相关命令：</p><ul><li>ps -ef | grep keyword  寻找keyword相关进程</li><li>echo $(pgrep -f keyword)  显示相关进程的进程号</li><li>kill -9 pid  杀掉进程号为pid的进程</li><li>kill -9 $(pgrep -f keyword)  杀掉keyword相关的所有进程</li></ul><h3 id="安装卸载和压缩解压"><a href="#安装卸载和压缩解压" class="headerlink" title="安装卸载和压缩解压"></a>安装卸载和压缩解压</h3><p>yum相关命令：</p><ul><li><p>yum search xxx  搜索</p></li><li><p>yum install -y xxx  安装</p></li><li><p>yum remove -y xxx  卸载</p></li><li><p>yum –help  命令帮助</p></li><li><p>man yum  命令帮助</p></li></ul><p>rpm命令：</p><ul><li>rpm -qa | grep http  搜索</li><li>rpm -e httpd-2.4.6-90.el7.centos.x86_64  卸载</li><li>rpm -e –nodeps httpd-tools-2.4.6-90.el7.centos.x86_64  不校验，直接删除</li></ul><p>wget下载安装包：</p><ul><li>wget <a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2.tar.gz</a>  下载到当前目录</li></ul><p>压缩解压：</p><ul><li>yum install -y zip unzip  安装zip和unzip</li><li>zip -r result.zip ./*  压缩当前目录下的所有文件进result.zip</li><li>zip -r result.zip data/*  压缩data目录下的所有文件进result.zip</li><li>unzip result.zip  解压result.zip</li><li>tar -xzvf hadoop-2.6.0-cdh5.16.2.tar.gz  解压</li><li>tar -czvf hadoop-2.6.0-cdh5.16.2.tar.gz  hadoop-2.6.0-cdh5.16.2/*  压缩</li></ul><p>命令帮助tar –help:<br>  tar -cf archive.tar foo bar  # Create archive.tar from files foo and bar.<br>  tar -tvf archive.tar         # List all files in archive.tar verbosely.<br>  tar -xf archive.tar          # Extract all files from archive.tar.</p><hr><p>参考：</p><p><a href="http://blog.itpub.net/30089851/viewspace-2131678/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2131678/</a></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux常用命令 </tag>
            
            <tag> Linux vi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux用户用户组&amp;/etc/passwd文件&amp;权限拒绝</title>
      <link href="/2018/03/16/dw/linux-3/"/>
      <url>/2018/03/16/dw/linux-3/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux用户用户组"><a href="#Linux用户用户组" class="headerlink" title="Linux用户用户组"></a>Linux用户用户组</h3><p>相关命令：</p><ul><li>ll /usr/sbin/user*  查看用户相关命令</li><li>ll /usr/sbin/group*  查看用户组相关命令</li><li>useradd vinx  创建普通用户vinx，默认创建同名用户组vinx，且设置这个用户主组为vinx</li><li>id vinx  查看用户vinx相关信息</li><li>userdel vinx  删除用户vinx</li></ul><h4 id="切换用户丢失样式"><a href="#切换用户丢失样式" class="headerlink" title="切换用户丢失样式"></a>切换用户丢失样式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟样式丢失</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># ll -a</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">total 12</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">drwx------  2 vinx vinx  59 Nov 16 21:16 .</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx  18 Apr 11  2018 .bash_logout</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx 193 Apr 11  2018 .bash_profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx 231 Apr 11  2018 .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># rm -rf .bash*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su  - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Last login: Sat Nov 16 21:29:09 CST 2017 on pts/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">-bash-4.2$ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">-bash-4.2$ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修正样式</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># cp /etc/skel/.* /home/vinx/</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">cp: omitting directory ‘/etc/skel/.’</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">cp: omitting directory ‘/etc/skel/..’</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># ll -a</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">total 12</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">drwx------  2 vinx vinx  59 Nov 16 21:32 .</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root  root   18 Nov 16 21:32 .bash_logout</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root  root  193 Nov 16 21:32 .bash_profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root  root  231 Nov 16 21:32 .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># chown vinx:vinx .bash*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># ll -a</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">total 12</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">drwx------  2 vinx vinx  59 Nov 16 21:32 .</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x. 5 root  root   44 Nov 16 21:16 ..</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx  18 Nov 16 21:32 .bash_logout</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx 193 Nov 16 21:32 .bash_profile</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 vinx vinx 231 Nov 16 21:32 .bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">Last login: Sat Nov 16 21:30:23 CST 2017 on pts/2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$</span></pre></td></tr></table></figure><h4 id="修改用户组"><a href="#修改用户组" class="headerlink" title="修改用户组"></a>修改用户组</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">添加vinx用户到另外一个组 bigdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># groupadd bigdata</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># cat /etc/group |grep bigdata</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">bigdata:x:1003:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># usermod -a -G bigdata vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">uid=1002(vinx) gid=1002(vinx) groups=1002(vinx),1003(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">修改bigdata为vinx的主组</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># usermod -a -G bigdata vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">uid=1002(vinx) gid=1002(vinx) groups=1002(vinx),1003(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># usermod --gid  bigdata vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">uid=1002(vinx) gid=1003(bigdata) groups=1003(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># usermod -a -G vinx vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">uid=1002(vinx) gid=1003(bigdata) groups=1003(bigdata),1002(vinx)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment">#</span></span></pre></td></tr></table></figure><h4 id="设置密码"><a href="#设置密码" class="headerlink" title="设置密码"></a>设置密码</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment"># passwd vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Changing password <span class="keyword">for</span> user vinx.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">New password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">BAD PASSWORD: The password is shorter than 8 characters</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Retype new password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">passwd: all authentication tokens updated successfully.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment">#</span></span></pre></td></tr></table></figure><h4 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h4><p>相关命令：</p><ul><li><p>su vinx</p></li><li><p>su - vinx  切换至该用户的家目录，且执行环境变量文件</p></li></ul><p>.bash_profile文件  su vinx不会执行，su - vinx 执行<br>.bashrc文件       su vinx和su - vinx都会执行</p><h4 id="普通用户获取root的最大权限"><a href="#普通用户获取root的最大权限" class="headerlink" title="普通用户获取root的最大权限"></a>普通用户获取root的最大权限</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改以下内容</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">vinx ALL=(root) NOPASSWD:ALL</span></pre></td></tr></table></figure><p>root用户创建文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># echo "www.example.com" &gt;&gt; test.log</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 18 Nov 16 21:58 test.log</span></pre></td></tr></table></figure><p>vinx用户验证sudo命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 root]$ sudo ls -l</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 18 Nov 16 21:58 test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 root]$ ls -l</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">ls: cannot open directory .: Permission denied</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 root]$ cat test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">cat: test.log: Permission denied</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 root]$ sudo cat test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">www.example.com</span></pre></td></tr></table></figure><h4 id="etc-profile和-bash-profile区别"><a href="#etc-profile和-bash-profile区别" class="headerlink" title="/etc/profile和~/.bash_profile区别"></a>/etc/profile和~/.bash_profile区别</h4><p><code>/etc/profile</code>：</p><p>为系统的每个用户设置环境信息和启动程序，当用户第一次登录时，该文件被执行，其配置对所有登录的用户都有效。当被修改时，重启或使用命令<code>source /etc/profile</code>才会生效。英文描述：System wide environment and startup programs, for login setup.</p><p><code>~/.bash_profile</code>：</p><p>为当前用户设置专属的环境信息和启动程序，当用户登录时该文件执行一次。默认情况下，它用于设置环境变量，并执行当前用户的<code>.bashrc</code>文件。理念类似于<code>/etc/profile</code>，只不过只对当前用户有效，需要重启或使用命令<code>source ~/.bash_profile</code>才能生效。（注意：Centos7系统命名为<code>.bash_profile</code>，其他系统可能是<code>.bash_login</code>或<code>.profile</code>）</p><h3 id="etc-passwd文件"><a href="#etc-passwd文件" class="headerlink" title="/etc/passwd文件"></a>/etc/passwd文件</h3><p>查看/etc/passwd文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi /etc/passwd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定位到vinx用户一行</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">vinx:x:1002:1003::/home/vinx:/bin/bash</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># /bin/bash解释器</span></span></pre></td></tr></table></figure><p>假如修改/etc/passwd文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vinx:x:1002:1003::/home/vinx:/bin/<span class="literal">false</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Last login: Sat Nov 16 21:57:32 CST 2017 on pts/0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">vinx:x:1002:1003::/home/vinx:/sbin/nologin</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">Last login: Sat Nov 16 22:08:52 CST 2017 on pts/0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">This account is currently not available.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment">#</span></span></pre></td></tr></table></figure><p>此时会发现切换用户失败。</p><p>场景：CDH中有很多组件，一般每一个组件都会创建一个相应的用户去管理，比如：</p><p>hdfs组件 ==&gt; hdfs用户</p><p>yarn组件 ==&gt; yarn用户</p><p>hbase组件 ==&gt; hbase用户</p><p>假如在切换用户中，发现切换不过去，需要修改/etc/passwd文件为/bin/bash。</p><h3 id="Permission-denied问题"><a href="#Permission-denied问题" class="headerlink" title="Permission denied问题"></a>Permission denied问题</h3><p>查看文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x 2 root root  6 Nov 16 22:15 testdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 root root 18 Nov 16 21:58 test.log</span></pre></td></tr></table></figure><p>-rw-r–r–</p><p>第一个字母：d文件夹，-文件，l连接</p><p>rw- r– r–</p><p>r: read 4</p><p>w: write 2</p><p>x: 执行 1</p><p>-: 没权限 0</p><p>7=rwx</p><p>5=r-x</p><p>3=-wx</p><p>rwx 第一组，7，代表该文件所属用户的权限，读写执行</p><p>r-x 第二组，5，代表该文件所属用户组的权限，读执行</p><p>r-x 第三组，5，代表其他组用户对该文件的权限，读执行</p><p>修改文件所属用户用户组和权限：</p><ul><li>chmod -R 777 test.log  修改文件权限，777代表任意的用户用户组，都有读写执行的权限</li><li>chown -R 用户:用户组 test.log  修改文件所属用户和用户组</li></ul><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><p>查看文件大小：</p><ul><li>ll 列出文件信息，相当于ls -l</li><li>du -sh test.log  查看文件大小</li><li>du -sh test  查看文件夹大小</li></ul><p>查找文件：</p><ul><li><code>find / -name &#39;*hadoop*&#39;</code>  从根目录开始查找包含hadoop的文件</li><li><code>find /usr/local - name &#39;*hadoop*&#39;</code>  指定目录查找包含hadoop的文件</li></ul><p>查看历史命令：</p><ul><li>history</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux常用命令 </tag>
            
            <tag> Linux用户用户组 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux环境变量&amp;alias别名&amp;history和rm -rf</title>
      <link href="/2018/03/13/dw/linux-2/"/>
      <url>/2018/03/13/dw/linux-2/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux环境变量"><a href="#Linux环境变量" class="headerlink" title="Linux环境变量"></a>Linux环境变量</h3><p>全局环境变量：/etc/profile，所有用户都可以使用。</p><p>个人环境变量有两个：<code>~/.bash_profile</code>和<code>~/.bashrc</code>，只供所属用户使用</p><p>生效环境变量文件：<code>source ./bashrc</code>或者<code>. ~/.bashrc</code></p><p>查看环境变量是否生效：<code>which java</code></p><p>打印环境变量：<code>echo $PATH</code></p><p>一般生效环境变量文件之后，习惯性的会which一下，看环境变量是否生效。</p><p><strong>command not found问题：</strong></p><p>一般出现command not found，可以which看看，然后查看软件是否安装，环境变量是否已配置</p><h3 id="alias别名"><a href="#alias别名" class="headerlink" title="alias别名"></a>alias别名</h3><p>命令的alias别名可以简化我们的操作。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># alias</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> cp=<span class="string">'cp -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> egrep=<span class="string">'egrep --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> fgrep=<span class="string">'fgrep --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> grep=<span class="string">'grep --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> l.=<span class="string">'ls -d .* --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ll=<span class="string">'ls -l --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ls=<span class="string">'ls --color=auto'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> mv=<span class="string">'mv -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> rm=<span class="string">'rm -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> <span class="built_in">which</span>=<span class="string">'alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment">#</span></span></pre></td></tr></table></figure><p>编辑.bashrc：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># vi .bashrc</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># .bashrc</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># User specific aliases and functions</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> rm=<span class="string">'rm -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> cp=<span class="string">'cp -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> mv=<span class="string">'mv -i'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Source global definitions</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ -f /etc/bashrc ]; <span class="keyword">then</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        . /etc/bashrc</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">fi</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加以下内容</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> j=<span class="string">'cd /home/vinx'</span></span></pre></td></tr></table></figure><p>生效环境变量文件并验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># source .bashrc</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># v</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 vinx]<span class="comment">#</span></span></pre></td></tr></table></figure><h3 id="history命令"><a href="#history命令" class="headerlink" title="history命令"></a>history命令</h3><p>history可以查看历史记录的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># history</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    1  <span class="built_in">history</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    2  ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    3  ls</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    4  <span class="built_in">which</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    5  <span class="built_in">history</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># !2   #执行历史指定的命令</span></span></pre></td></tr></table></figure><h3 id="删除命令"><a href="#删除命令" class="headerlink" title="删除命令"></a>删除命令</h3><p>生成新文件：</p><ul><li>touch test.log</li><li>cat /dev/null &gt; test.log</li><li>vi test.log</li></ul><p>删除文件：</p><ul><li>rm -f test.log  删除文件</li><li>rm -rf test  删除文件夹</li></ul><p><code>rm -rf</code>是高危命令，但在有些场景中又必须使用，比较安全的做法是设置回收站。</p><p>在shell脚本中也可以使用<code>rm -rf</code></p><p>比如shell脚本中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">K='/home/vinx/data'</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">判断$K是否存在</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">rm -rf $K/*</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux常用命令 </tag>
            
            <tag> Linux环境变量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux用户和用户组</title>
      <link href="/2018/03/11/linux/linux-user-group/"/>
      <url>/2018/03/11/linux/linux-user-group/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux用户和用户组"><a href="#Linux用户和用户组" class="headerlink" title="Linux用户和用户组"></a>Linux用户和用户组</h3><hr><h4 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 查看关于user的命令</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# ll &#x2F;usr&#x2F;sbin&#x2F;user*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 118192 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;useradd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root  80360 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;userdel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-rws--x--x. 1 root root  40312 Jun 10  2014 &#x2F;usr&#x2F;sbin&#x2F;userhelper</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 113840 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;usermod</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rwsr-xr-x. 1 root root  11288 Aug  4  2017 &#x2F;usr&#x2F;sbin&#x2F;usernetctl</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"># 查看关于group的命令</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# ll &#x2F;usr&#x2F;sbin&#x2F;group*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 65480 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;groupadd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 57016 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;groupdel</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 57064 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;groupmems</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">-rwxr-x---. 1 root root 76424 Nov  6  2016 &#x2F;usr&#x2F;sbin&#x2F;groupmod</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"># 添加用户vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# useradd vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">uid&#x3D;1001(vinx) gid&#x3D;1001(vinx) groups&#x3D;1001(vinx)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">释义：创建一个普通用户vinx，默认创建用户的用户组vinx，且设置用户的主组为vinx，并且创建&#x2F;home&#x2F;vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"># 查看linux上的所有普通用户</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]# cd &#x2F;home</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]# ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">drwx------. 23 hadoop  hadoop  4096 Nov 24 04:02 hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">drwx------   3 hadoop2 hadoop2   78 Nov 24 04:26 hadoop2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">drwx------   8 vinx    vinx     221 Nov 22 04:27 vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"># 添加vinx用户到另外一个组bigdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# groupadd bigdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# cat &#x2F;etc&#x2F;group | grep bigdata</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">bigdata:x:1004:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# usermod -a -G bigdata vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">uid&#x3D;1003(vinx) gid&#x3D;1003(vinx) groups&#x3D;1003(vinx),1004(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"># 修改bigdata为vinx的主组</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# usermod --gid bigdata vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">uid&#x3D;1003(vinx) gid&#x3D;1004(bigdata) groups&#x3D;1004(bigdata)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# usermod -a -G vinx vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]# id vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">uid&#x3D;1003(vinx) gid&#x3D;1004(bigdata) groups&#x3D;1004(bigdata),1003(vinx)</span></pre></td></tr></table></figure><h4 id="删除用户"><a href="#删除用户" class="headerlink" title="删除用户"></a>删除用户</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">root@hadoop000 home]<span class="comment"># userdel vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">id: vinx: no such user</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># cat /etc/passwd | grep vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># cat /etc/group | grep vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">因为vinx用户组只有vinx用户，当这个用户删除时，组会校验就它自己，会自动删除</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">但是/home下仍然有vinx文件夹</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">drwx------. 23 hadoop  hadoop  4096 Nov 24 04:02 hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">drwx------   3 hadoop2 hadoop2   78 Nov 24 04:26 hadoop2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">drwx------   8    1001    1001  221 Nov 22 04:27 vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新创建用户vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># useradd vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">useradd: warning: the home directory already exists.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">Not copying any file from skel directory into it.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">Creating mailbox file: File exists</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># id vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">uid=1003(vinx) gid=1003(vinx) groups=1003(vinx)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># ll</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">total 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">drwx------. 23 hadoop  hadoop  4096 Nov 24 04:02 hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">drwx------   3 hadoop2 hadoop2   78 Nov 24 04:26 hadoop2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">drwx------   8    1001    1001  221 Nov 22 04:27 vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此时切换至vinx用户时</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">Last failed login: Sun Nov 24 05:01:57 CST 2017 on pts/1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">There was 1 failed login attempt since the last successful login.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">su: warning: cannot change directory to /home/vinx: Permission denied</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">-bash: /home/vinx/.bash_profile: Permission denied</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">-bash-4.2$ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理切换用户出现-bash-4.2$的情况</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]<span class="comment"># rm -rf .bash*</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]<span class="comment"># cp /etc/skel/.* /home/vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 home]<span class="comment"># chown vinx:vinx vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]<span class="comment"># chown vinx:vinx .bash*</span></span></pre></td></tr></table></figure><h4 id="设置密码"><a href="#设置密码" class="headerlink" title="设置密码"></a>设置密码</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop000 vinx]<span class="comment"># passwd vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Changing password <span class="keyword">for</span> user vinx.</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">New password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">BAD PASSWORD: The password is shorter than 8 characters</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">Retype new password: </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">passwd: all authentication tokens updated successfully.</span></pre></td></tr></table></figure><h4 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">su vinx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">su - vinx  切换至该用户的家目录，且执行环境变量文件</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">.bash_profile  su vinx不会执行，su - vinx执行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">.bashrc        su vinx和su - vinx都执行</span></pre></td></tr></table></figure><h4 id="普通用户获取root的最大权限"><a href="#普通用户获取root的最大权限" class="headerlink" title="普通用户获取root的最大权限"></a>普通用户获取root的最大权限</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">vi /etc/sudoers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在文件中添加</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root ALL=(ALL) ALL</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">vinx ALL=(root) NOPASSWD:ALL</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">切换至vinx用户，命令前使用sudo</span></pre></td></tr></table></figure><h4 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用格式</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">chmod -R 777 directory/file</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">chown -R user:group directory/file</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">-----------------------------------------------------------------------------</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">drwxr-xr-x  2 root root    6 Nov 24 06:14 <span class="built_in">test</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root root    0 Nov 24 06:14 test.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">第一个字母：d文件夹，-文件，l连接</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">rwx r-x r-x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">r: <span class="built_in">read</span> 4</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">w: write 2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">x: 执行 1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">-: 没权限 0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">7=rwx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">3=-wx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">5=r-x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">rwx 第一组 7 代表文件或文件夹的用户root的权限：读写执行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">r-x 第二组 5 代表文件或文件夹的用户组root的权限：读执行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">r-x 第三组 5 代表其他组的所属用户对这个文件或文件夹的权限：读执行</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">777 代表任意的用户用户组，都读写执行</span></pre></td></tr></table></figure><h4 id="查看大小"><a href="#查看大小" class="headerlink" title="查看大小"></a>查看大小</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">ll</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">du -sh xxx.log</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件夹</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">du -sh xxx</span></pre></td></tr></table></figure><h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从根目录全局查找</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">find / -name <span class="string">'*hadoop*'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定目录中查找</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">find /usr/<span class="built_in">local</span> -name <span class="string">'*hadoop*'</span></span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux用户用户组 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令&amp;定位ERROR</title>
      <link href="/2018/03/10/dw/linux-1/"/>
      <url>/2018/03/10/dw/linux-1/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux常用命令"><a href="#Linux常用命令" class="headerlink" title="Linux常用命令"></a>Linux常用命令</h3><ul><li>pwd  查看当前目录路径</li><li>cd /home  切换到/home目录</li><li>cd -  回退到上一次目录</li><li>cd 切换到用户家目录</li><li>cd ~ 同上</li><li>cd ../  回退到上一层目录</li><li>cd ../../  回退到上两层目录</li><li>ls  查看当前目录的所有文件</li><li>ll  查看所有文件，相当于ls -l</li><li>ll -a 查看所有文件，包括隐藏文件</li><li>ll -h  查看所有文件，并显示文件大小</li><li>ll -rt  查看所有文件，并按时间排序</li><li>clear  清空屏幕</li><li>ls –help  查看ls相关的命令帮助</li><li>mkdir  创建文件夹</li><li>mkdir -p a/b/c  级联创建文件夹</li><li>mkdir a b c  创建多个文件夹</li><li>mv  移动文件</li><li>cp  拷贝文件</li><li>touch test.log  创建一个新文件</li><li>echo “” &gt; test.log  创建一个新文件（慎用，会产生一个字节）</li><li>cat /dev/null &gt; test.log  把一个文件设置为空</li></ul><h3 id="如何定位ERROR"><a href="#如何定位ERROR" class="headerlink" title="如何定位ERROR"></a>如何定位ERROR</h3><h4 id="查看文件内容"><a href="#查看文件内容" class="headerlink" title="查看文件内容"></a>查看文件内容</h4><ul><li>cat  文件内容全部显示</li><li>more  文件内容一页一页的往下翻，按空格键翻页，按q退出</li><li>less  查看文件内容，按上下箭头往上下翻页，按q键退出</li></ul><h4 id="监控日志文件"><a href="#监控日志文件" class="headerlink" title="监控日志文件"></a>监控日志文件</h4><ul><li>tail -f test.log</li><li>tail -F test.log  ==&gt; -f + retry</li><li>tail -300f test.log  实时监控倒数300行</li></ul><h4 id="定位ERROR"><a href="#定位ERROR" class="headerlink" title="定位ERROR"></a>定位ERROR</h4><p>场景1：文件内容很小，几十M。</p><p>解决方法：下载到windows，使用编辑器查找ERROR</p><p>安装上传下载工具：<code>yum install -y lrzsz</code></p><p>场景2：文件内容很大，几百M。</p><p>解决方法：<code>cat test.log | grep ERROR</code></p><p><strong>定位ERROR的3种命令：</strong></p><ul><li><code>cat test.log | grep -A 10 ERROR</code>  后10行</li><li><code>cat test.log | grep -B 10 ERROR</code>  前10行</li><li><code>cat test.log | grep -C 30 ERROR</code>  前后各30行</li><li><code>cat test.log | grep -C 30 ERROR &gt; error.log</code>  错误信息写入到error.log文件</li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux常用命令 </tag>
            
            <tag> 定位ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux vi模式&amp;查看系统资源</title>
      <link href="/2018/02/14/linux/linux-vi/"/>
      <url>/2018/02/14/linux/linux-vi/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux-vi命令"><a href="#Linux-vi命令" class="headerlink" title="Linux vi命令"></a>Linux vi命令</h3><hr><h4 id="vi命令的三种模式"><a href="#vi命令的三种模式" class="headerlink" title="vi命令的三种模式"></a>vi命令的三种模式</h4><p><img src="https://vinxikk.github.io/img/linux/vi.png" alt="三种模式的关系"></p><p>模拟流程：vi test.log进入命令模式，i键进入编辑模式，esc键退出编辑模式并进入命令模式，shift+:进入尾行模式，输入wq并回车保存并退出。</p><h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><p>shift+:进入尾行模式后，/keyword，回车后自动匹配，N键寻找下一个</p><h4 id="设置行号"><a href="#设置行号" class="headerlink" title="设置行号"></a>设置行号</h4><p>进入尾行模式：</p><ul><li>set nu 显示行号</li><li>set nonu  取消行号显示</li></ul><h4 id="清空文件内容"><a href="#清空文件内容" class="headerlink" title="清空文件内容"></a>清空文件内容</h4><ul><li>cat /dev/null &gt; test.log</li><li>echo ‘’ &gt; test.log  存在1个字节，慎用</li></ul><h4 id="命令模式常用快捷键"><a href="#命令模式常用快捷键" class="headerlink" title="命令模式常用快捷键"></a>命令模式常用快捷键</h4><ul><li><p>dd  删除当前行</p></li><li><p>dG  删除光标当前及以下的所有行</p></li><li><p>ndd  删除光标当前及以下的n行</p></li><li><p>gg  跳转到第一行的第一个字母</p></li><li><p>G  跳转到最后一行的第一个字母</p></li><li><p>shift + $  行尾</p></li><li><p>gg + dG  清空文件</p></li></ul><h3 id="系统命令"><a href="#系统命令" class="headerlink" title="系统命令"></a>系统命令</h3><h4 id="查看系统资源"><a href="#查看系统资源" class="headerlink" title="查看系统资源"></a>查看系统资源</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 磁盘</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# df -h</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;sda3        38G  3.3G   33G  10% &#x2F;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">tmpfs          1000M  8.0K 1000M   1% &#x2F;dev&#x2F;shm</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;sda1       194M   34M  151M  19% &#x2F;boot</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#x2F;dev&#x2F;sr0        4.2G  4.2G     0 100% &#x2F;media&#x2F;CentOS_6.5_Final</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"># 内存</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# free -m</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">             total       used       free     shared    buffers     cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Mem:          1998        591       1407          0         22        279</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">-&#x2F;+ buffers&#x2F;cache:        289       1709</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">Swap:         2047          0       2047</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# free -g</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">             total       used       free     shared    buffers     cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">Mem:             1          0          1          0          0          0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">-&#x2F;+ buffers&#x2F;cache:          0          1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">Swap:            1          0          1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"># 负载</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# top</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">top - 20:04:15 up 22 min,  3 users,  load average: 0.02, 0.02, 0.03</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">Tasks: 141 total,   1 running, 140 sleeping,   0 stopped,   0 zombie</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">Cpu(s):  0.0%us,  0.3%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">Mem:   2046592k total,   704988k used,  1341604k free,    22964k buffers</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">Swap:  2097144k total,        0k used,  2097144k free,   383432k cached</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"> 2309 root      20   0 15028 1336 1004 R  0.7  0.1   0:02.45 top                                                                                                           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">    7 root      20   0     0    0    0 S  0.3  0.0   0:02.54 events&#x2F;0                                                                                                      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">    1 root      20   0 19364 1536 1228 S  0.0  0.1   0:02.41 init                                                                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    2 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kthreadd                                                                                                      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">    3 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration&#x2F;0                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">    4 root      20   0     0    0    0 S  0.0  0.0   0:00.05 ksoftirqd&#x2F;0                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">    5 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration&#x2F;0                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">    6 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 watchdog&#x2F;0                                                                                                    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">    8 root      20   0     0    0    0 S  0.0  0.0   0:00.00 cgroup                                                                                                        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">    9 root      20   0     0    0    0 S  0.0  0.0   0:00.01 khelper                                                                                                       </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">   10 root      20   0     0    0    0 S  0.0  0.0   0:00.00 netns                                                                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">   11 root      20   0     0    0    0 S  0.0  0.0   0:00.00 async&#x2F;mgr                                                                                                     </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">   12 root      20   0     0    0    0 S  0.0  0.0   0:00.00 pm                                                                                                            </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">   13 root      20   0     0    0    0 S  0.0  0.0   0:00.02 sync_supers                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">   14 root      20   0     0    0    0 S  0.0  0.0   0:00.01 bdi-default                                                                                                   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">   15 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kintegrityd&#x2F;0                                                                                                 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">   16 root      20   0     0    0    0 S  0.0  0.0   0:00.71 kblockd&#x2F;0                                                                                                     </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">   17 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kacpid                                                                                                        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">   18 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kacpi_notify                                                                                                  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">   19 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kacpi_hotplug                                                                                                 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">   20 root      20   0     0    0    0 S  0.0  0.0   0:00.00 ata_aux                                                                                                       </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">   21 root      20   0     0    0    0 S  0.0  0.0   0:00.13 ata_sff&#x2F;0                                                                                                     </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">   22 root      20   0     0    0    0 S  0.0  0.0   0:00.00 ksuspend_usbd                                                                                                 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">   23 root      20   0     0    0    0 S  0.0  0.0   0:00.02 khubd                                                                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">   24 root      20   0     0    0    0 S  0.0  0.0   0:00.02 kseriod                                                                                                       </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">   25 root      20   0     0    0    0 S  0.0  0.0   0:00.00 md&#x2F;0                                                                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">   26 root      20   0     0    0    0 S  0.0  0.0   0:00.00 md_misc&#x2F;0</span></pre></td></tr></table></figure><h4 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# ps -ef | grep ssh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">root      1451     1  0 19:42 ?        00:00:00 &#x2F;usr&#x2F;sbin&#x2F;sshd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">root      2223  1451  0 19:44 ?        00:00:00 sshd: root@pts&#x2F;1 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">root      2353  2227  0 20:08 pts&#x2F;1    00:00:00 grep ssh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# ps -ef | grep ssh | grep -v grep</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">root      1451     1  0 19:42 ?        00:00:00 &#x2F;usr&#x2F;sbin&#x2F;sshd</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">root      2223  1451  0 19:44 ?        00:00:00 sshd: root@pts&#x2F;1 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">进程用户  进程pid 父id                              进程用户的内容</span></pre></td></tr></table></figure><h4 id="查看端口"><a href="#查看端口" class="headerlink" title="查看端口"></a>查看端口</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]# netstat -nlp | grep 1451</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      1451&#x2F;sshd           </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">tcp        0      0 :::22                       :::*                        LISTEN      1451&#x2F;sshd</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux vi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL案例&amp;TopN实现</title>
      <link href="/2018/01/15/mysql/mysql-example-topn/"/>
      <url>/2018/01/15/mysql/mysql-example-topn/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL案例及面试题"><a href="#MySQL案例及面试题" class="headerlink" title="MySQL案例及面试题"></a>MySQL案例及面试题</h3><hr><h4 id="员工信息统计"><a href="#员工信息统计" class="headerlink" title="员工信息统计"></a>员工信息统计</h4><p>三张表：</p><ul><li>dept - 部门表</li><li>salgrade - 工资等级表</li><li>emp - 员工信息表（类比于流水表）</li></ul><p><strong>表结构如下：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 部门表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># dept部门表(deptno部门编号/dname部门名称/loc地点)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    deptno <span class="built_in">numeric</span>(<span class="number">2</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    dname <span class="built_in">varchar</span>(<span class="number">14</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    loc <span class="built_in">varchar</span>(<span class="number">13</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 工资等级表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># salgrade工资等级表(grade 等级/losal此等级的最低/hisal此等级的最高)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> salgrade (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    grade <span class="built_in">numeric</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    losal <span class="built_in">numeric</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    hisal <span class="built_in">numeric</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 员工表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># emp员工表(empno员工号/ename员工姓名/job工作/mgr上级编号/hiredate受雇日期/sal薪金/comm佣金/deptno部门编号)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 工资 ＝ 薪金 ＋ 佣金</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp (</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    empno <span class="built_in">numeric</span>(<span class="number">4</span>) <span class="keyword">not</span> <span class="literal">null</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">    ename <span class="built_in">varchar</span>(<span class="number">10</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">    job <span class="built_in">varchar</span>(<span class="number">9</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    mgr <span class="built_in">numeric</span>(<span class="number">4</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    hiredate datetime,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">    sal <span class="built_in">numeric</span>(<span class="number">7</span>, <span class="number">2</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    comm <span class="built_in">numeric</span>(<span class="number">7</span>, <span class="number">2</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">    deptno <span class="built_in">numeric</span>(<span class="number">2</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure><p><strong>题目和SQL语句：</strong></p><ol><li><p>查询出部门编号为30的所有员工的编号和姓名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> empno,ename <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span>;</span></pre></td></tr></table></figure></li><li><p>找出部门编号为10中所有经理，和部门编号为20中所有销售员的详细资料</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> (deptno=<span class="number">10</span> <span class="keyword">and</span> job=<span class="string">'MANAGER'</span>) <span class="keyword">or</span> (deptno=<span class="number">20</span> <span class="keyword">and</span> job=<span class="string">'SALESMAN'</span>);</span></pre></td></tr></table></figure></li><li><p>查询所有员工详细信息，用工资降序排序，如果工资相同使用入职日期升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> (sal+<span class="keyword">ifnull</span>(comm,<span class="number">0</span>)) <span class="keyword">desc</span>,hiredate <span class="keyword">asc</span>;</span></pre></td></tr></table></figure></li><li><p>列出最低薪金大于1500的各种工作及从事此工作的员工人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：使用having</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> job,<span class="keyword">count</span>(job) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> job </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">having</span> <span class="keyword">min</span>(sal) &gt; <span class="number">1500</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> job,<span class="keyword">count</span>(<span class="number">0</span>) <span class="keyword">from</span> emp <span class="keyword">where</span> job <span class="keyword">in</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> temp.job <span class="keyword">from</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">(<span class="keyword">select</span> job,<span class="keyword">min</span>(sal) minsal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> job) <span class="keyword">as</span> temp <span class="keyword">where</span> temp.minsal&gt;<span class="number">1500</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">group</span> <span class="keyword">by</span> job;</span></pre></td></tr></table></figure></li><li><p>列出在销售部工作的员工的姓名，假定不知道销售部的部门编号</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno,ename <span class="keyword">from</span> emp <span class="keyword">where</span> deptno <span class="keyword">in</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno <span class="keyword">from</span> dept <span class="keyword">where</span> dname=<span class="string">'SALES'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure></li><li><p>查询姓名以S开头的、以S结尾的、包含S字符、第二个字母为L的员工信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">like</span> <span class="string">'S%'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">like</span> <span class="string">'%S'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">like</span> <span class="string">'%S%'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">like</span> <span class="string">'_L%'</span>;</span></pre></td></tr></table></figure></li><li><p>查询每种工作的最高工资、最低工资、人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> job,<span class="keyword">max</span>(sal+<span class="keyword">ifnull</span>(comm,<span class="number">0</span>)) maxsal,<span class="keyword">min</span>(sal+<span class="keyword">ifnull</span>(comm,<span class="number">0</span>)) minsal,<span class="keyword">count</span>(empno) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> job;</span></pre></td></tr></table></figure></li><li><p>列出薪金高于公司平均薪金的所有员工号，员工姓名，所在部门名称，上级领导，工资，工资等级</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno,e.ename,d.dname,m.ename,e.sal+<span class="keyword">ifnull</span>(e.comm,<span class="number">0</span>) salsum,s.grade <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> emp m <span class="keyword">on</span> e.mgr=m.empno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> salgrade s <span class="keyword">on</span> e.sal+<span class="keyword">ifnull</span>(e.comm,<span class="number">0</span>) <span class="keyword">between</span> s.losal <span class="keyword">and</span> s.hisal</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; (<span class="keyword">select</span> <span class="keyword">avg</span>(sal) <span class="keyword">from</span> emp);</span></pre></td></tr></table></figure></li></ol><ol start="9"><li><p>列出薪金高于在部门30工作的所有/任何一个员工的薪金的员工姓名和薪金、部门名称</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有 可以用all，也可以用max</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># max</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.sal,e.deptno,d.dname <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">max</span>(sal) maxsal <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># all</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.sal,e.deptno,d.dname <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">all</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> sal <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- -----------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 任何 可以用any，也可以用min</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># min</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.sal,e.deptno,d.dname <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">min</span>(sal) maxsal <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># any</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.ename,e.sal,e.deptno,d.dname <span class="keyword">from</span> emp e</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno=d.deptno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.sal &gt; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">any</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> sal <span class="keyword">from</span> emp <span class="keyword">where</span> deptno=<span class="number">30</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr></table></figure></li></ol><h4 id="MySQL的TopN实现"><a href="#MySQL的TopN实现" class="headerlink" title="MySQL的TopN实现"></a>MySQL的TopN实现</h4><p>求出每个部门每个职业的薪水和，薪水和最高的2个职位（实际就是topN的问题）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建视图sal，保存每个部门每个职业的薪水和</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> sal <span class="keyword">as</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> deptno,job,<span class="keyword">sum</span>(sal) sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno,job;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求出每个职业的top1，求top2可以将0改为1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">a.*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sal a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> sal b <span class="keyword">where</span> a.deptno=b.deptno <span class="keyword">and</span> a.sal&lt;b.sal</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">) = <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> a.deptno;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
            <tag> MySQL Top-N </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maven项目打包为jar的几种方式</title>
      <link href="/2017/08/11/java/maven-package/"/>
      <url>/2017/08/11/java/maven-package/</url>
      
        <content type="html"><![CDATA[<h3 id="直接打包，不打包依赖包"><a href="#直接打包，不打包依赖包" class="headerlink" title="直接打包，不打包依赖包"></a>直接打包，不打包依赖包</h3><p>直接打包，不打包依赖包，仅打包出项目中的代码到jar包中。</p><p>在pom中添加如下plugin即可，随后执行maven install。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">   <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr></table></figure><h3 id="将依赖jar包输出到lib目录方式"><a href="#将依赖jar包输出到lib目录方式" class="headerlink" title="将依赖jar包输出到lib目录方式"></a>将依赖jar包输出到lib目录方式</h3><p>将项目中的jar包的依赖包输出到指定的目录下，修改outputDirectory配置，如下面的<code>${project.build.directory}/lib</code>。如想将打包好的jar包可以通过命令直接运行，如<code>java -jar xxx.jar</code>，还需要制定manifest配置的classpathPrefix与上面配置的相对应，如上面把依赖jar包输出到了lib，则这里的classpathPrefix也应指定为lib/；同时，并指定出程序的入口类，在配置mainClass节点中配好入口类的全类名。</p><p>这种打包方式对于Java项目是通用的，不管是SpringBoot的项目还是传统的Java项目，都可行。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- java编译插件 --&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-jar-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">addClasspath</span>&gt;</span>true<span class="tag">&lt;/<span class="name">addClasspath</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">classpathPrefix</span>&gt;</span>lib/<span class="tag">&lt;/<span class="name">classpathPrefix</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.yourpakagename.mainClassName<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-dependency-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>copy<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">phase</span>&gt;</span>install<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">goal</span>&gt;</span>copy-dependencies<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">                    <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/lib<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span></pre></td></tr></table></figure><p>有时为了方便，可以把classpath指定在当前目录上，默认的classpath会在jar包内，可以在main方法配置后加上manifestEntries配置，指定classpath，如：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-jar-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">classesDirectory</span>&gt;</span>target/classes/<span class="tag">&lt;/<span class="name">classesDirectory</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">archive</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">manifest</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">&lt;!-- 主函数的入口 --&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.yourpakagename.mainClassName<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">                <span class="comment">&lt;!-- 打包时 MANIFEST.MF文件不记录的时间戳版本 --&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">useUniqueVersions</span>&gt;</span>false<span class="tag">&lt;/<span class="name">useUniqueVersions</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">addClasspath</span>&gt;</span>true<span class="tag">&lt;/<span class="name">addClasspath</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">classpathPrefix</span>&gt;</span>lib/<span class="tag">&lt;/<span class="name">classpathPrefix</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">manifestEntries</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">Class-Path</span>&gt;</span>.<span class="tag">&lt;/<span class="name">Class-Path</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">manifestEntries</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">archive</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr></table></figure><h3 id="将项目依赖包和项目打为一个包"><a href="#将项目依赖包和项目打为一个包" class="headerlink" title="将项目依赖包和项目打为一个包"></a>将项目依赖包和项目打为一个包</h3><p>这种打包方式会将项目中的依赖包和项目代码都打为一个jar包，其配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.5.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">archive</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">manifest</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.xxg.Main<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">archive</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">            <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span>  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr></table></figure><p>此种方式对于传统的Java项目打包没问题，但是打有Spring框架的jar包就不可以了。可以采用maven-shade-plugin的插件来打包，实现Spring框架的打包。</p><h3 id="SpringBoot项目打包"><a href="#SpringBoot项目打包" class="headerlink" title="SpringBoot项目打包"></a>SpringBoot项目打包</h3><p>SpringBoot项目打包最常用且最简单的方式是用SpringBoot的打包plugin。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span></pre></td></tr></table></figure><p>在pom中加入此插件，再点击maven install或package就会把当前项目里所有依赖包和当前项目的源码都打成一个jar包，同时还会将没有依赖包的jar包也打出来。</p>]]></content>
      
      
      <categories>
          
          <category> Maven </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Maven-Package </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>零拷贝的实现原理</title>
      <link href="/2017/08/09/java/zero-copy/"/>
      <url>/2017/08/09/java/zero-copy/</url>
      
        <content type="html"><![CDATA[<h3 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h3><p>伪代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">File.read(file, buf, len);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Socket.send(socket, buf, len);</span></pre></td></tr></table></figure><p>这种方式一共涉及了4次数据拷贝。</p><p><img src="https://vinxikk.github.io/img/java/file-4-copy.png" alt="文件拷贝流程"></p><ol><li>应用程序中调用<code>read()</code>方法，这里会涉及到一次上下文切换（用户态 -&gt; 内核态），底层采用DMA（direct memory access）读取磁盘的文件，并把内容存储到内核地址空间的读取缓存区。</li><li>由于应用程序无法读取内核地址空间的数据，如果应用程序要操作这些数据，必须把这些内容从读取缓冲区拷贝到用户缓冲区。这个时候，<code>read()</code>调用返回，且引发一次上下文切换（内核态 -&gt; 用户态），现在数据已经被拷贝到了用户地址空间缓冲区，这时，如果有需要，应用程序可以操作修改这些内容。</li><li>我们最终目的是把这个文件内容通过Socket传到另一个服务中，调用Socket的<code>send()</code>方法，这里又涉及到一次上下文切换（用户态 -&gt; 内核态），同时，文件内容被进行第三次拷贝，被再次拷贝到内核地址空间缓冲区，但是这次的缓冲区与目标套接字相关联，与读取缓冲区没有半点关系。</li><li><code>send()</code>调用返回，引发第四次的上下文切换，同时进行第四次的数据拷贝，通过DMA把数据从目标套接字相关的缓存区传到协议引擎进行发送。</li></ol><p>在整个过程中，过程1和4是由DMA负责，并不会消耗CPU，只有过程2和3的拷贝需要CPU参与。</p><p>如果在应用程序中，不需要操作内容，过程2和3就是多余的，如果可以直接把内核态读取缓存区数据直接拷贝到套接字相关的缓存区，就可以达到优化的目的。</p><p><img src="https://vinxikk.github.io/img/java/file-3-copy.png" alt="文件拷贝流程"></p><p>这种实现，可以有以下几点改进：</p><ul><li>上下文切换的次数从四次减少到了两次</li><li>数据拷贝次数从四次减少到了三次（其中DMA copy 2次，CPU copy 1次）</li></ul><p>在Java中，<code>FileChannel</code>的<code>transferTo()</code>方法可以实现这个过程，该方法将数据从文件通道传输到给定的可写字节通道，上面的<code>file.read()</code>和<code>socket.send()</code>调用动作可以替换为<code>transferTo()</code>调用。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">transferTo</span><span class="params">(<span class="keyword">long</span> position, <span class="keyword">long</span> count, WritableByteChannel target)</span></span>;</span></pre></td></tr></table></figure><p>在UNIX和各种Linux系统中，此调用被传递到sendfile()系统调用中，最终实现将数据从一个文件描述符传输到了另一个文件描述符。</p><p>如果底层网络接口卡支持收集操作的话，可以进一步的优化。</p><p>在Linux内核2.4及后期版本中，针对套接字缓冲区描述符做了相应调整，DMA自带了收集功能，对于用户方面，用法还是一样的，但是内部操作已经发生了改变：</p><p><img src="https://vinxikk.github.io/img/java/file-2-copy.png" alt="文件拷贝流程"></p><p>第一步，transferTo()方法引发DMA将文件内容拷贝到内核读取缓冲区。</p><p>第二步，把包含数据位置和长度信息的描述符追加到套接字缓冲区，避免了内容整体的拷贝，DMA引擎直接把数据从内核缓冲区传到协议引擎，从而消除了最后一次CPU参与的拷贝动作。</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zero-Copy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java的IO：BIO、NIO和AIO</title>
      <link href="/2017/07/09/java/java-io/"/>
      <url>/2017/07/09/java/java-io/</url>
      
        <content type="html"><![CDATA[<h3 id="Java-IO"><a href="#Java-IO" class="headerlink" title="Java IO"></a>Java IO</h3><p>IO，即Input/Output，通常指数据在内存和硬盘或其他设备之间的输入和输出。</p><p>在Java中，提供了一些API，可以供开发者来读写外部数据或文件，这些API称为Java IO。</p><p>Java中有三种IO并存，分别是BIO、NIO和AIO。</p><h4 id="Java-BIO"><a href="#Java-BIO" class="headerlink" title="Java BIO"></a>Java BIO</h4><p>BIO，即Block-IO，是一种同步且阻塞的通信模式。是一个比较传统的通信方式，但并发处理能力低、通信耗时。</p><h4 id="Java-NIO"><a href="#Java-NIO" class="headerlink" title="Java NIO"></a>Java NIO</h4><p>NIO，即Non-Block，是Java SE 1.4后，针对网络传输优化的新功能，是一种非阻塞同步的通信模式。</p><p>NIO与原来的I/O有同样的作用和目的，他们之间最重要的区别是数据打包和传输的方式。原来的I/O以流的方式处理数据，而NIO以块的方式处理数据。</p><p>面向流的I/O系统一次一个字节地处理数据，一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。</p><p>面向块的I/O系统以块的形式处理数据，每一个操作都在一步中产生或消费一个数据块。按块处理数据比按（流式的）字节处理数据要快得多，但是面向块的I/O缺少一些面向流的I/O所具有的优雅和简单。</p><h4 id="Java-AIO"><a href="#Java-AIO" class="headerlink" title="Java AIO"></a>Java AIO</h4><p>AIO，即 Asynchronous IO，是异步非阻塞的IO。在NIO的基础上引入了新的异步通道的概念，并提供了异步文件通道和异步套接字通道的实现。</p><h4 id="三种IO的区别"><a href="#三种IO的区别" class="headerlink" title="三种IO的区别"></a>三种IO的区别</h4><p>BIO （Blocking I/O）：同步阻塞I/O模式。</p><p>NIO （New I/O）：同步非阻塞模式。</p><p>AIO （Asynchronous I/O）：异步非阻塞I/O模型。</p><p><strong>适用场景：</strong></p><p>BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序简单易理解。</p><p>NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。</p><p>AIO方式适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="BIO"><a href="#BIO" class="headerlink" title="BIO"></a>BIO</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java-IO </tag>
            
            <tag> BIO </tag>
            
            <tag> NIO </tag>
            
            <tag> AIO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL DELETE语法错误</title>
      <link href="/2017/06/19/mysql/mysql-delete-error/"/>
      <url>/2017/06/19/mysql/mysql-delete-error/</url>
      
        <content type="html"><![CDATA[<h4 id="DELETE语法错误"><a href="#DELETE语法错误" class="headerlink" title="DELETE语法错误"></a>DELETE语法错误</h4><p>错误信息如下：</p><p><img src="https://vinxikk.github.io/img/mysql/mysql-delete-error.png" alt="错误语句"></p><p>错误原因：</p><p><img src="https://vinxikk.github.io/img/mysql/mysql-delete-error-answer.png" alt="错误原因"></p><p>解决办法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="string">`7-team`</span> <span class="keyword">where</span> <span class="keyword">ID</span> <span class="keyword">not</span> <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">min</span>(<span class="keyword">ID</span>) <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> <span class="string">`7-team`</span>) <span class="keyword">as</span> b <span class="keyword">group</span> <span class="keyword">by</span> b.Name);</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL-ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL行转列和列转行</title>
      <link href="/2017/05/17/mysql/mysql-row-column/"/>
      <url>/2017/05/17/mysql/mysql-row-column/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL行转列、列转行"><a href="#MySQL行转列、列转行" class="headerlink" title="MySQL行转列、列转行"></a>MySQL行转列、列转行</h3><p>行转列主要用于对数据进行聚合统计，如统计某类目的商品在某个时间区间的销售情况。</p><p>列转行主要用于将一条数据拆分成多条。</p><p>数据准备：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ws_view(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">not</span> <span class="literal">null</span> auto_increment,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">website <span class="built_in">varchar</span>(<span class="number">80</span>) <span class="keyword">default</span> <span class="literal">null</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="built_in">date</span> <span class="built_in">date</span> <span class="keyword">default</span> <span class="literal">null</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">pageview <span class="built_in">int</span>(<span class="number">11</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">primary <span class="keyword">key</span> (<span class="keyword">id</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-01'</span>,<span class="number">100</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">2</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-01'</span>,<span class="number">200</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">3</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-02'</span>,<span class="number">300</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">4</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-02'</span>,<span class="number">350</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">5</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-03'</span>,<span class="number">310</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">6</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-03'</span>,<span class="number">360</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">7</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-04'</span>,<span class="number">350</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">8</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-04'</span>,<span class="number">380</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">9</span>,<span class="string">'Baidu'</span>,<span class="string">'2013-09-01'</span>,<span class="number">800</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ws_view`</span> (<span class="string">`id`</span>,<span class="string">`website`</span>,<span class="string">`date`</span>,<span class="string">`pageview`</span>) <span class="keyword">VALUES</span> (<span class="number">10</span>,<span class="string">'Google'</span>,<span class="string">'2013-09-01'</span>,<span class="number">700</span>);</span></pre></td></tr></table></figure><h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>主要思路是分组后使用case进行条件判断。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    a.date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">sum</span>(<span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Baidu'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">end</span>) <span class="string">'sum_Baidu'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">max</span>(<span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Baidu'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">end</span>) <span class="string">'max_Baidu'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">sum</span>(<span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Google'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">end</span>) <span class="string">'sum_Google'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">max</span>(<span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Google'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">end</span>) <span class="string">'max_Google'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    ws_view a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="built_in">date</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------+-----------+-----------+------------+------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">| date       | sum_Baidu | max_Baidu | sum_Google | max_Google |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------+-----------+-----------+------------+------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">| 2013-09-01 |       900 |       800 |        900 |        700 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">| 2013-09-02 |       300 |       300 |        350 |        350 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">| 2013-09-03 |       310 |       310 |        360 |        360 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">| 2013-09-04 |       350 |       350 |        380 |        380 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------------+-----------+-----------+------------+------------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>主要思路也是分组后使用case。</p><p>求每天的pageview总数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  a.date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">concat</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="string">'Baidu:'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">cast</span>(<span class="keyword">sum</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> a.website </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">when</span> <span class="string">'Baidu'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"> ) <span class="keyword">as</span> <span class="built_in">char</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">';'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="string">'Google'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">cast</span>(<span class="keyword">sum</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">when</span> <span class="string">'Google'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="built_in">char</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="string">'sum_str'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    ws_view a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.date;</span></pre></td></tr></table></figure><p>使用mysql提供的函数分组：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">a.date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">a.website, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group_concat</span>(a.website, <span class="string">'_sum:'</span>, a.pageview) <span class="keyword">sum</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ws_view a </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.date,a.website;</span></pre></td></tr></table></figure><p>普通group结合字符串拼接：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    a.date,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">concat</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="string">'Baidu_sum:'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">cast</span>(<span class="keyword">sum</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Baidu'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"> ) <span class="keyword">as</span> <span class="built_in">char</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="string">'Baidu'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">concat</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  <span class="string">'Google_sum:'</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">cast</span>(<span class="keyword">sum</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> a.website</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">when</span> <span class="string">'Google'</span> <span class="keyword">then</span> a.pageview</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="built_in">char</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">) <span class="keyword">as</span> <span class="string">'Google'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    ws_view a</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.date;</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL行列互转 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL的JOIN</title>
      <link href="/2017/05/15/mysql/mysql-join/"/>
      <url>/2017/05/15/mysql/mysql-join/</url>
      
        <content type="html"><![CDATA[<h3 id="JOIN的用法"><a href="#JOIN的用法" class="headerlink" title="JOIN的用法"></a>JOIN的用法</h3><hr><p>mysql中join主要用于两张表之间的连接，常用的有内连接、左连接、右连接。如下图：</p><p><img src="https://vinxikk.github.io/img/mysql/join.png" alt="JOIN的几种情况"></p><p>两张表：</p><ul><li>dept - 部门表</li><li>emp - 员工表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部门表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">dno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">dname <span class="built_in">varchar</span>(<span class="number">50</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> dept <span class="keyword">values</span> (<span class="number">10</span>,<span class="string">'A'</span>),(<span class="number">20</span>,<span class="string">'B'</span>),(<span class="number">30</span>,<span class="string">'C'</span>),(<span class="number">40</span>,<span class="string">'D'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 员工表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">eno <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">ename <span class="built_in">varchar</span>(<span class="number">50</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">sal <span class="built_in">int</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">dno <span class="built_in">int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> emp <span class="keyword">values</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>,<span class="string">'zhangsan'</span>,<span class="number">500</span>,<span class="number">10</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">(<span class="number">2</span>,<span class="string">'lisi'</span>,<span class="number">300</span>,<span class="number">20</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>,<span class="string">'wangwu'</span>,<span class="number">600</span>,<span class="number">30</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">(<span class="number">4</span>,<span class="string">'alice'</span>,<span class="number">400</span>,<span class="number">20</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">(<span class="number">5</span>,<span class="string">'tom'</span>,<span class="number">700</span>,<span class="number">10</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">(<span class="number">6</span>,<span class="string">'mike'</span>,<span class="number">800</span>,<span class="number">50</span>);</span></pre></td></tr></table></figure><h4 id="CROSS-JOIN-笛卡尔积"><a href="#CROSS-JOIN-笛卡尔积" class="headerlink" title="CROSS JOIN: 笛卡尔积"></a>CROSS JOIN: 笛卡尔积</h4><p>要理解各种join首先要理解笛卡尔积。</p><p>如果A表有m条记录，B表有n条记录，笛卡尔积的结果就是m*n条记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 相当于没有指明连接条件</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> emp.*,dept.dname <span class="keyword">from</span> emp <span class="keyword">cross</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> emp.*,dept.dname <span class="keyword">from</span> emp <span class="keyword">inner</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> emp.*,dept.dname <span class="keyword">from</span> emp,dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">24 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="INNER-JOIN-内连接"><a href="#INNER-JOIN-内连接" class="headerlink" title="INNER JOIN: 内连接"></a>INNER JOIN: 内连接</h4><p>内连接是最常用的连接操作，用于求两个表的交集，从笛卡尔积的角度看就是从笛卡尔积中挑出ON子句条件成立的记录。</p><p>写法有4种：INNER JOIN, JOIN(省略INNER), WHERE(等值连接), STRAIGHT_JOIN。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下SQL都是等效的，相当于上图中的第1中情况</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">inner</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e,dept d <span class="keyword">where</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">straight_join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="LEFT-JOIN-左连接"><a href="#LEFT-JOIN-左连接" class="headerlink" title="LEFT JOIN: 左连接"></a>LEFT JOIN: 左连接</h4><p>左连接就是求两张表的交集+左表剩下的数据。从笛卡尔积的角度讲，就是先从笛卡尔积中挑出ON子句条件成立的记录，然后加上左表中剩余的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图3</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> d.dno <span class="keyword">is</span> <span class="literal">null</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">| eno  | ename | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">|    6 | mike  |  800 |   50 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="RIGHT-JOIN-右连接"><a href="#RIGHT-JOIN-右连接" class="headerlink" title="RIGHT JOIN: 右连接"></a>RIGHT JOIN: 右连接</h4><p>同理右连接就是求两个表的交集+右表剩下的数据。从笛卡尔积的角度描述，就是从笛卡尔积中挑出ON子句条件成立的记录，然后加上右表中剩余的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图4</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">outer</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| NULL | NULL     | NULL | NULL | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图5</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.dno <span class="keyword">is</span> <span class="literal">null</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">| eno  | ename | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">| NULL | NULL  | NULL | NULL | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="两个集合的并集"><a href="#两个集合的并集" class="headerlink" title="两个集合的并集"></a>两个集合的并集</h4><p>就是从笛卡尔积中挑出ON子句条件成立的记录，然后加上左表中剩余的记录，再加上右表中剩余的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图6</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">|    6 | mike     |  800 |   50 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">| NULL | NULL     | NULL | NULL | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">7 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- --------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图7</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> d.dno <span class="keyword">is</span> <span class="literal">null</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">union</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.dno <span class="keyword">is</span> <span class="literal">null</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">| eno  | ename | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">|    6 | mike  |  800 |   50 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">| NULL | NULL  | NULL | NULL | D     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">2 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="USING子句"><a href="#USING子句" class="headerlink" title="USING子句"></a>USING子句</h4><p>当两表中关联的列同名时，可以使用USING子句来简化ON语法，格式为：USING(col)。</p><p>SELECT * 时，USING会除去USING指定的列，而ON不会。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 等同于inner join</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from emp inner join dept using(dno);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">| dno  | eno  | ename    | sal  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">|   10 |    1 | zhangsan |  500 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">|   20 |    2 | lisi     |  300 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">|   30 |    3 | wangwu   |  600 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">|   20 |    4 | alice    |  400 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">|   10 |    5 | tom      |  700 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> emp.*,dept.dname <span class="keyword">from</span> emp <span class="keyword">inner</span> <span class="keyword">join</span> dept <span class="keyword">using</span>(dno);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.*,d.dname <span class="keyword">from</span> emp e <span class="keyword">inner</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">| eno  | ename    | sal  | dno  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">|    1 | zhangsan |  500 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">|    2 | lisi     |  300 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">|    3 | wangwu   |  600 |   30 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">|    4 | alice    |  400 |   20 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">|    5 | tom      |  700 |   10 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+----------+------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure><h4 id="NATURAL-自然连接"><a href="#NATURAL-自然连接" class="headerlink" title="NATURAL: 自然连接"></a>NATURAL: 自然连接</h4><p>自然连接就是USING子句的简化版，它找出两张表中相同的列作为连接条件进行连接。</p><p>在dept和emp表中，相同的列是dno，所以会以dno为连接条件。</p><p>另外：</p><ul><li>自然连接：select * from emp natural join dept;</li><li>笛卡尔积：select * from emp natura join dept;</li><li>笛卡尔积：select * from emp nature join dept;</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自然连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">natural</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">inner</span> <span class="keyword">join</span> dept <span class="keyword">using</span>(dno);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.dno,e.eno,e.ename,e.sal,d.dname <span class="keyword">from</span> emp e </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.dno,e.eno,e.ename,e.sal,d.dname <span class="keyword">from</span> emp e,dept d </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">where</span> e.dno=d.dno;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">| dno  | eno  | ename    | sal  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">|   10 |    1 | zhangsan |  500 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">|   20 |    2 | lisi     |  300 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">|   30 |    3 | wangwu   |  600 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">|   20 |    4 | alice    |  400 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">|   10 |    5 | tom      |  700 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- ---------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 左自然连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">natural</span> <span class="keyword">left</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">| dno  | eno  | ename    | sal  | dname |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">|   10 |    1 | zhangsan |  500 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">|   10 |    5 | tom      |  700 | A     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">|   20 |    2 | lisi     |  300 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">|   20 |    4 | alice    |  400 | B     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">|   30 |    3 | wangwu   |  600 | C     |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">|   50 |    6 | mike     |  800 | NULL  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+------+----------+------+-------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- --------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 右自然连接</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">natural</span> <span class="keyword">right</span> <span class="keyword">join</span> dept;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+----------+------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">| dno  | dname | eno  | ename    | sal  |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+----------+------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">|   10 | A     |    1 | zhangsan |  500 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">|   20 | B     |    2 | lisi     |  300 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">|   30 | C     |    3 | wangwu   |  600 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">|   20 | B     |    4 | alice    |  400 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">|   10 | A     |    5 | tom      |  700 |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">|   40 | D     | NULL | NULL     | NULL |</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">+<span class="comment">------+-------+------+----------+------+</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">6 rows in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL-JOIN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux tree命令</title>
      <link href="/2017/04/14/linux/linux-tree/"/>
      <url>/2017/04/14/linux/linux-tree/</url>
      
        <content type="html"><![CDATA[<h4 id="安装tree"><a href="#安装tree" class="headerlink" title="安装tree"></a>安装tree</h4><p>需要以root用户进行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">yum install -y tree</span></pre></td></tr></table></figure><h4 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">yum list installed tree</span></pre></td></tr></table></figure><p><img src="https://vinxikk.github.io/img/linux/tree-installed.png" alt="tree安装成功"></p><h4 id="tree使用"><a href="#tree使用" class="headerlink" title="tree使用"></a>tree使用</h4><p>直接执行tree命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree</span></pre></td></tr></table></figure><p>查看tree命令参数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree --<span class="built_in">help</span></span></pre></td></tr></table></figure><p>只查看当前目录下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 1</span></pre></td></tr></table></figure><p>区分文件和文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 1 -C</span></pre></td></tr></table></figure><p>列出权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 1 -C -p</span></pre></td></tr></table></figure><p>查看3层目录下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 3 -C -p</span></pre></td></tr></table></figure><p>列出相对路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 3 -C -p -f</span></pre></td></tr></table></figure><p>只列出文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">tree -L 3 -C -p -d</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL常用语句&amp;CURD操作</title>
      <link href="/2017/03/17/mysql/mysql-basic/"/>
      <url>/2017/03/17/mysql/mysql-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="MySQL常用语句"><a href="#MySQL常用语句" class="headerlink" title="MySQL常用语句"></a>MySQL常用语句</h3><hr><p>MySQL语法主要分为3种：</p><ul><li>DDL - 定义: create, drop…</li><li>DML - 操作: insert, delete, update, select…</li><li>DCL - 控制</li></ul><h4 id="INSERT"><a href="#INSERT" class="headerlink" title="INSERT"></a>INSERT</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加单条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_a <span class="keyword">value</span> (col1,col2,...);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加多条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_a <span class="keyword">values</span> (col1,col2,...),(col1,col2,...)...;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将表b的查询结果插入到表a</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_a <span class="keyword">select</span> * <span class="keyword">from</span> tb_b;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建b表，并将a表中的数据导入b表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb_b <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> tb_a;</span></pre></td></tr></table></figure><h4 id="DELETE"><a href="#DELETE" class="headerlink" title="DELETE"></a>DELETE</h4><p>在生产上，使用DELETE语句时最好习惯性加上WHERE条件，执行操作需要谨慎，避免数据丢失的灾难。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除指定记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_a <span class="keyword">where</span> ctime=<span class="string">'2017-03-12'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除多条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_a <span class="keyword">where</span> ctime <span class="keyword">in</span> (<span class="string">'2017-03-12'</span>,<span class="string">'2017-03-13'</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_a <span class="keyword">where</span> ctime &gt; <span class="string">'2017-03-12'</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清空表数据，把where条件去掉</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 不带where的delete语句和truncate table语句都可以删除表中所有内容，效率上truncate比delete快，但truncate删除后不记录mysql日志，不可恢复数据。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- delete效果有点像将表中所有记录一条一条的删除，而truncate相当于保留表结构，重新创建了这个表，所有的状态都相当于新表。</span></span></pre></td></tr></table></figure><h4 id="UPDATE"><a href="#UPDATE" class="headerlink" title="UPDATE"></a>UPDATE</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新指定数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> tb_a <span class="keyword">set</span> col2=<span class="string">'test'</span> <span class="keyword">where</span> col1=<span class="number">2</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 在product_info表中增加列`product_count`，并更新其值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> product_info <span class="keyword">pi</span> <span class="keyword">set</span> pi.<span class="string">`product_count`</span> = (<span class="keyword">select</span> pio.<span class="string">`product_count_old`</span> <span class="keyword">from</span> product_info_old pio <span class="keyword">where</span> pio.<span class="string">`product_id_old`</span> = pi.<span class="string">`product_id`</span>) <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> <span class="literal">null</span> <span class="keyword">from</span> product_info_old pio <span class="keyword">where</span> pio.<span class="string">`product_id_old`</span> = pi.<span class="string">`product_id`</span>);</span></pre></td></tr></table></figure><h4 id="SELECT"><a href="#SELECT" class="headerlink" title="SELECT"></a>SELECT</h4><p>工作中使用最多的就是SELECT，同时可以结合JOIN, UNION等实现复杂的统计需求。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询全部记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询前5条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a <span class="keyword">limit</span> <span class="number">5</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询指定范围数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a <span class="keyword">limit</span> <span class="number">0</span>,<span class="number">2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a <span class="keyword">limit</span> <span class="number">1</span>,<span class="number">2</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1:记录起始位置，2：返回记录的条数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询最后一条记录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> tb_a <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">1</span>;</span></pre></td></tr></table></figure><h4 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 完整复制表到另一个表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.复制表结构</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb_b <span class="keyword">like</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2.导入数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_a <span class="keyword">select</span> * <span class="keyword">from</span> tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看表结构（列）</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">desc tb_a;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- alter -----------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># alter: 修改基本表，对表结构进行操作，比如对字段增加、删除、修改类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># update: 修改表中的数据，修改某一行某一列的值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改表名</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">rename</span> <span class="keyword">to</span> tb_new;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在列名1后添加列名2</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">add</span> <span class="keyword">column</span> col_2 col_type <span class="keyword">after</span> col_1;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除列</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">drop</span> <span class="keyword">column</span> col_1;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改列名及数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">change</span> col_old col_new col_new_type;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改列属性</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb_a <span class="keyword">modify</span> <span class="keyword">column</span> col_1 attr_new;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- eg:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> product_info <span class="keyword">modify</span> <span class="keyword">column</span> pid <span class="built_in">varchar</span>(<span class="number">50</span>);</span></pre></td></tr></table></figure><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># char和varchar的区别</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># char: 定长，效率高，一般用于固定长度的表单提交数据存储，如：身份证号，手机号，密码等</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># varchar: 不定长，效率偏低</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 效率: char &gt; varchar &gt; text, 但如果使用的是InnoDB引擎的话，推荐使用varchar代替char。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- char和varchar可以有默认值，text不能指定默认值。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 对于int类型，如果不需要存取负值，最好加上unsigned。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 对于经常出现在where语句中的字段，考虑加索引，整型的尤其适合加索引。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 存储价格、金额 -------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用decimal(m,n)来精确表达价格。不要使用float/double等浮点数据类型，因为他们是不精确的，特别是在计算的时候。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">salary decimal(5,2)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 5(precision精度)：代表十进制数字的数目</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 2(scale数据范围)：代表小数点后的数字位数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 在这种情况下，salary列可以存储的值范围是从-99.99到99.99.实际上，mysql在这个列中可以存储的数值可以一直到999.99，因为它没有存储正数的符号。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- decimal和numeric值作为字符串存储，而不是作为二进制浮点数，以便保存那些值的小数精度。一个字符用于值的每一位、小数点（如果scale&gt;0）、“-”符号（对于负值）。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 不使用float或double的原因：float和double是以二进制存储的，所以有一定的误差。</span></span></pre></td></tr></table></figure><h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><p>更新一个包含索引的表比更新一个没有索引的表需要更多的时间，这是由于索引本身也需要更新。因此，理想的做法是仅仅在常常被搜索的列（以及表）上面创建索引。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为某列创建索引</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">index</span> col_index <span class="keyword">on</span> tb_a(col_1);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除某列的索引</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">index</span> col_index <span class="keyword">on</span> tb_a;</span></pre></td></tr></table></figure><h4 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> <span class="keyword">replace</span> <span class="keyword">view</span> view_name <span class="keyword">as</span> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.name,b.class <span class="keyword">from</span> tb_a a <span class="keyword">inner</span> <span class="keyword">join</span> tb_b b </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> a.id=b.id;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询视图数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> view_name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> view_name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看视图 --------------------------------------------------------------------</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 视图</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> information_schema.VIEWS;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> information_schema.TABLES;</span></pre></td></tr></table></figure><h4 id="存储过程和函数"><a href="#存储过程和函数" class="headerlink" title="存储过程和函数"></a>存储过程和函数</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存储过程</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">procedure</span> <span class="keyword">status</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 函数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">function</span> <span class="keyword">status</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看存储过程或函数的创建代码</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">procedure</span> proc_name;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">function</span> func_name;</span></pre></td></tr></table></figure><h4 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">triggers</span> [<span class="keyword">from</span> db_name] [<span class="keyword">like</span> expr]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">triggers</span>\G</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对INFORMATION_SCHEMA数据库中的TRIGGERS表查询</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">triggers</span> T <span class="keyword">where</span> trigger_name=<span class="string">"mytrigger"</span>\G</span></pre></td></tr></table></figure><h4 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h4><p>事务是一个最小的不可再分的工作单元，通常一个事务对应一个完整的业务。例如：银行转账从A卡到B卡，需要扣除A中的钱，然后将钱加入到B中。</p><p>在mysql中如果需要使用事务，那么必须使用InnoDB存储引擎。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL语法 </tag>
            
            <tag> MySQL-CURD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础</title>
      <link href="/2017/02/11/linux/linux-basic/"/>
      <url>/2017/02/11/linux/linux-basic/</url>
      
        <content type="html"><![CDATA[<h3 id="Linux基础命令"><a href="#Linux基础命令" class="headerlink" title="Linux基础命令"></a>Linux基础命令</h3><hr><h4 id="xshell连接linux"><a href="#xshell连接linux" class="headerlink" title="xshell连接linux"></a>xshell连接linux</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以root用户连接至ip地址为ipaddr的linux系统</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">ssh root@ipaddr</span></pre></td></tr></table></figure><h4 id="查看ip"><a href="#查看ip" class="headerlink" title="查看ip"></a>查看ip</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">ifconfig</span></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Welcome to my blog</title>
      <link href="/2017/01/21/hello-world/"/>
      <url>/2017/01/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p><strong>“ 把问题写下来就已经解决了一半。</strong></p><p><img src="https://vinxikk.github.io/img/snow.png" alt="snow"></p>]]></content>
      
      
      <categories>
          
          <category> Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Other </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
