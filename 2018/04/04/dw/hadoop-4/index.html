<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="HDFS副本放置策略&amp;读写流程&amp;PID文件&amp;常用命令&amp;磁盘均衡"><meta name="keywords" content="Hadoop,HDFS"><meta name="author" content="VinxC"><meta name="copyright" content="VinxC"><title>HDFS副本放置策略&amp;读写流程&amp;PID文件&amp;常用命令&amp;磁盘均衡 | VinxC's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#副本放置策略"><span class="toc-number">1.</span> <span class="toc-text">副本放置策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS文件读写流程"><span class="toc-number">2.</span> <span class="toc-text">HDFS文件读写流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#写流程（FSDataOutputStream）"><span class="toc-number">2.1.</span> <span class="toc-text">写流程（FSDataOutputStream）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#读流程（FSDataInputStream）"><span class="toc-number">2.2.</span> <span class="toc-text">读流程（FSDataInputStream）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pid文件"><span class="toc-number">3.</span> <span class="toc-text">pid文件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#创建用户目录下的tmp文件夹："><span class="toc-number">3.1.</span> <span class="toc-text">创建用户目录下的tmp文件夹：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#修改hadoop-env-sh："><span class="toc-number">3.2.</span> <span class="toc-text">修改hadoop-env.sh：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#修改yarn-env-sh"><span class="toc-number">3.3.</span> <span class="toc-text">修改yarn-env.sh</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS常用命令"><span class="toc-number">4.</span> <span class="toc-text">HDFS常用命令</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#常用命令"><span class="toc-number">4.1.</span> <span class="toc-text">常用命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#配置回收站"><span class="toc-number">4.2.</span> <span class="toc-text">配置回收站</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#安全模式"><span class="toc-number">4.3.</span> <span class="toc-text">安全模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#文件系统的检查"><span class="toc-number">4.4.</span> <span class="toc-text">文件系统的检查</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多节点、单节点的磁盘均衡"><span class="toc-number">5.</span> <span class="toc-text">多节点、单节点的磁盘均衡</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#各DN节点的数据均衡"><span class="toc-number">5.1.</span> <span class="toc-text">各DN节点的数据均衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#一个DN节点的多个磁盘的数据均衡"><span class="toc-number">5.2.</span> <span class="toc-text">一个DN节点的多个磁盘的数据均衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#为什么DN在生产上挂载多个物理的磁盘目录"><span class="toc-number">5.3.</span> <span class="toc-text">为什么DN在生产上挂载多个物理的磁盘目录</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://vinxikk.github.io/img/avatar.png"></div><div class="author-info__name text-center">VinxC</div><div class="author-info__description text-center">A Bigdata Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">90</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">95</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">17</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://molunerfinn.com" target="_blank" rel="noopener">Molunerfinn</a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">VinxC's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">HDFS副本放置策略&amp;读写流程&amp;PID文件&amp;常用命令&amp;磁盘均衡</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-04</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hadoop/">Hadoop</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2k</span><span class="post-meta__separator">|</span><span>Reading time: 8 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="副本放置策略"><a href="#副本放置策略" class="headerlink" title="副本放置策略"></a>副本放置策略</h3><p>假设副本数（dfs.replication）为3。</p>
<p>第一个副本：假如上传节点为DN节点，则存放在本节点上（假如Client不在集群范围内，则随机选取一台磁盘不太慢 、CPU不太繁忙的节点放置）。</p>
<p>第二个副本：存放在与第一个副本不同机架的一个节点上。</p>
<p>第三个副本：和第二个副本在同一个机架，放在不同的节点上。</p>
<h3 id="HDFS文件读写流程"><a href="#HDFS文件读写流程" class="headerlink" title="HDFS文件读写流程"></a>HDFS文件读写流程</h3><h4 id="写流程（FSDataOutputStream）"><a href="#写流程（FSDataOutputStream）" class="headerlink" title="写流程（FSDataOutputStream）"></a>写流程（FSDataOutputStream）</h4><p>Client上传文件到HDFS，也就是类似于下面的命令：</p>
<p>hadoop fs -put test.log /</p>
<p><img src="https://vinxikk.github.io/img/dw/hdfs-output.png" alt="HDFS文件写流程"></p>
<p>过程：</p>
<ol>
<li>Client 调用FileSystem.create(filePath)方法，与NN进行[RPC]通信，check是否存在及是否有权限创建。假如不OK，就返回错误信息；假如OK，就创建一个新文件，不关联任何的block块，返回一个FSDataOutputStream对象。</li>
<li>Client调用FSDataOutputStream对象的write()方法，先将第一块的第一个副本写到第一个DN，第一个副本写完就传输给第二个DN，第二个副本写完就传输给第三个DN，直至写完第三个副本。然后返回一个ack packet确认包给第二个DN，第二个DN接收到第三个的ack packet确认包加上自身OK，就返回一个ack packet确认包给第一个DN，第一个DN接收到第二个DN的ack packet确认包加上自身OK，就返回ack packet确认包给FSDataOutputStream对象，标志第一个块的3个副本写完。依次写完余下的块。</li>
<li>当文件写入数据完成后，Client调用FSDataOutputStream.close()方法，关闭输出流。</li>
<li>然后调用FileSystem.complete()方法，告诉NN该文件写入成功。</li>
</ol>
<h4 id="读流程（FSDataInputStream）"><a href="#读流程（FSDataInputStream）" class="headerlink" title="读流程（FSDataInputStream）"></a>读流程（FSDataInputStream）</h4><p>Client从HDFS下载文件到本地，类似于下面的命令：</p>
<p>hadoop fs -get /test.log</p>
<p><img src="https://vinxikk.github.io/img/dw/hdfs-input.png" alt="HDFS文件读流程"></p>
<p>过程：</p>
<ol>
<li><p>Client调用FileSystem.open(filePath)方法，与NN进行[RPC]通信，返回该文件的部分或者全部的block列表，也就是返回FSDataInputStream对象。</p>
</li>
<li><p>Client调用FSDataInputStream.read()方法。</p>
<p>a.与第一个块最近的DN进行read，读取完成后，会check；假如OK，就关闭与当前DN的通信；假如失败，会记录失败块+DN信息，下次不会再读取，会去该块的第二个DN地址读取。</p>
<p>b.接着去第二个块的最近的DN上通信读取，check后，关闭通信。</p>
<p>c.假如block列表读取完成后，文件还未结束，FileSystem会再次从NN获取该文件的下一批次的block列表。</p>
<p>（感觉就是连续的数据流，对于客户端操作是透明无感知的）</p>
</li>
<li><p>Client调用FSDataInputStream.close()方法，关闭输入流。</p>
</li>
</ol>
<h3 id="pid文件"><a href="#pid文件" class="headerlink" title="pid文件"></a>pid文件</h3><p>HADOOP和YARN的pid文件默认存储在/tmp目录下。系统会自动清理/tmp文件夹下30天不访问的文件，比如hadoop-env.sh中的配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The directory <span class="built_in">where</span> pid files are stored. /tmp by default.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> NOTE: this should be <span class="built_in">set</span> to a directory that can only be written to by </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">       the user that will run the hadoop daemons.  Otherwise there is the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">       potential <span class="keyword">for</span> a symlink attack.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span></pre></td></tr></table></figure>

<p>可以看到HADOOP_PID_DIR默认设置为/tmp目录下，所以需要更改pid的存放目录。</p>
<h4 id="创建用户目录下的tmp文件夹："><a href="#创建用户目录下的tmp文件夹：" class="headerlink" title="创建用户目录下的tmp文件夹："></a>创建用户目录下的tmp文件夹：</h4><p>mkdir /home/vinx/tmp</p>
<p>chmod -R 777 /home/vinx/tmp</p>
<h4 id="修改hadoop-env-sh："><a href="#修改hadoop-env-sh：" class="headerlink" title="修改hadoop-env.sh："></a>修改hadoop-env.sh：</h4><p>vi hadoop-env.sh</p>
<p>export HADOOP_PID_DIR=/home/vinx/tmp</p>
<h4 id="修改yarn-env-sh"><a href="#修改yarn-env-sh" class="headerlink" title="修改yarn-env.sh"></a>修改yarn-env.sh</h4><p>vi yarn-env.sh</p>
<p>export YARN_PID_DIR=/home/vinx/tmp</p>
<p>重新启动HDFS和YARN，可以看到在/home/vinx/tmp下生成了对应的pid文件。</p>
<h3 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h3><p>hadoop fs == hdfs dfs</p>
<p>查看hadoop fs命令使用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 ~]<span class="comment"># su - vinx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hadoop fs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span></pre></td></tr></table></figure>

<p>查看hdfs dfs命令使用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hdfs dfs</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span></pre></td></tr></table></figure>

<p>可以看到hdfs dfs底层就是调用了hadoop fs。</p>
<h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><ul>
<li>hdfs dfs -ls /                     查看hdfs根目录下的文件</li>
<li>hdfs dfs -put test.log /     将本地test.log文件上传至hdfs根目录</li>
<li>hdfs dfs -get /test.log ./   将hdfs根目录下的test.log下载至本地当前目录下</li>
<li>hdfs dfs -rm /test.log       删除hdfs根目录下的test.log，如果设置了回收站，会放入回收站</li>
<li>hdfs dfs -rm -skipTrash /test.log    直接删除test.log</li>
<li>hdfs dfs -copyFromLocal test.log /  类似于上面的-put</li>
<li>hdfs dfs -copyToLocal /test.log ./     类似于上面的-get</li>
</ul>
<h4 id="配置回收站"><a href="#配置回收站" class="headerlink" title="配置回收站"></a>配置回收站</h4><p>cd /home/vinx/app/hadoop/etc/hadoop</p>
<p>vi core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"># 设置时间为7天，默认单位：minute</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>10080<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></pre></td></tr></table></figure>

<p>start-dfs.sh重新启动hdfs，执行hdfs dfs -rm /test.log，此时test.log文件并没有直接删除，而是会移动到回收站。若想直接删除文件，可以使用hdfs dfs -rm -skipTrash /test.log。CDH默认是开启回收站的。</p>
<h4 id="安全模式"><a href="#安全模式" class="headerlink" title="安全模式"></a>安全模式</h4><p>安全模式下只能read，不能write，即只能查看或下载hdfs上的文件，不能修改或者从本地上传文件至hdfs。</p>
<p>进入安全模式：</p>
<p>hdfs dfsadmin -safemode enter</p>
<p>如果NN的log显示进入了safe mode，需要执行以下命令让其离开安全模式：</p>
<p>hdfs dfsadmin -safemode leave</p>
<p>注意：尽量在下游HDFS少做安全模式，而是上游来控制数据同步。</p>
<h4 id="文件系统的检查"><a href="#文件系统的检查" class="headerlink" title="文件系统的检查"></a>文件系统的检查</h4><p>从hdfs根目录开启检查文件系统：</p>
<p>hdfs fsck /</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[vinx@hadoop001 ~]$ hdfs fsck /</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">18/12/08 17:50:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">Connecting to namenode via http://hadoop001:50070/fsck?ugi=vinx&amp;path=%2F</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">FSCK started by vinx (auth:SIMPLE) from /192.168.137.130 <span class="keyword">for</span> path / at Sun Dec 08 17:50:14 CST 2018</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">........Status: HEALTHY</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"> Total size:	175138 B</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"> Total <span class="built_in">dirs</span>:	17</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"> Total files:	8</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"> Total symlinks:		0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"> Total blocks (validated):	7 (avg. block size 25019 B)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"> Minimally replicated blocks:	7 (100.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"> Over-replicated blocks:	0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"> Under-replicated blocks:	0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"> Mis-replicated blocks:		0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"> Default replication factor:	1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"> Average block replication:	1.0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"> Corrupt blocks:		0</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"> Missing replicas:		0 (0.0 %)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"> Number of data-nodes:		1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"> Number of racks:		1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">FSCK ended at Sun Dec 08 17:50:14 CST 2018 <span class="keyword">in</span> 36 milliseconds</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">The filesystem under path <span class="string">'/'</span> is HEALTHY</span></pre></td></tr></table></figure>

<p>主要关注两个指标：</p>
<ul>
<li>Corrupt blocks    损坏的块</li>
<li>Missing replicas  丢失的副本</li>
</ul>
<h3 id="多节点、单节点的磁盘均衡"><a href="#多节点、单节点的磁盘均衡" class="headerlink" title="多节点、单节点的磁盘均衡"></a>多节点、单节点的磁盘均衡</h3><h4 id="各DN节点的数据均衡"><a href="#各DN节点的数据均衡" class="headerlink" title="各DN节点的数据均衡"></a>各DN节点的数据均衡</h4><p>官网中hdfs-site.xml的balance配置说明：</p>
<table>
<thead>
<tr>
<th>name</th>
<th>value</th>
<th>description</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.datanode.balance.bandwidthPerSec</td>
<td>10m</td>
<td>Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)to specify the size (such as 128k, 512m, 1g, etc.). Or provide complete size in bytes (such as 134217728 for 128 MB).</td>
</tr>
</tbody></table>
<p>参数指定了每个DN节点做数据均衡时，可以利用的最大带宽，单位是bytes/second。</p>
<p>可以看到dfs.datanode.balance.bandwidthPerSec默认值为10m，生产上一般设为30m。</p>
<p>默认数据均衡的阈值threshold = 10.0，即所有节点的磁盘used与集群的平均used之差要小于这个阈值。</p>
<p>指定阈值为10%，并启动数据均衡shell脚本：</p>
<p>sbin/start-balancer.sh -threshold 10</p>
<p>每天定时执行sbin/start-balancer.sh，做数据平衡和毛刺修正。</p>
<p>调度工具可以选择crontab或Azkaban。</p>
<h4 id="一个DN节点的多个磁盘的数据均衡"><a href="#一个DN节点的多个磁盘的数据均衡" class="headerlink" title="一个DN节点的多个磁盘的数据均衡"></a>一个DN节点的多个磁盘的数据均衡</h4><p>官网HDFS Disk Balancer：</p>
<p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html</a></p>
<p>其中：dfs.disk.balancer.enabled must be set to true in hdfs-site.xml.即hdfs-site.xml中的dfs.disk.balancer.enabled必须设置为true。</p>
<p>使用步骤：</p>
<p>hdfs diskbalancer -plan hadoop001                             生成hadoop001.plan.json<br>hdfs diskbalancer -execute hadoop001.plan.json      执行<br>hdfs diskbalancer -query hadoop001                           查询状态</p>
<p>什么时候执行diskbalancer：</p>
<ol>
<li>新盘加入</li>
<li>监控磁盘剩余空间，小于阈值10%，发邮件预警，手动执行</li>
</ol>
<h4 id="为什么DN在生产上挂载多个物理的磁盘目录"><a href="#为什么DN在生产上挂载多个物理的磁盘目录" class="headerlink" title="为什么DN在生产上挂载多个物理的磁盘目录"></a>为什么DN在生产上挂载多个物理的磁盘目录</h4><p>/data01 disk1<br>/data02 disk2<br>/data03 disk3</p>
<ol>
<li>为了高效率读写数据</li>
<li>提前规划好2-3年存储量 ，避免后期加磁盘维护的工作量</li>
</ol>
<p>官网中hdfs-site.xml的相关参数：</p>
<table>
<thead>
<tr>
<th>name</th>
<th>value</th>
<th>description</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.datanode.data.dir</td>
<td>file://${hadoop.tmp.dir}/dfs/data</td>
<td>Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. The default storage type will be DISK if the directory does not have a storage type tagged explicitly. Directories that do not exist will be created if local filesystem permission allows.</td>
</tr>
</tbody></table>
<p>参数指定dfs数据节点应该在本地文件系统上存储其块的位置。</p>
<p>比如设置dfs.datanode.data.dir = /data01,/data02,/data03,/data04，注意以逗号分隔，comma-delimited</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">VinxC</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="/https:/vinxikk.github.io/2018/04/04/dw/hadoop-4/">https://vinxikk.github.io/2018/04/04/dw/hadoop-4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/HDFS/">HDFS</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/04/08/dw/hadoop-5/"><i class="fa fa-chevron-left">  </i><span>YARN架构&amp;InputSplit和MapTask的关系&amp;Shuffle机制&amp;压缩格式</span></a></div><div class="next-post pull-right"><a href="/2018/04/01/dw/hadoop-3/"><span>HDFS架构&amp;NameNode和DataNode工作机制&amp;BLOCK和副本数&amp;小文件问题</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2020 By VinxC</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>