<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="YARN架构&amp;InputSplit和MapTask的关系&amp;Shuffle机制&amp;压缩格式"><meta name="keywords" content="YARA,Split分片机制,Shuffle机制"><meta name="author" content="VinxC"><meta name="copyright" content="VinxC"><title>YARN架构&amp;InputSplit和MapTask的关系&amp;Shuffle机制&amp;压缩格式 | VinxC's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#MR-on-YARN流程"><span class="toc-number">1.</span> <span class="toc-text">MR on YARN流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ResourceManager"><span class="toc-number">1.1.</span> <span class="toc-text">ResourceManager</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NodeManager"><span class="toc-number">1.2.</span> <span class="toc-text">NodeManager</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MR-on-YARN"><span class="toc-number">1.3.</span> <span class="toc-text">MR on YARN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#input-split与map-task的关系"><span class="toc-number">2.</span> <span class="toc-text">input split与map task的关系</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#切片机制"><span class="toc-number">2.1.</span> <span class="toc-text">切片机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FileInputFormat源码分析"><span class="toc-number">2.2.</span> <span class="toc-text">FileInputFormat源码分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shuffle机制"><span class="toc-number">3.</span> <span class="toc-text">Shuffle机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MapReduce计算框架的不足之处"><span class="toc-number">3.1.</span> <span class="toc-text">MapReduce计算框架的不足之处</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#压缩格式"><span class="toc-number">4.</span> <span class="toc-text">压缩格式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#压缩的好处和坏处"><span class="toc-number">4.1.</span> <span class="toc-text">压缩的好处和坏处</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#压缩格式-1"><span class="toc-number">4.2.</span> <span class="toc-text">压缩格式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#压缩的应用场景"><span class="toc-number">4.3.</span> <span class="toc-text">压缩的应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#压缩参数配置"><span class="toc-number">4.4.</span> <span class="toc-text">压缩参数配置</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://vinxikk.github.io/img/avatar.png"></div><div class="author-info__name text-center">VinxC</div><div class="author-info__description text-center">A Bigdata Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">16</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">19</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">7</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://molunerfinn.com" target="_blank" rel="noopener">Molunerfinn</a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">VinxC's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">YARN架构&amp;InputSplit和MapTask的关系&amp;Shuffle机制&amp;压缩格式</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-08</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hadoop/">Hadoop</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3.6k</span><span class="post-meta__separator">|</span><span>Reading time: 13 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="MR-on-YARN流程"><a href="#MR-on-YARN流程" class="headerlink" title="MR on YARN流程"></a>MR on YARN流程</h3><p>YARN是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p>
<h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><p>官网原文：</p>
<p>The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system.</p>
<p>The ResourceManager has two main components: Scheduler and ApplicationsManager.</p>
<p>RM组成：</p>
<ul>
<li>Application Manager - 应用程序管理器</li>
<li>Resource Scheduler - memory+cpu资源调度器</li>
</ul>
<h4 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h4><p>官网原文：</p>
<p>The NodeManager is responsible for launching and managing containers on a node. Containers execute tasks as specified by the AppMaster.</p>
<p>NM组成：</p>
<ul>
<li>container - 虚拟概念，执行MR、Spark计算任务的最小单元</li>
</ul>
<h4 id="MR-on-YARN"><a href="#MR-on-YARN" class="headerlink" title="MR on YARN"></a>MR on YARN</h4><p><img src="https://vinxikk.github.io/img/dw/mr-on-yarn.png" alt="MR on YARN流程"></p>
<p>描述：</p>
<ol>
<li>用户向YARN提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令等。</li>
<li>RM为该job分配第一个container，运行job的ApplicationMaster。</li>
<li>App Master向Application Manager注册，这样就可以在RM WEB界面查询这个job的运行状态。</li>
<li>App Master采用轮询的方式通过RPC协议向RM申请和领取资源。</li>
<li>一旦App Master拿到资源，就对应的与NM通信，要求启动任务。</li>
<li>NM为任务设置好运行环境（jar包等），将任务启动命令写在一个脚本里，并通过该脚本启动任务task。</li>
<li>各个task通过RPC协议向App Master汇报自己的状态和进度，以此让App Master随时掌握各个task的运行状态，从而在task运行失败后重启任务。</li>
<li>App Master向Application Manager注销且关闭自己。</li>
</ol>
<p>总的来说就是两步：</p>
<ul>
<li>启动App Master，申请资源；</li>
<li>运行任务，直到任务运行完成。</li>
</ul>
<h3 id="input-split与map-task的关系"><a href="#input-split与map-task的关系" class="headerlink" title="input split与map task的关系"></a>input split与map task的关系</h3><p>首先，<strong>split数量和map task数量一一对应</strong>。</p>
<p>下图是wordcount的流程：</p>
<p><img src="https://vinxikk.github.io/img/dw/wordcount-mr.png" alt="wordcount流程"></p>
<p>可以看到，一个job的Map阶段map task并行度（个数），由客户端提交job时的切片个数决定。</p>
<p>假如有以下两个文件，blocksize=128M，则split和map task的关系如下图所示：</p>
<p><img src="https://vinxikk.github.io/img/dw/split-maptask.png" alt="split-maptask的关系"></p>
<p>也就是说，有多少个切片，就会启动相应数量的map task进行数据处理。那么，如果需要确定map task的数量，只需要确定切片的实际数量即可。</p>
<h4 id="切片机制"><a href="#切片机制" class="headerlink" title="切片机制"></a>切片机制</h4><p>默认使用FileInputFormat类处理数据输入，遵循如下的切片机制：</p>
<ol>
<li>按照文件的内容长度进行切片</li>
<li>切片大小，默认等于block大小</li>
<li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li>
</ol>
<p>比如待处理数据有两个文件：</p>
<p>file01.txt 320M</p>
<p>file02.txt 10M</p>
<p>经过FileInputFormat的切片机制运算后，形成的切片信息如下：</p>
<p>file01.txt.split1–  0~128</p>
<p>file01.txt.split2–  128~256</p>
<p>file01.txt.split3–  256~320</p>
<p>file02.txt.split1–  0~10M</p>
<p>那么切片的数量是否就是分块的数量+小文件的数据量呢？其实是不一定的，因为源码中还有这样一个参数：private static final double SPLIT_SLOP = 1.1，也就是还有10%的切片裕度，下面结合源码进行说明。</p>
<h4 id="FileInputFormat源码分析"><a href="#FileInputFormat源码分析" class="headerlink" title="FileInputFormat源码分析"></a>FileInputFormat源码分析</h4><p>版本：hadoop2.6.0-cdh5.15.1</p>
<p>类路径：package org.apache.hadoop.mapreduce.lib.input.FileInputFormat.java;</p>
<p>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> SPLIT_SLOP = <span class="number">1.1</span>;   <span class="comment">// 10% slop</span></span></pre></td></tr></table></figure>

<p>获取文件切片列表和核心方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * Generate the list of files and make them into FileSplits.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * <span class="doctag">@param</span> job the job context</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   * <span class="doctag">@throws</span> IOException</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">   */</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// generate splits</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;InputSplit&gt;();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    List&lt;FileStatus&gt; files = listStatus(job);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      Path path = file.getPath();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">long</span> length = file.getLen();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        BlockLocation[] blkLocations;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">          blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">          FileSystem fs = path.getFileSystem(job.getConfiguration());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">          blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> blockSize = file.getBlockSize();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">long</span> bytesRemaining = length;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">          <span class="comment">// while循环判断切完剩下的部分是否大于块的1.1倍，大于的话继续切分</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">                        blkLocations[blkIndex].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">                        blkLocations[blkIndex].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">            bytesRemaining -= splitSize;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">          <span class="comment">//剩余部分小于等于1.1倍且不为0，就将剩下的划分为一块</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">                       blkLocations[blkIndex].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">                       blkLocations[blkIndex].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">          <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">            <span class="comment">// Log only if the file is big enough to be splitted</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> (length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">              LOG.debug(<span class="string">"File is not splittable so no parallelization "</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">                  + <span class="string">"is possible: "</span> + file.getPath());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">            &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">          &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">          splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">                      blkLocations[<span class="number">0</span>].getCachedHosts()));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">      &#125; <span class="keyword">else</span> &#123; </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">        <span class="comment">//Create empty hosts array for zero length files</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">        splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Save the number of input files for metrics/loadgen</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">    sw.stop();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">      LOG.debug(<span class="string">"Total # of splits generated by getSplits: "</span> + splits.size()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">          + <span class="string">", TimeTaken: "</span> + sw.now(TimeUnit.MILLISECONDS));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> splits;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr></table></figure>

<p>整体流程：</p>
<ol>
<li>找到数据存储的目录</li>
<li>遍历处理（规划切片）目录下的每一个文件</li>
<li>遍历第一个文件<ol>
<li>获取文件大小</li>
<li>计算切片大小。默认情况下，切片大小=blocksize</li>
<li>每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分为一块切片</li>
<li>将切片信息写到一个切片规划文件中</li>
<li>整个切片的核心过程在getSplit()方法中完成</li>
<li>数据切片只是在逻辑上对输入数据进行分片，并不会在磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。</li>
<li>注意：block是HDFS物理上存储的数据，切片是对数据逻辑上的划分</li>
</ol>
</li>
<li>提交切片规划文件到YARN上，YARN上的AppMaster就可以根据切片规划文件计算开启map task个数。</li>
</ol>
<p>总结：所以，切片的数量并不一定等于分块的数量+小文件的数据量，还要考虑大文件切分后剩余的部分是否大于块的1.1倍。</p>
<h3 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h3><p>MapReduce确保每个reduce的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reduce的过程）就是Shuffle。</p>
<p><img src="https://vinxikk.github.io/img/dw/shuffle.png" alt="Shuffle"></p>
<p><img src="https://vinxikk.github.io/img/dw/shuffle-detail.png" alt="Shuffle机制"></p>
<p><strong>Shuffle过程：</strong></p>
<p>shuffle一开始就是map阶段做输出操作，map在做输出时会在内存里开启一个环形缓冲区，这个缓冲区是专门用来输出的，默认大小是100MB，并且在配置文件里为这个缓冲区设定了一个阈值，默认是0.80。同时map还会为输出操作启动一个守护线程，如果缓冲区的内存达到了阈值的80%的时候，这个守护线程就会把内容写到磁盘上，这个过程叫spill（溢写）。另外的20%内存可以继续写入要写进磁盘的数据，写入磁盘和写入内存操作互不干扰，如果缓存区被撑爆了，那么map就会阻塞写入内存的操作，让写入磁盘操作完成后再继续执行写入内存操作。</p>
<p>写入磁盘前会有个排序操作，这个是在写入磁盘操作时进行，不是在写入内存时进行的，如果我们定义了combiner函数，那么排序前还会执行combiner操作。每次spill操作也就是写入磁盘时就会写一个溢出文件，也就是说在做map输出时有几次spill就会产生多少个溢出文件，等map输出全部做完后，map会合并这些输出文件。</p>
<p>这个过程里还会有一个Partitioner操作，和map阶段的输入分片（Input Split）很像，一个Partitioner对应一个reduce作业，如果我们的MR操作只有一个reduce操作，那么Partitioner就只有一个，如果我们有多个reduce操作，那么Partitioner对应的就会有多个，Partitioner因此就是reduce的输入分片。</p>
<p>到了reduce阶段就是合并map输出文件了，Partitioner会找到对应的map输出文件，然后进行复制操作，复制操作时reduce会开启几个复制线程，这些线程默认个数是5个，这个复制过程和map写入磁盘过程类似，也有阈值和内存大小，阈值可以在配置文件里配置，而内存大小是直接使用reduce的tasktracker的内存大小，复制时候reduce还会进行排序操作和合并文件操作，这些操作完了就会进行reduce计算了。</p>
<p>上面也提到，将map的输出作为reduce的输入的过程就是shuffle（洗牌）。</p>
<p>那么shuffle到底是属于map阶段还是reduce阶段呢？比较公认的说法是，shuffle应该是属于reduce阶段，因为map阶段的主要任务是数据映射，而shuffle的目的是主要是为reduce阶段做准备的。</p>
<h4 id="MapReduce计算框架的不足之处"><a href="#MapReduce计算框架的不足之处" class="headerlink" title="MapReduce计算框架的不足之处"></a>MapReduce计算框架的不足之处</h4><p>从上图可以看到，shuffle阶段中会有多次写入磁盘的操作，基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。另外，当一些查询（Hive）翻译到MapReduce任务时，往往会产生多个stage（阶段），而这些串联的stage又依赖于底层文件系统（HDFS）来存储每一个stage的输出结果，而I/O的效率往往较低，从而影响了MapReduce的运行速度。相比之下，Spark的运算结果中间不落地，而且兼容HDFS、Hive，实际应用中会以Spark替代MR作为常用的计算引擎。</p>
<h3 id="压缩格式"><a href="#压缩格式" class="headerlink" title="压缩格式"></a>压缩格式</h3><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。</p>
<h4 id="压缩的好处和坏处"><a href="#压缩的好处和坏处" class="headerlink" title="压缩的好处和坏处"></a>压缩的好处和坏处</h4><p>好处：</p>
<ul>
<li>减少磁盘存储空间</li>
<li>降低IO（网络的IO和磁盘的IO）</li>
<li>加快数据在磁盘和网络中的传输速度，从而提高系统的处理速度</li>
</ul>
<p>坏处：</p>
<ul>
<li>由于使用数据时，需要先将数据解压，加重CPU负荷</li>
</ul>
<p>基本原则：</p>
<ol>
<li>运算密集型的job，少用压缩</li>
<li>IO密集型的job，多用压缩</li>
</ol>
<h4 id="压缩格式-1"><a href="#压缩格式-1" class="headerlink" title="压缩格式"></a>压缩格式</h4><p>Hadoop中的压缩格式：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>扩展名</th>
<th>是否支持分割</th>
<th>Hadoop编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>N/A</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>Yes</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>Lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>Yes(if index)</td>
<td>com.hadoop.compression.lzo.LzoCodec</td>
</tr>
<tr>
<td>LZ4</td>
<td>N/A</td>
<td>LZ4</td>
<td>.lz4</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
</tr>
<tr>
<td>Snappy</td>
<td>N/A</td>
<td>Snappy</td>
<td>.snappy</td>
<td>No</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩比：</p>
<p><img src="https://vinxikk.github.io/img/dw/compress-codec-size.png" alt="压缩之后的size"></p>
<p>压缩时间：</p>
<p><img src="https://vinxikk.github.io/img/dw/compress-codec-time.png" alt="压缩时间-两次不同大小的文件"></p>
<p>可以看出，压缩比越高，压缩时间越长，压缩比：Snappy&gt;LZ4&gt;LZO&gt;GZIP&gt;BZIP2</p>
<p>压缩格式的优缺点：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>压缩比在四种压缩方式中较高；Hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分Linux系统都自带gzip命令，使用方便</td>
<td>不支持split</td>
</tr>
<tr>
<td>lzo</td>
<td>压缩/解压速度也比较快，合理的压缩率；支持split，是Hadoop中最流行的压缩格式；支持hadoop native库；需要在Linux系统下自行安装lzop命令，使用方便</td>
<td>压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）</td>
</tr>
<tr>
<td>snappy</td>
<td>压缩速度快；支持hadoop native库</td>
<td>不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令d. bzip2</td>
</tr>
<tr>
<td>bzip2</td>
<td>支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便</td>
<td>压缩/解压速度慢；不支持native</td>
</tr>
</tbody></table>
<h4 id="压缩的应用场景"><a href="#压缩的应用场景" class="headerlink" title="压缩的应用场景"></a>压缩的应用场景</h4><p>在Hadoop中的应用场景主要在三方面：输入，中间，输出。</p>
<p>整体思路：hdfs ==&gt; map ==&gt; shuffle ==&gt; reduce</p>
<ol>
<li>Use Compressd Map Input: 从HDFS中读取文件进行MR作业，如果数据很大，可以使用压缩并且选择支持分片的压缩方式（bzip2, LZO），可以实现并行处理，提高效率，减少磁盘读取时间，同时选择合适的存储格式，例如：Sequence Files, RC, ORC等。</li>
<li>Compress Intermediate Data: Map输出作为Reduce的输入，需要经过shuffle这一过程，需要把数据读取到一个环形缓冲区，然后读取到本地磁盘，所以选择压缩可以减少存储文件所占空间，提升数据传输速率。建议使用压缩速度快的压缩方式，例如：Snappy和LZO。</li>
<li>Compress Reducer Output: 进行归档处理或者链接MR的工作（该作业的输出作为下个作业的输入），压缩可以减少存储文件所占空间，提升数据传输速率，如果作为归档处理，可以采用高的压缩比（Gzip, bzip2），如果作为下个作业的输入，考虑是否要分片进行选择。</li>
</ol>
<h4 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h4><p>要在Hadoop中启动压缩，可以配置如下参数：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs <br/>（在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress<br/>（在mapred-site.xml中配置）</td>
<td>False</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec<br/>（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress<br/>（在mapred-site.xml中配置）</td>
<td>False</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec<br/>（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress. DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type<br/>（在mapred-site.xml中配置）</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<hr>
<p>参考：</p>
<p><a href="https://www.cnblogs.com/sharpxiajun/p/3151395.html" target="_blank" rel="noopener">https://www.cnblogs.com/sharpxiajun/p/3151395.html</a></p>
<p><a href="https://blog.csdn.net/yljphp/article/details/89067858" target="_blank" rel="noopener">https://blog.csdn.net/yljphp/article/details/89067858</a></p>
<p><a href="https://blog.csdn.net/yu0_zhang0/article/details/79524842" target="_blank" rel="noopener">https://blog.csdn.net/yu0_zhang0/article/details/79524842</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">VinxC</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="/https:/vinxikk.github.io/2018/04/08/dw/hadoop-5/">https://vinxikk.github.io/2018/04/08/dw/hadoop-5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/YARA/">YARA</a><a class="post-meta__tags" href="/tags/Split%E5%88%86%E7%89%87%E6%9C%BA%E5%88%B6/">Split分片机制</a><a class="post-meta__tags" href="/tags/Shuffle%E6%9C%BA%E5%88%B6/">Shuffle机制</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/04/22/es/es-basic/"><i class="fa fa-chevron-left">  </i><span>ElasticSearch入门</span></a></div><div class="next-post pull-right"><a href="/2018/04/04/dw/hadoop-4/"><span>HDFS副本放置策略&amp;读写流程&amp;PID文件&amp;常用命令&amp;磁盘均衡</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2019 By VinxC</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>