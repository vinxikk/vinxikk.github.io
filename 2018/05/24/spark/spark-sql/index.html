<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Spark SQL基本概念&amp;DataFrame&amp;Dataset"><meta name="keywords" content="Spark SQL"><meta name="author" content="VinxC"><meta name="copyright" content="VinxC"><title>Spark SQL基本概念&amp;DataFrame&amp;Dataset | VinxC's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是Spark-SQL"><span class="toc-number">1.</span> <span class="toc-text">什么是Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Dataset和DataFrame"><span class="toc-number">1.1.</span> <span class="toc-text">Dataset和DataFrame</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建DataFrame"><span class="toc-number">2.</span> <span class="toc-text">创建DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#通过case-class创建DataFrame"><span class="toc-number">2.1.</span> <span class="toc-text">通过case class创建DataFrame</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用SparkSession"><span class="toc-number">2.2.</span> <span class="toc-text">使用SparkSession</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用json文件来创建DataFrame"><span class="toc-number">2.3.</span> <span class="toc-text">使用json文件来创建DataFrame</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DataFrame操作"><span class="toc-number">2.4.</span> <span class="toc-text">DataFrame操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Global-Temporary-View"><span class="toc-number">2.5.</span> <span class="toc-text">Global Temporary View</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建Dataset"><span class="toc-number">3.</span> <span class="toc-text">创建Dataset</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#使用序列创建Dataset"><span class="toc-number">3.1.</span> <span class="toc-text">使用序列创建Dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用json创建Dataset"><span class="toc-number">3.2.</span> <span class="toc-text">使用json创建Dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用HDFS数据创建Dataset"><span class="toc-number">3.3.</span> <span class="toc-text">使用HDFS数据创建Dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dataset案例"><span class="toc-number">3.4.</span> <span class="toc-text">Dataset案例</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://vinxikk.github.io/img/avatar.png"></div><div class="author-info__name text-center">VinxC</div><div class="author-info__description text-center">A Bigdata Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">82</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">92</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">15</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://molunerfinn.com" target="_blank" rel="noopener">Molunerfinn</a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">VinxC's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Spark SQL基本概念&amp;DataFrame&amp;Dataset</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-05-24</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Spark/">Spark</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.9k</span><span class="post-meta__separator">|</span><span>Reading time: 7 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="什么是Spark-SQL"><a href="#什么是Spark-SQL" class="headerlink" title="什么是Spark SQL"></a>什么是Spark SQL</h3><p><img src="https://vinxikk.github.io/img/spark/what-is-spark-sql.png" alt="Spark SQL是什么"></p>
<p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。</p>
<p>Hive是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性。</p>
<p>由于MapReduce这种计算模型执行效率比较慢，所以Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！同时Spark SQL也支持从Hive中读取数据。</p>
<h4 id="Dataset和DataFrame"><a href="#Dataset和DataFrame" class="headerlink" title="Dataset和DataFrame"></a>Dataset和DataFrame</h4><p><strong>DataFrame：</strong></p>
<p>DataFrame是组织成命名列的数据集，它在概念上等同于关系数据库中的表，但在底层具有更丰富的优化。DataFrame可以从各种来源构建，例如：</p>
<ul>
<li>结构化数据文件</li>
<li>Hive中的表</li>
<li>外部数据库或现有RDD</li>
</ul>
<p><img src="https://vinxikk.github.io/img/spark/sql-rdd-dataframe.png" alt="RDD和DataFrame"></p>
<p>从上图可以看出，DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合，DataFrame是分布式的Row对象的集合。</p>
<p>DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化。</p>
<p><strong>Dataset：</strong></p>
<p>Dataset是数据的分布式集合。Dataset是在Spark 1.6中添加的一个新接口，是DataFrame之上更高一级的抽象。它提供了RDD的优点（强类型化，使用强大的lambda函数的能力）以及Spark SQL优化后的执行引擎的优点。一个Dataset 可以从JVM对象构造，然后使用函数转换（map， flatMap，filter等）去操作。 Dataset API 支持Scala和Java。 Python不支持Dataset API。</p>
<h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><h4 id="通过case-class创建DataFrame"><a href="#通过case-class创建DataFrame" class="headerlink" title="通过case class创建DataFrame"></a>通过case class创建DataFrame</h4><ol>
<li><p>定义case class（相当于表的结构schema）</p>
<p><code>case class Emp(empno: Int, ename: String, job: String, mgr: String, hiredate: String, sal: Int, comm: String, deptno: Int)</code></p>
<p>由于mgr和comm列中包含null值，简单起见，将对应的case class类型定义为String</p>
</li>
<li><p>将HDFS上的数据读入RDD，并将RDD与case class关联</p>
<p><code>val lines = sc.textFile(&quot;hdfs://rshost001:9000/date/emp.csv&quot;).map(_.split(&quot;,&quot;))</code></p>
<p><code>val allEmp = lines.map(x =&gt; Emp(x(0).toInt, x(1), x(2), x(3), x(4), x(5).toInt, x(6), x(7).toInt))</code></p>
</li>
<li><p>将RDD转换成DataFrame</p>
<p><code>val allEmpDF = allEmp.toDF</code></p>
</li>
<li><p>通过DataFrame查询数据</p>
<p><img src="https://vinxikk.github.io/img/spark/sql-dataframe-example.png" alt="DataFrame示例"></p>
</li>
</ol>
<h4 id="使用SparkSession"><a href="#使用SparkSession" class="headerlink" title="使用SparkSession"></a>使用SparkSession</h4><p>Apache Spark 2.0引入了SparkSession，其为用户提供了一个统一的切入点来使用Spark的各项功能，并且允许用户通过它调用DataFrame和Dataset相关API来编写Spark程序。最重要的是，它减少了用户需要了解的一些概念，使得我们可以很容易地与Spark交互。</p>
<p>在2.0版本之前，与Spark交互之前必须先创建SparkConf和SparkContext。然而在Spark 2.0中，我们可以通过SparkSession来实现同样的功能，而不需要显式地创建SparkConf, SparkContext 以及 SQLContext，因为这些对象已经封装在SparkSession中。</p>
<p><img src="https://vinxikk.github.io/img/spark/sql-sparksession.png" alt="SparkSession"></p>
<ol>
<li><p>创建StructType，来定义schema结构信息</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> myschema = <span class="type">StructType</span>(<span class="type">List</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"empno"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"ename"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"job"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"mgr"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"hiredate"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"sal"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"comm"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      <span class="type">StructField</span>(<span class="string">"deptno"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    ))</span></pre></td></tr></table></figure>

<p>需要：<code>import org.apache.spark.sql.types._</code></p>
</li>
<li><p>读入数据并切分数据</p>
<p><code>val empRDD = sc.textFile(&quot;hdfs://rshost001:9000/data/emp.csv&quot;).map(_.split(&quot;,&quot;))</code></p>
</li>
<li><p>将RDD中的数据映射成Row</p>
<p><code>val rowRDD = empRDD.map(line =&gt; Row(line(0).toInt, line(1), line(2), line(3), line(4), line(5).toInt, line(6), line(7).toInt))</code></p>
<p>需要：<code>import org.apache.spark.sql.Row</code></p>
</li>
<li><p>创建DataFrame</p>
<p><code>val df = spark.createDataFrame(rowRDD, myschema)</code></p>
</li>
</ol>
<h4 id="使用json文件来创建DataFrame"><a href="#使用json文件来创建DataFrame" class="headerlink" title="使用json文件来创建DataFrame"></a>使用json文件来创建DataFrame</h4><p>源文件：<code>$SPARK_HOME/examples/src/main/resources/people.json</code></p>
<p><code>val df = spark.read.json(path)</code></p>
<p>查看数据和schema信息：</p>
<p><img src="https://vinxikk.github.io/img/spark/sql-json-dataframe.png" alt="通过json创建DF"></p>
<h4 id="DataFrame操作"><a href="#DataFrame操作" class="headerlink" title="DataFrame操作"></a>DataFrame操作</h4><p>DataFrame操作也称作无类型的Dataset操作。</p>
<p>查询所有员工的姓名：</p>
<p><img src="https://vinxikk.github.io/img/spark/sql-df-sql-1.png" alt="查询所有员工的姓名"></p>
<p>查询所有员工的姓名和薪水，并给薪水加100块钱：</p>
<p><img src="https://vinxikk.github.io/img/spark/sql-df-sql-2.png" alt="查询所有员工的姓名和薪水"></p>
<p>查询工资大于2000的员工：</p>
<p><img src="https://vinxikk.github.io/img/spark/sql-df-sql-3.png" alt="查询工资大于2000的员工"></p>
<p>求每个部门的员工人数：</p>
<p><img src="https://vinxikk.github.io/img/spark/sql-df-sql-4.png" alt="求每个部门的员工人数"></p>
<p>更多请参考：</p>
<p><strong><a href="http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.sql.Dataset</a></strong></p>
<p>在DataFrame中使用SQL语句：</p>
<ol>
<li><p>将DataFrame注册成表（视图）：<code>df.createOrReplaceTempView(&quot;emp&quot;)</code></p>
</li>
<li><p>执行查询：</p>
<p><code>spark.sql(&quot;select * from emp&quot;).show</code></p>
<p><code>spark.sql(&quot;select * from emp where deptno=10&quot;).show</code></p>
<p><code>spark.sql(&quot;select deptno,sum(sal) from emp group by deptno&quot;).show</code></p>
</li>
</ol>
<h4 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h4><p>上面使用的是一个在Session生命周期中的临时views。在Spark SQL中，如果你想拥有一个临时的view，并想在不同的Session中共享，而且在application的运行周期内可用，那么就需要创建一个全局的临时view。并记得使用的时候加上global_temp作为前缀来引用它，因为全局的临时view是绑定到系统保留的数据库global_temp上。</p>
<ol>
<li><p>创建一个普通的view和全局的view</p>
<p><code>df.createOrReplaceTempView(&quot;emp1&quot;)</code><br><code>df.createGlobalTempView(&quot;emp2&quot;)</code></p>
</li>
<li><p>在当前会话中执行查询，均可查询出结果</p>
<p><code>spark.sql(&quot;select * from emp1&quot;).show</code><br><code>spark.sql(&quot;select * from global_temp.emp2&quot;).show</code></p>
</li>
<li><p>开启一个新的会话，执行同样的查询</p>
<p><code>spark.newSession.sql(&quot;select * from emp1&quot;).show</code>     （运行出错）<br><code>spark.newSession.sql(&quot;select * from global_temp.emp2&quot;).show</code></p>
</li>
</ol>
<h3 id="创建Dataset"><a href="#创建Dataset" class="headerlink" title="创建Dataset"></a>创建Dataset</h3><p>DataFrame的引入，可以让Spark更好的处理结构数据的计算，但其中一个主要的问题是：缺乏编译时类型安全。为了解决这个问题，Spark采用新的Dataset API (DataFrame API的类型扩展)。</p>
<p><img src="https://vinxikk.github.io/img/spark/sql-dataset.png" alt="Dataset与RDD、DataFrame的关系"></p>
<p>Dataset是一个分布式的数据收集器。这是在Spark1.6之后新加的一个接口，兼顾了RDD的优点（强类型，可以使用功能强大的lambda）以及Spark SQL的执行器高效性的优点。所以可以把DataFrames看成是一种特殊的Datasets，即：Dataset(Row)。</p>
<h4 id="使用序列创建Dataset"><a href="#使用序列创建Dataset" class="headerlink" title="使用序列创建Dataset"></a>使用序列创建Dataset</h4><p>定义case class：</p>
<p><code>case class People(id: Int, name: String)</code></p>
<p>生成序列，并创建Dataset：</p>
<p><code>val ds = Seq(People(1, &quot;Tom&quot;), People(2, &quot;Mary&quot;)).toDS</code></p>
<p>查看结果：</p>
<p>scala&gt; <code>ds.show</code><br>+—+—-+<br>| id|name|<br>+—+—-+<br>|  1| Tom|<br>|  2|Mary|<br>+—+—-+</p>
<p>scala&gt; <code>ds.collect</code><br><code>res1: Array[People] = Array(People(1,Tom), People(2,Mary))</code></p>
<h4 id="使用json创建Dataset"><a href="#使用json创建Dataset" class="headerlink" title="使用json创建Dataset"></a>使用json创建Dataset</h4><p>定义case class：</p>
<p><code>case class Person(name: String, gender: String)</code></p>
<p>通过json数据生成DataFrame：</p>
<p><code>val df = spark.read.json(sc.parallelize(&quot;&quot;&quot;{&quot;gender&quot;: &quot;Male&quot;, &quot;name&quot;: &quot;Tom&quot;}&quot;&quot;&quot; :: Nil))</code></p>
<p>将DataFrame转成Dataset：</p>
<p><code>df.as[Person].show</code></p>
<p><code>df.as[Person].collect</code></p>
<h4 id="使用HDFS数据创建Dataset"><a href="#使用HDFS数据创建Dataset" class="headerlink" title="使用HDFS数据创建Dataset"></a>使用HDFS数据创建Dataset</h4><p>读取HDFS数据，并创建Dataset：</p>
<p><code>val linesDS = spark.read.text(&quot;hdfs://rshost001:9000/data/data.txt&quot;).as[String]</code></p>
<p>对Dataset进行操作，分词后，查询长度大于3的单词：</p>
<p><code>val words = linesDS.flatMap(_.split(&quot; &quot;)).filter(_.length &gt; 3)</code></p>
<p><code>words.show</code></p>
<p><code>words.collect</code></p>
<p>执行WordCount程序：</p>
<p><code>val result = linesDS.flatMap(_.split(&quot; &quot;)).map((_,1)).groupByKey(x =&gt; x._1).count</code></p>
<p><code>result.show</code></p>
<p>排序：</p>
<p><code>result.orderBy($&quot;value&quot;).show</code></p>
<h4 id="Dataset案例"><a href="#Dataset案例" class="headerlink" title="Dataset案例"></a>Dataset案例</h4><p>使用emp.json生成DataFrame：</p>
<p><code>val empDF = spark.read.json(&quot;/home/vinx/data/emp.json&quot;)</code></p>
<p>查询工资大于3000的员工：</p>
<p><code>empDF.where($&quot;sal&quot; &gt;= 3000).show</code></p>
<p>创建case class：</p>
<p><code>case class Emp(empno:Long,ename:String,job:String,hiredate:String,mgr:String,sal:Long,comm:String,deptno:Long)</code></p>
<p>生成Dataset，并查询数据：</p>
<p><code>val empDS = empDF.as[Emp]</code></p>
<p>查询工资大于3000的员工：</p>
<p><code>empDS.filter(_.sal &gt; 3000).show</code></p>
<p>查看10号部门的员工：</p>
<p><code>empDS.filter(_.deptno == 10).show</code></p>
<p>多表查询：</p>
<ol>
<li><p>创建部门表</p>
<p><code>val deptRDD=sc.textFile(&quot;/home/vinx/data/dept.csv&quot;).map(_.split(&quot;,&quot;))</code></p>
<p><code>case class Dept(deptno:Int,dname:String,loc:String)</code></p>
<p><code>val deptDS = deptRDD.map(x=&gt;Dept(x(0).toInt,x(1),x(2))).toDS</code></p>
</li>
<li><p>创建员工表</p>
<p><code>val empRDD = sc.textFile(&quot;/home/vinx/data/emp.csv&quot;).map(_.split(&quot;,&quot;))</code></p>
<p><code>case class Emp(empno:Int,ename:String,job:String,mgr:String,hiredate:String,sal:Int,comm:String,deptno:Int)</code></p>
<p><code>val empDS = empRDD.map(x =&gt; Emp(x(0).toInt,x(1),x(2),x(3),x(4),x(5).toInt,x(6),x(7).toInt)).toDS</code></p>
</li>
<li><p>执行多表查询，等值连接</p>
<p><code>val result = deptDS.join(empDS,&quot;deptno&quot;)</code></p>
<p>另一种写法：</p>
<p><code>val result = deptDS.joinWith(empDS,deptDS(&quot;deptno&quot;)=== empDS(&quot;deptno&quot;))</code></p>
<p><code>joinWith</code>和<code>join</code>的区别是连接后的新Dataset的schema会不一样</p>
</li>
<li><p>查看执行计划：<code>result.explain</code></p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">VinxC</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="/https:/vinxikk.github.io/2018/05/24/spark/spark-sql/">https://vinxikk.github.io/2018/05/24/spark/spark-sql/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark-SQL/">Spark SQL</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/05/25/spark/spark-sql-datasource/"><i class="fa fa-chevron-left">  </i><span>Spark SQL常用数据源</span></a></div><div class="next-post pull-right"><a href="/2018/05/22/spark/spark-dependency/"><span>RDD的血缘关系：窄依赖和宽依赖&amp;Spark任务中的Stage</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2020 By VinxC</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>