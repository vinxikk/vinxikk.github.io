<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="RDD高级算子"><meta name="keywords" content="Spark"><meta name="author" content="VinxC"><meta name="copyright" content="VinxC"><title>RDD高级算子 | VinxC's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD的高级算子"><span class="toc-number">1.</span> <span class="toc-text">RDD的高级算子</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mapPartitionsWithIndex"><span class="toc-number">1.1.</span> <span class="toc-text">mapPartitionsWithIndex</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#aggregate"><span class="toc-number">1.2.</span> <span class="toc-text">aggregate</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#aggregateByKey"><span class="toc-number">1.3.</span> <span class="toc-text">aggregateByKey</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#coalesce与repartition"><span class="toc-number">1.4.</span> <span class="toc-text">coalesce与repartition</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#aggregate和aggregateByKey的区别"><span class="toc-number">2.</span> <span class="toc-text">aggregate和aggregateByKey的区别</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#aggregate-1"><span class="toc-number">2.1.</span> <span class="toc-text">aggregate</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#aggregateByKey-1"><span class="toc-number">2.2.</span> <span class="toc-text">aggregateByKey</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduceByKey与groupByKey的区别"><span class="toc-number">3.</span> <span class="toc-text">reduceByKey与groupByKey的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduceByKey和aggregateByKey"><span class="toc-number">4.</span> <span class="toc-text">reduceByKey和aggregateByKey</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://vinxikk.github.io/img/avatar.png"></div><div class="author-info__name text-center">VinxC</div><div class="author-info__description text-center">A Bigdata Developer</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">75</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">89</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">15</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://molunerfinn.com" target="_blank" rel="noopener">Molunerfinn</a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">VinxC's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">RDD高级算子</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-05-21</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Spark/">Spark</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3.4k</span><span class="post-meta__separator">|</span><span>Reading time: 14 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="RDD的高级算子"><a href="#RDD的高级算子" class="headerlink" title="RDD的高级算子"></a>RDD的高级算子</h3><h4 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h4><p>把每个partition中的分区号和对应的值拿出来</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="comment">//...</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr></table></figure>

<p>接收一个函数参数：</p>
<ul>
<li>第一个参数：分区号</li>
<li>第二个参数：分区中的元素</li>
</ul>
<p>示例：将每个分区中的元素和分区号打印出来。</p>
<p><code>val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)</code></p>
<p>创建一个函数返回RDD中的每个分区号和元素：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func1</span></span>(index:<span class="type">Int</span>, iter:<span class="type">Iterator</span>[<span class="type">Int</span>]):<span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">   iter.toList.map( x =&gt; <span class="string">"[PartID:"</span> + index + <span class="string">", value="</span> + x + <span class="string">"]"</span> ).iterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p>调用：<code>rdd1.mapPartitionsWithIndex(func1).collect</code></p>
<p>结果：</p>
<p><code>res18: Array[String] = Array([PartID:0, value=1], [PartID:0, value=2], [PartID:0, value=3], [PartID:0, value=4], [PartID:1, value=5], [PartID:1, value=6], [PartID:1, value=7], [PartID:1, value=8], [PartID:1, value=9])</code></p>
<h4 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h4><p>先对局部聚合，再对全局聚合。</p>
<p><img src="https://vinxikk.github.io/img/spark/rdd-aggregate.png" alt="aggregate算子"></p>
<p><strong>示例1（数字）：</strong></p>
<p><code>val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)</code></p>
<p>查看每个分区中的元素：</p>
<p>scala&gt; <code>rdd1.mapPartitionsWithIndex(func1).collect</code><br><code>res19: Array[String] = Array([PartID:0, value=1], [PartID:0, value=2], [PartID:1, value=3], [PartID:1, value=4], [PartID:1, value=5])</code></p>
<p>将每个分区中的最大值求和（初始值是0）：</p>
<p>scala&gt; <code>rdd1.aggregate(0)(math.max(_,_),_+_)</code><br><code>res20: Int = 7</code></p>
<p>如果初始值是10，则结果为30：</p>
<p>scala&gt; <code>rdd1.aggregate(10)(math.max(_,_),_+_)</code><br><code>res21: Int = 30</code></p>
<p>如果是求和（初始值为0）：</p>
<p>scala&gt; <code>rdd1.aggregate(0)(_+_,_+_)</code><br><code>res22: Int = 15</code></p>
<p>如果初始值是10，则结果是45：</p>
<p>scala&gt; <code>rdd1.aggregate(10)(_+_,_+_)</code><br><code>res23: Int = 45</code></p>
<p><strong>示例2（字符串）：</strong></p>
<p><code>val rdd2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;),2)</code></p>
<p>修改查看分区元素的函数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func2</span></span>(index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  iter.toList.map(x =&gt; <span class="string">"[partID:"</span> +  index + <span class="string">", val: "</span> + x + <span class="string">"]"</span>).iterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p>两个分区中的元素：</p>
<p><code>[partID:0, val: a], [partID:0, val: b], [partID:0, val: c],</code><br><code>[partID:1, val: d], [partID:1, val: e], [partID:1, val: f]</code></p>
<p>运行结果：</p>
<p>scala&gt; <code>rdd2.aggregate(&quot;&quot;)(_+_,_+_)</code><br><code>res25: String = abcdef</code></p>
<p>scala&gt; <code>rdd2.aggregate(&quot;|&quot;)(_+_,_+_)</code><br><code>res26: String = ||abc|def</code></p>
<p><strong>示例3：</strong></p>
<p><code>val rdd3 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;4567&quot;),2)</code></p>
<p><code>rdd3.aggregate(&quot;&quot;)((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y)</code></p>
<p>结果可能是：”24”，也可能是：”42”</p>
<p><code>val rdd4 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;&quot;),2)</code></p>
<p><code>rdd4.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)</code></p>
<p>结果是：”10”，也可能是”01”</p>
<p>原因：注意有个初始值””，其长度0，然后0.toString变成字符串</p>
<p><code>val rdd5 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;&quot;,&quot;345&quot;),2)</code></p>
<p><code>rdd5.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)</code></p>
<p>结果是：”11”，原因同上。</p>
<h4 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h4><p>准备数据：</p>
<p><code>val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 2)</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func3</span></span>(index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Int</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  iter.toList.map(x =&gt; <span class="string">"[partID:"</span> +  index + <span class="string">", val: "</span> + x + <span class="string">"]"</span>).iterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p>两个分区中的元素：</p>
<p><img src="https://vinxikk.github.io/img/spark/rdd-aggregate-example.png" alt="分区中的元素"></p>
<p>将每个分区中的动物最多的个数求和：</p>
<p>scala&gt; <code>pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect</code><br><code>res69: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))</code></p>
<p>将每种动物个数求和：</p>
<p>scala&gt; <code>pairRDD.aggregateByKey(0)(_ + _, _ + _).collect</code><br><code>res71: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))</code></p>
<p>也可以使用<code>reduceByKey</code>：</p>
<p>scala&gt; <code>pairRDD.reduceByKey(_ + _).collect</code><br><code>res73: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))</code></p>
<h4 id="coalesce与repartition"><a href="#coalesce与repartition" class="headerlink" title="coalesce与repartition"></a>coalesce与repartition</h4><p>都是将RDD中的分区进行重分区。</p>
<p>区别是，coalesce默认不会进行shuffle(false)，而repartition会进行shuffle(true)，即会将数据真正通过网络进行重分区。</p>
<p>示例：</p>
<p><code>val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func4</span></span>(index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">Int</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  iter.toList.map(x =&gt; <span class="string">"[partID:"</span> +  index + <span class="string">", val: "</span> + x + <span class="string">"]"</span>).iterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p>下面两句话是等价的：</p>
<p><code>val rdd2 = rdd1.repartition(3)</code><br><code>val rdd3 = rdd1.coalesce(3,true)</code> =&gt;如果是false，查看RDD的length依然是2</p>
<h3 id="aggregate和aggregateByKey的区别"><a href="#aggregate和aggregateByKey的区别" class="headerlink" title="aggregate和aggregateByKey的区别"></a>aggregate和aggregateByKey的区别</h3><h4 id="aggregate-1"><a href="#aggregate-1" class="headerlink" title="aggregate"></a>aggregate</h4><p>aggregate函数的签名如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span></span></pre></td></tr></table></figure>

<p>这个函数是一个柯里化的方法，输入参数分为两个部分：<code>(zeroValue: U)</code>和<code>(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U)</code>。</p>
<p>aggregate先对每个分区的元素做聚合，然后对所有分区的结果做聚合，聚合过程中，使用的是给定的聚集函数以及初始值<code>zeroValue</code>。</p>
<p>这个函数能返回一个与原始RDD不同的类型U，因此，需要一个合并RDD类型T到结果类型U的函数，还需要一个合并类型U的函数。这两个函数都可以修改和返回他们的第一个参数，而不是重新构建一个U类型的参数以避免重新分配内存。</p>
<p><code>zeroValue</code>：seqOp的每个分区的累积结果的初始值以及combOp的不同分区的组合结果的初始值，这通常将是初始元素（例如，”Nil”表的列表连接或”0”表示求和）。</p>
<p><code>seqOp</code>：每个分区累积结果的聚合函数。</p>
<p><code>combOp</code>：用于组合不同分区的结果。</p>
<p>seqOp操作会聚合各分区中的元素，然后combOp操作把所有分区的聚合结果再次聚合，两个操作的初始值都是zeroValue。</p>
<p>seqOp的操作是遍历分区中的所有元素(T)，第一个T跟zeroValue做操作，结果再作为与第二个T做操作的zeroValue，直到遍历完整个分区。</p>
<p>combOp操作是把各分区聚合的结果，再聚合。</p>
<p>aggregate函数返回一个跟RDD不同类型的值。因此，需要一个操作seqOp来把分区中的元素T合并成一个U，另外一个操作combOp把所有U聚合。</p>
<p><strong>示例1（求平均值）：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> (mul, sum, count) = sc.parallelize(list, <span class="number">2</span>).aggregate((<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>))(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    (acc, number) =&gt; (acc._1 * number, acc._2 + number, acc._3 + <span class="number">1</span>),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    (x, y) =&gt; (x._1 * y._1, x._2 + y._2, x._3 + y._3)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(sum / count, mul)</span></pre></td></tr></table></figure>

<p>sum是求和，count是累积元素的个数，mul是求各元素的乘积。</p>
<p>具体过程：</p>
<ol>
<li>初始值是(1, 0, 0)</li>
<li>number是函数中的T，也就是list中的元素，此时类型为Int。而acc的类型为(Int, Int, Int)。<code>acc._1 * num</code>是各元素相乘(初始值为1)，<code>acc._2 + number</code>为各元素相加，<code>acc._3 + 1</code>为累积元素的个数。</li>
<li><code>sum / count</code>用来计算平均值</li>
</ol>
<p><strong>示例2：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> raw = <span class="type">List</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"d"</span>, <span class="string">"f"</span>, <span class="string">"g"</span>, <span class="string">"h"</span>, <span class="string">"o"</span>, <span class="string">"q"</span>, <span class="string">"x"</span>, <span class="string">"y"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> (biggerthanf, lessthanf) = sc.parallelize(raw, <span class="number">1</span>).aggregate((<span class="number">0</span>, <span class="number">0</span>))(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    (cc, str) =&gt; &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">var</span> biggerf = cc._1</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">var</span> lessf = cc._2</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> (str.compareTo(<span class="string">"f"</span>) &gt;= <span class="number">0</span>) biggerf = cc._1 + <span class="number">1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(str.compareTo(<span class="string">"f"</span>) &lt; <span class="number">0</span>) lessf = cc._2 + <span class="number">1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        (biggerf, lessf)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    &#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    (x, y) =&gt; (x._1 + y._1, x._2 + y._2)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr></table></figure>

<p>统计在raw这个List中，比”f”大与比”f”小的元素分别有多少个。</p>
<h4 id="aggregateByKey-1"><a href="#aggregateByKey-1" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h4><p><code>aggregate</code>是针对序列的操作，<code>aggregateByKey</code>则是针对&lt;k,v&gt;对的操作。顾名思义，<code>aggregateByKey</code>就是针对key做<code>aggregate</code>操作。</p>
<p>对PairRDD中相同的Key值进行聚合操作，在聚合过程中同样使用了一个中立的初始值。</p>
<p>和<code>aggregate</code>函数类似，<code>aggregateByKey</code>返回值的类型不需要和RDD中value的类型一致。</p>
<p>因为<code>aggregateByKey</code>是对相同Key中的值进行聚合操作，所以<code>aggregateByKey</code>函数最终返回的类型还是PairRDD，对应的结果是Key和聚合后的值，而<code>aggregate</code>函数直接返回的是非RDD的结果。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)] = self.withScope &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    aggregateByKey(zeroValue, defaultPartitioner(self))(seqOp, combOp)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p>针对&lt;k,v&gt;对的操作，Spark中还有一个<code>combineByKey</code>的函数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(<span class="literal">null</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p><code>aggregateByKey</code>的实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>, partitioner: <span class="type">Partitioner</span>)(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)] = self.withScope &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Serialize the zero value to a byte array so that we can get a new clone of it on each key</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> zeroBuffer = <span class="type">SparkEnv</span>.get.serializer.newInstance().serialize(zeroValue)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> zeroArray = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Byte</span>](zeroBuffer.limit)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    zeroBuffer.get(zeroArray)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> cachedSerializer = <span class="type">SparkEnv</span>.get.serializer.newInstance()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> createZero = () =&gt; cachedSerializer.deserialize[<span class="type">U</span>](<span class="type">ByteBuffer</span>.wrap(zeroArray))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">// We will clean the combiner closure later in `combineByKey`</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> cleanedSeqOp = self.context.clean(seqOp)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    combineByKeyWithClassTag[<span class="type">U</span>](</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        (v: <span class="type">V</span>) =&gt; cleanedSeqOp(createZero(), v),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        cleanedSeqOp, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        combOp, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        partitioner</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    )</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p>从源码可以看出，<code>aggregateByKey</code>调用的就是<code>combineByKey</code>方法。</p>
<p><code>seqOp</code>方法就是<code>mergeValue</code>，<code>combOp</code>方法则是<code>mergeCombiners</code>，<code>cleanedSeqOp(createZero(), v)</code>是<code>createCombiner</code>, 也就是传入的<code>seqOp</code>函数, 只不过其中一个值是传入的zeroValue而已。</p>
<p>因此, 当<code>createCombiner</code>和<code>mergeValue</code>函数的操作相同, <code>aggregateByKey</code>更为合适。</p>
<p><strong>示例：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AggregateByKeyOp</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>])&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">      .setAppName(<span class="string">"AggregateByKey"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">      .setMaster(<span class="string">"local"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> data=<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> rdd=sc.parallelize(data, <span class="number">2</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//合并不同partition中的值，a，b得数据类型为zeroValue的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">combOp</span></span>(a:<span class="type">String</span>,b:<span class="type">String</span>):<span class="type">String</span>=&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">          println(<span class="string">"combOp: "</span>+a+<span class="string">"\t"</span>+b)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">          a+b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//合并在同一个partition中的值，a的数据类型为zeroValue的数据类型，b的数据类型为原value的数据类型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">seqOp</span></span>(a:<span class="type">String</span>,b:<span class="type">Int</span>):<span class="type">String</span>=&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">          println(<span class="string">"SeqOp:"</span>+a+<span class="string">"\t"</span>+b)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">          a+b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">      &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">      rdd.foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//zeroValue:中立值,定义返回value的类型，并参与运算</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//seqOp:用来在同一个partition中合并值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">      <span class="comment">//combOp:用来在不同partiton中合并值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">      <span class="keyword">val</span> aggregateByKeyRDD=rdd.aggregateByKey(<span class="string">"100"</span>)(seqOp, combOp)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">      sc.stop()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p>运行过程：</p>
<p>分区一数据：</p>
<p>((1,3)<br>(1,2)<br>分区二数据：</p>
<p>(1,4)<br>(2,3)</p>
<p>分区一相同key的数据进行合并<br>seq: 100     3   //(1,3)开始和中立值进行合并  合并结果为 1003<br>seq: 1003     2   //(1,2)再次合并 结果为 10032</p>
<p>分区二相同key的数据进行合并<br>seq: 100     4  //(1,4) 开始和中立值进行合并 1004<br>seq: 100     3  //(2,3) 开始和中立值进行合并 1003</p>
<p>将两个分区的结果进行合并<br>key为2的，只在一个分区存在，不需要合并 (2,1003)<br>(2,1003)</p>
<p>key为1的, 在两个分区存在，并且数据类型一致，合并<br>comb: 10032     1004<br>(1,100321004)</p>
<h3 id="reduceByKey与groupByKey的区别"><a href="#reduceByKey与groupByKey的区别" class="headerlink" title="reduceByKey与groupByKey的区别"></a>reduceByKey与groupByKey的区别</h3><p>在Spark中，一切的操作都是基于RDD的。RDD中有一种特殊的format：pair RDD，即RDD的每一行是一个(key, value)的格式。</p>
<p><code>reduceByKey(func, numPartitions=None)</code></p>
<p><em>Merge the values for each key using an associative reduce function. This will also perform the merginglocally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce. Output will be hash-partitioned with numPartitions partitions, or the default parallelism level if numPartitions is not specified.</em></p>
<p><code>reduceByKey</code>用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。</p>
<p><code>groupByKey(numPartitions=None)</code></p>
<p><em>Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions. Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will provide much better performance.</em></p>
<p><code>groupByKey</code>也是对每个key进行操作，但只生成一个sequence。</p>
<p>如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择<code>reduceByKey</code>/<code>aggregateByKey</code>更好。这是因为<code>groupByKey</code>不能自定义函数，我们需要先用<code>groupByKey</code>生成RDD，然后才能对此RDD通过map进行自定义函数操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> words = <span class="type">Array</span>(<span class="string">"one"</span>, <span class="string">"two"</span>, <span class="string">"two"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordCountsWithReduce = wordPairsRDD.reduceByKey(_ + _)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"> </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> wordCountsWithGroup = wordPairsRDD.groupByKey().map(t =&gt; (t._1, t._2.sum))</span></pre></td></tr></table></figure>

<p>上面得到的<code>wordCountsWithReduce</code>和<code>wordCountsWithGroup</code>是完全一样的，但是，它们的内部运算过程是不同的。</p>
<p>当采用<code>reduceByKey</code>时，Spark可以在每个分区移动数据之前将待输出数据与一个共用的key结合。借助下图可以理解在<code>reduceByKey</code>里究竟发生了什么。 注意在数据对被搬移前同一机器上同样的key是怎样被组合的(<code>reduceByKey</code>中的lamdba函数)。然后lamdba函数在每个区上被再次调用来将所有值reduce成一个最终结果。整个过程如下：</p>
<p><img src="https://vinxikk.github.io/img/spark/rdd-reduce-by-key.png" alt="reduceByKey"></p>
<p>当采用<code>groupByKey</code>时，由于它不接收函数，spark只能先将所有的键值对(key-value pair)都移动，这样的后果是集群节点之间的开销很大，导致传输延时。整个过程如下：</p>
<p><img src="https://vinxikk.github.io/img/spark/rdd-group-by-key.png" alt="groupByKey"></p>
<p>因此，在对大数据进行复杂计算时，<code>reduceByKey</code>优于<code>groupByKey</code>。</p>
<p>另外，如果仅仅是group处理，那么以下函数应该优于<code>groupByKey</code>：</p>
<ol>
<li><code>combineByKey</code>：组合数据，但是组合之后的数据类型与输入时值的类型不一样。</li>
<li><code>foldByKey</code>：合并每一个key的所有值，在级联函数和“零值”中使用。</li>
</ol>
<p><code>reduceByKey</code>是先在单台机器中计算，再将结果进行shuffle，减少运算量。</p>
<p><code>groupByKey</code>是对RDD中的所有数据做shuffle,根据不同的Key映射到不同的partition中再进行aggregate。</p>
<p><code>aggregateByKey</code>是先对每个partition中的数据根据不同的Key进行aggregate，然后将结果进行shuffle，完成各个partition之间的aggregate。</p>
<h3 id="reduceByKey和aggregateByKey"><a href="#reduceByKey和aggregateByKey" class="headerlink" title="reduceByKey和aggregateByKey"></a>reduceByKey和aggregateByKey</h3><p>假设有一系列元组，以用户ID为key，以用户在某一时间点访问的网站为value：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> userAccesses = sc.parallelize(<span class="type">Array</span>(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u1"</span>, <span class="string">"site1"</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u2"</span>, <span class="string">"site1"</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u1"</span>, <span class="string">"site1"</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u2"</span>, <span class="string">"site3"</span>), </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    (<span class="string">"u2"</span>, <span class="string">"site4"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">))</span></pre></td></tr></table></figure>

<p>要对这个列表进行处理，获得某个用户访问过且去重后的所有站点。</p>
<p>因为<code>groupByKey</code>运算量较大，可选方案有<code>reduceByKey</code>、<code>aggregateByKey</code>。</p>
<p><code>reduceByKey</code>代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> mapedUserAccess = userAccesses.map(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    userSite =&gt; (userSite._1, <span class="type">Set</span>(userSite._2))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> distinctSite = mapedUserAccess.reduceByKey(_++_)</span></pre></td></tr></table></figure>

<p>但上述代码的问题是，RDD的每个值都将创建一个Set，如果处理一个巨大的RDD,这些对象将大量吞噬内存，并且对垃圾回收造成压力。</p>
<p>如果使用<code>aggregateByKey</code>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> zeroValue = collecyion.mutable.set[<span class="type">String</span>]()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> aggregated = userAccesses.aggregateByKey(zeroValue)(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    (set,v) =&gt; set += v, </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    (setOne, setTwo) =&gt; setOne ++= setTwo</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr></table></figure>

<p>为避免<code>reduceByKey</code>内存问题，可用<code>aggregateByKey</code>。</p>
<p><code>aggregateByKey</code>函数的使用，需为它提供以下三个参数：</p>
<ol>
<li><p>零值（zero）：即聚合的初始值</p>
</li>
<li><p>函数f:(U, V)</p>
<p>把值V合并到数据结构U， 该函数在分区内合并值时使用</p>
</li>
<li><p>函数 g:(U, U)</p>
<p>合并两个数据结构U，在分区间合并值时调用此函数。</p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">VinxC</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="/https:/vinxikk.github.io/2018/05/21/spark/spark-high-level-operation/">https://vinxikk.github.io/2018/05/21/spark/spark-high-level-operation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/05/22/spark/spark-dependency/"><i class="fa fa-chevron-left">  </i><span>RDD的血缘关系：窄依赖和宽依赖&amp;Spark任务中的Stage</span></a></div><div class="next-post pull-right"><a href="/2018/05/20/spark/spark-basic/"><span>Spark架构和部署&amp;WordCount原理</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2020 By VinxC</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>